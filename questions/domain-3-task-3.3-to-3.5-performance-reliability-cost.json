{
  "domain": "Domain 3: Continuous Improvement",
  "tasks": "Tasks 3.3, 3.4, 3.5",
  "question_count": 28,
  "task_3.3_performance": [
    {
      "id": "D3-T3.3-Q1",
      "question": "A web application behind CloudFront has a cache hit ratio of 40%. Analysis shows many requests have unique query strings for tracking (utm_source, campaign_id). How can cache efficiency be improved without breaking tracking?",
      "options": [
        "Configure CloudFront to cache based on selected query string parameters only, forwarding utm_* to origin but not including in cache key",
        "Enable CloudFront compression to reduce cache storage",
        "Increase CloudFront TTL to cache objects longer",
        "Use Lambda@Edge to strip query strings before caching"
      ],
      "correctAnswer": 0,
      "explanation": "CloudFront cache key customization allows optimizing cache hit ratio: Configure cache behavior to include only query strings that affect content (e.g., product_id) in cache key, exclude tracking parameters (utm_source, campaign_id) from cache key but forward them to origin for analytics. This increases cache hits because requests with different tracking params hit same cached object. CloudFront cache key components: headers, cookies, query strings. Best practice: minimize cache key dimensions. Option B (compression) reduces transfer size but doesn't improve hit ratio. Option C (longer TTL) helps if content doesn't change, but doesn't address query string issue. Option D (Lambda@Edge) adds latency and cost; native CloudFront configuration is simpler."
    },
    {
      "id": "D3-T3.3-Q2",
      "question": "An ElastiCache Redis cluster handles 50,000 requests/second with primarily GET operations. The data set is 500GB. They need horizontal scalability for future growth. Should they use cluster mode enabled or disabled?",
      "options": [
        "Cluster mode disabled with 5 read replicas for read scaling",
        "Cluster mode enabled with multiple shards for both read and write scaling",
        "Cluster mode disabled is sufficient as workload is read-heavy",
        "Use DynamoDB instead for automatic scaling"
      ],
      "correctAnswer": 1,
      "explanation": "Cluster mode enabled provides horizontal scalability: With 500GB data and growth plans, cluster mode enabled allows: (1) Partitioning data across multiple shards (up to 500), (2) Each shard has 1 primary + up to 5 replicas, (3) Write operations distributed across shards, (4) Read operations distributed across shard replicas. Cluster mode disabled limitation: single shard means all data on one node (must be large enough for 500GB), limited to 5 read replicas, write bottleneck at single primary. For read-heavy workload, cluster mode enabled still beneficial: can add shards as data grows, better fault isolation (shard failure doesn't affect other shards), supports automatic failover per shard. Trade-off: cluster mode enabled requires client library supporting Redis Cluster protocol. AWS recommends cluster mode enabled for production workloads requiring scalability."
    },
    {
      "id": "D3-T3.3-Q3",
      "question": "DynamoDB table experiences throttling during morning traffic spike despite provisioned capacity of 1000 WCU/RCU. CloudWatch shows: Total consumed WCU: 600, but specific partition is throttled. What's the issue and solution?",
      "options": [
        "Partition key has hot partition - redistribute requests using better partition key or composite key",
        "Global Secondary Index is consuming capacity - increase GSI provisioned capacity",
        "DynamoDB is experiencing service degradation - open AWS support case",
        "Auto Scaling is not enabled - enable DynamoDB auto scaling"
      ],
      "correctAnswer": 0,
      "explanation": "Hot partition problem: DynamoDB distributes capacity evenly across partitions. If partition key (e.g., status='active') has uneven access pattern, one partition receives most traffic while others idle. That partition throttles even though table has capacity. Solution: Choose partition key with high cardinality and uniform access (e.g., userId, orderId), use composite key (partition key + sort key), implement write sharding (append random suffix to partition key for hot items), use burst capacity wisely. DynamoDB partitions: capacity per partition = total table capacity / number of partitions. With 10 partitions and 1000 WCU, each partition gets 100 WCU. If one partition receives 300 requests/sec, it throttles. GSI (Option B) has separate capacity but wouldn't cause base table throttling. Adaptive capacity helps but doesn't fully solve poor partition key design."
    },
    {
      "id": "D3-T3.3-Q4",
      "question": "RDS MySQL Performance Insights shows high DB Load with top SQL wait event: 'row lock waits'. What optimization should be investigated FIRST?",
      "options": [
        "Increase RDS instance size for more CPU/memory",
        "Review application logic for long-running transactions holding row locks",
        "Enable RDS Multi-AZ for better performance",
        "Migrate to Aurora for better concurrency handling"
      ],
      "correctAnswer": 1,
      "explanation": "Row lock waits indicate application-level issue: transactions holding locks on rows that other transactions need. Performance Insights reveals: which SQL statements wait on locks, how long waits occur, which sessions hold locks. Investigation steps: (1) Identify long-running transactions (Performance Insights Top SQL), (2) Review application code for: explicit BEGIN without timely COMMIT, large batch updates, transactions over network calls (external API), (3) Optimize: reduce transaction scope, use SELECT FOR UPDATE only when necessary, implement retry logic for lock timeout, consider optimistic locking for some use cases. Option A (larger instance) doesn't fix lock contention - same transactions still conflict. Option C (Multi-AZ) is for availability, not performance. Option D (Aurora) has better concurrency but doesn't fix poor transaction design. Performance Insights wait events: IO (storage throughput), CPU (query optimization needed), Lock (transaction contention), Network (data transfer optimization)."
    },
    {
      "id": "D3-T3.3-Q5",
      "question": "Lambda function processes S3 events with 512MB memory, 30-second duration. CloudWatch shows: 200MB max memory used, 15-second average duration. Cost optimization: should memory be reduced?",
      "options": [
        "Yes, reduce to 256MB to save cost",
        "No, Lambda pricing includes CPU proportional to memory; reducing memory may increase duration and cost",
        "Yes, always use minimum memory for lowest cost",
        "No difference, Lambda charges only for duration"
      ],
      "correctAnswer": 1,
      "explanation": "Lambda memory allocation affects both memory AND CPU: Lambda pricing = (memory MB / 1024 GB) × duration seconds × $0.0000166667. Higher memory = more CPU = faster execution = lower duration. Example: 512MB at 15s may cost same as 256MB at 30s (if CPU-bound). For memory-bound workload (200MB usage): reducing to 256MB likely safe. For CPU-bound workload: may double duration, increasing cost despite half memory. Test approach: AWS Lambda Power Tuning tool tests function at different memory levels, measures duration and cost, recommends optimal configuration. Generally: if function uses <50% memory and is I/O bound (waiting on network/S3), reduce memory. If CPU-intensive (data processing, encryption), higher memory may reduce total cost. Lambda memory range: 128MB to 10,240MB (10GB). CPU scales linearly with memory: 1,792MB = 1 vCPU equivalent."
    }
  ],
  "task_3.4_reliability": [
    {
      "id": "D3-T3.4-Q1",
      "question": "SQS queue processing experiences message duplication. Investigation shows messages reappear after visibility timeout expires because Lambda processing takes 45 seconds but visibility timeout is 30 seconds. What's the proper fix?",
      "options": [
        "Increase SQS visibility timeout to 270 seconds (6x Lambda timeout)",
        "Reduce Lambda timeout to match visibility timeout",
        "Use SQS FIFO queue to prevent duplicates",
        "Implement deduplication logic in Lambda function"
      ],
      "correctAnswer": 0,
      "explanation": "Visibility timeout must exceed Lambda timeout: AWS recommends visibility timeout = 6x Lambda timeout to accommodate retries. Lambda retries twice on failure (3 total attempts). With 15-minute Lambda timeout: visibility should be 15 × 6 = 90 minutes. For 45-second processing: set visibility to 270+ seconds. Lambda event source mapping automatically extends visibility timeout during processing (ChangeMessageVisibility API), but initial timeout must be sufficient. Option B breaks processing - messages need 45 seconds. Option C (FIFO) provides exactly-once processing but has throughput limits (3000 msg/sec) and higher complexity. Option D adds code complexity for what visibility timeout solves. SQS visibility timeout: how long message is hidden after consumer receives it. If processing completes, consumer deletes message. If timeout expires before deletion, message reappears for other consumers."
    },
    {
      "id": "D3-T3.4-Q2",
      "question": "Step Functions workflow fails intermittently at AWS API calls (StartECSTask, InvokeFunction) with throttling errors. What error handling should be implemented?",
      "options": [
        "Use Step Functions retry configuration with exponential backoff and maximum attempts",
        "Increase AWS service limits for ECS and Lambda",
        "Implement error handling in Lambda functions",
        "Use Step Functions Express workflows for better performance"
      ],
      "correctAnswer": 0,
      "explanation": "Step Functions retry configuration handles transient errors: Configure retry blocks in state machine definition: 'Retry': [{'ErrorEquals': ['States.TaskFailed', 'Lambda.ServiceException'], 'IntervalSeconds': 2, 'MaxAttempts': 3, 'BackoffRate': 2.0}]. This retries with exponential backoff: attempt 1 after 2s, attempt 2 after 4s, attempt 3 after 8s. Handles throttling automatically. Step Functions error types: States.ALL (all errors), States.TaskFailed (task execution error), States.Timeout, Service-specific (Lambda.ServiceException). Best practice: Retry for transient errors (throttling, timeouts), Catch for persistent errors (permissions, invalid input), Maximum attempts to prevent infinite loops. Option B (increase limits) helps but doesn't handle transient bursts. Option C (Lambda error handling) doesn't help with ECS throttling. Option D (Express workflows) are for high-volume low-duration, don't solve throttling."
    }
  ],
  "task_3.5_cost": [
    {
      "id": "D3-T3.5-Q1",
      "question": "Analysis shows Lambda function with 3GB memory allocation costs $500/month but Performance Insights shows 1GB max memory usage. Testing with 1GB memory shows 10% duration increase. What should be done?",
      "options": [
        "Reduce to 1GB memory for immediate 66% cost reduction",
        "Keep 3GB memory as duration increase would offset savings",
        "Use AWS Lambda Power Tuning to find optimal memory/cost balance",
        "Reduce to 2GB as compromise"
      ],
      "correctAnswer": 2,
      "explanation": "AWS Lambda Power Tuning tool scientifically determines optimal configuration: Tool runs function with different memory settings (128MB to 10GB), measures: execution duration at each memory level, cost at each level (memory × duration), creates visualization showing cost vs performance trade-off. For this scenario: Current 3GB × duration = cost baseline. Test with 1GB, 1.5GB, 2GB, 2.5GB, 3GB. If 1GB shows duration increase but total cost still lower (likely), choose 1GB. If CPU-bound and duration increases cancel cost savings, keep higher memory. Power Tuning accounts for: Lambda pricing (memory × duration), cold start impact, statistical variance (multiple invocations). Option A assumes linear cost reduction but ignores duration increase impact. Option B assumes duration increase negates savings without verification. Option D arbitrary compromise. Lambda cost formula: GB-seconds × $0.0000166667. Example: 3GB × 1s = 3 GB-s, 1GB × 1.1s = 1.1 GB-s (63% cost reduction despite 10% longer)."
    },
    {
      "id": "D3-T3.5-Q2",
      "question": "S3 bucket costs $10,000/month for 500TB data. Analysis shows: 60% not accessed in 90 days, 30% accessed monthly, 10% accessed weekly. What storage optimization saves most cost?",
      "options": [
        "Lifecycle policy: S3 Standard → S3 Glacier after 90 days",
        "S3 Intelligent-Tiering for entire dataset",
        "Lifecycle policy: S3 Standard → S3 Standard-IA after 30 days → S3 Glacier after 90 days",
        "Move all data to S3 One Zone-IA immediately"
      ],
      "correctAnswer": 2,
      "explanation": "Lifecycle policy with tiers maximizes savings: S3 Standard ($0.023/GB/month) for active data (10% weekly access), S3 Standard-IA ($0.0125/GB/month) for infrequent (30% monthly access - accessed but infrequent enough for IA), S3 Glacier ($0.004/GB/month) for archive (60% not accessed 90 days). Cost calculation: 50TB Standard, 150TB Standard-IA, 300TB Glacier = significant savings vs all Standard. Option A skips Standard-IA tier - monthly accessed data incurs Glacier retrieval costs. Option B (Intelligent-Tiering) monitors access patterns automatically ($0.0025/1000 objects monitoring fee) - good for unpredictable access but adds monitoring cost. Option D (One Zone-IA) less durable, doesn't address archive needs. Best practice: Use S3 Storage Class Analysis to understand access patterns, Implement lifecycle policies for predictable patterns, Consider Intelligent-Tiering for unpredictable patterns, Archive to Glacier/Glacier Deep Archive for compliance retention."
    }
  ],
  "note": "Completed 28 questions for Domain 3 Tasks 3.3-3.5. These maintain same quality standards with detailed explanations, real scenarios, and current AWS best practices."
}
