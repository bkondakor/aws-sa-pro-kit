{
  "domain": "Mixed Domains - Advanced Scenarios",
  "task": "Batch 4: Migration & Modernization",
  "question_count": 15,
  "questions": [
    {
      "id": "NEW-Q46",
      "question": "A retail company needs to migrate a 500 TB Oracle database to AWS with minimal downtime (<4 hours). The database has active transactions 24/7 with 10,000 transactions per second. They've tested AWS DMS but full load takes 72 hours. The database uses Oracle features like stored procedures, sequences, and materialized views. What migration strategy achieves the RTO requirement?",
      "options": [
        "Use AWS DMS with full load followed by CDC (Change Data Capture) for ongoing replication, then cutover during a maintenance window",
        "Implement Oracle GoldenGate for real-time replication to Aurora PostgreSQL with Babelfish, perform initial bulk load via S3, then enable continuous replication",
        "Use AWS DMS with initial seeding via AWS Snowball Edge to transfer 500 TB offline, then enable CDC for ongoing changes until cutover",
        "Migrate to Amazon RDS for Oracle first using Oracle RMAN backup/restore, then use DMS to convert to Aurora PostgreSQL with minimal downtime"
      ],
      "correctAnswer": 2,
      "explanation": "Using AWS DMS with initial seeding via Snowball Edge provides the fastest migration path to meet the 4-hour RTO requirement. The challenge is the 72-hour full load time for 500 TB. Snowball Edge allows you to perform the initial bulk data transfer offline: (1) order Snowball Edge devices, (2) use AWS DMS on the Snowball Edge device to extract the full database snapshot (this happens offline without consuming network bandwidth), (3) ship the Snowball Edge back to AWS where the data is loaded into the target database, (4) once bulk load completes, enable DMS CDC to replicate ongoing changes from the source Oracle database, (5) after CDC catches up (typically hours), perform final cutover during a 4-hour window. This approach reduces the 72-hour network transfer to a few days of shipping plus a few hours of CDC catch-up. Option A (standard DMS) can't meet the 4-hour RTO because the 72-hour full load is too long. Option B (GoldenGate) is a valid approach and provides real-time replication, but initial bulk load via S3 still requires significant time to export 500 TB to S3, and GoldenGate licensing costs are high. Option D (RDS Oracle first) adds an additional migration step and doesn't solve the bulk transfer time issue. Snowball Edge with DMS is specifically designed for large database migrations where network transfer time is prohibitive."
    },
    {
      "id": "NEW-Q47",
      "question": "A company has a monolithic .NET Framework application (200,000 lines of code) running on Windows Server 2012 R2 on-premises. They need to migrate to AWS quickly (3 months) with minimal refactoring. The application uses IIS, SQL Server 2014, Windows services, and scheduled tasks. It requires 8 vCPUs, 32 GB RAM, and processes 1 million transactions daily. What is the MOST appropriate migration strategy?",
      "options": [
        "Rehost (lift-and-shift) to EC2 Windows instances using AWS Application Migration Service (MGN) and migrate SQL Server to RDS for SQL Server",
        "Replatform by containerizing the application with Windows containers on ECS, and migrate database to RDS for SQL Server",
        "Refactor the application to .NET Core, containerize with Linux containers on EKS, and migrate to Aurora PostgreSQL with Babelfish",
        "Use AWS App2Container to automatically containerize the .NET application and deploy to ECS with RDS for SQL Server"
      ],
      "correctAnswer": 0,
      "explanation": "Rehosting (lift-and-shift) using AWS Application Migration Service (formerly CloudEndure) is the most appropriate strategy given the tight 3-month timeline and minimal refactoring requirement. MGN provides automated rehost migration: (1) install MGN agent on source servers, (2) continuous replication of server state to AWS, (3) automated conversion to AWS format, (4) testing on AWS without impacting source, (5) cutover with minutes of downtime. For the SQL Server database, migrating to RDS for SQL Server maintains compatibility while reducing operational overhead. This approach requires minimal application changes - primarily configuration updates for RDS endpoints - and can be completed within 3 months. Option B (containerizing) requires significant effort to containerize a 200,000-line .NET Framework application, handle Windows services and scheduled tasks in container orchestration, which is not 'minimal refactoring' and unlikely to complete in 3 months. Option C (refactor to .NET Core + PostgreSQL) is a full re-architecture requiring major code changes, extensive testing, and conversion from SQL Server to PostgreSQL - this would take 12+ months, not 3. Option D (App2Container) is interesting but has limitations: App2Container works best for simpler applications, and Windows services/scheduled tasks require additional orchestration setup; also, .NET Framework support is limited compared to .NET Core. The rehost approach minimizes risk, meets the timeline, and provides a foundation for future modernization."
    },
    {
      "id": "NEW-Q48",
      "question": "A financial institution is migrating 5000 servers from their data center to AWS over 12 months. They need to maintain detailed inventory, track migration progress, group servers by applications, and plan migration waves. The migration team (50 people) needs collaboration tools and reporting. They're using AWS Application Discovery Service for discovery. What AWS service should they use for migration planning and tracking?",
      "options": [
        "AWS Migration Hub for centralized tracking, application grouping, and migration progress monitoring across MGN and DMS",
        "AWS Service Catalog to create migration portfolios and track server provisioning status",
        "Custom solution using DynamoDB for inventory, Lambda for automation, and QuickSight for reporting dashboards",
        "AWS Systems Manager with custom automation documents and Parameter Store for migration state tracking"
      ],
      "correctAnswer": 0,
      "explanation": "AWS Migration Hub is the purpose-built service for large-scale migration planning, tracking, and execution. Migration Hub integrates with AWS Application Discovery Service to import the discovered server inventory, allows grouping servers into applications, supports migration wave planning, tracks migration status across multiple tools (Application Migration Service, Database Migration Service, Server Migration Service), provides progress dashboards, and supports team collaboration. For a 5000-server migration with 50 team members, Migration Hub provides: (1) centralized inventory from Application Discovery Service, (2) application grouping and dependency mapping, (3) automated status tracking as servers migrate through MGN/SMS, (4) progress reporting and metrics, (5) multi-account support. Option B (Service Catalog) is for governing IT service provisioning, not migration tracking - it's designed for creating and managing catalogs of approved AWS services. Option C (custom solution) would require significant development effort and maintenance when a purpose-built service exists; for 5000 servers, the complexity would be substantial. Option D (Systems Manager) provides automation and configuration management but not migration-specific features like application grouping, dependency mapping, and migration wave planning. Migration Hub is specifically designed for this use case and is AWS's recommended approach for large-scale migrations."
    },
    {
      "id": "NEW-Q49",
      "question": "A company has a VMware environment with 300 VMs (mix of Linux and Windows) that they want to migrate to AWS. The VMs use VMware-specific features like snapshots, vMotion, and shared VMFS datastores. The operations team has deep VMware expertise but limited AWS experience. They need to maintain operational consistency during the migration period (6 months) with the ability to migrate VMs gradually. What is the MOST suitable migration approach?",
      "options": [
        "Deploy VMware Cloud on AWS, migrate VMs using vMotion to maintain operational consistency, then gradually convert to native AWS services",
        "Use AWS Application Migration Service to convert and migrate VMs to EC2, retraining the team on AWS operations",
        "Implement AWS Server Migration Service (SMS) with incremental replication and automated EC2 conversion",
        "Use AWS Import/Export to create AMIs from VMware VMs and launch EC2 instances from the AMIs"
      ],
      "correctAnswer": 0,
      "explanation": "VMware Cloud on AWS is the most suitable approach for this scenario. It provides a VMware SDDC (Software-Defined Data Center) running natively on AWS infrastructure, allowing the team to use their existing VMware expertise and tools (vCenter, vMotion, vSphere) while migrating to AWS. The migration approach: (1) deploy VMware Cloud on AWS, (2) establish connectivity (DX or VPN) between on-premises VMware and VMware Cloud on AWS, (3) use vMotion to migrate VMs with zero downtime and no VM conversion, (4) operate VMs in VMware Cloud on AWS while gradually transitioning to native AWS services (EC2, RDS) as the team gains AWS expertise. This provides operational consistency, leverages existing skills, and allows gradual modernization. Option B (Application Migration Service) requires immediate operational change and AWS training, which doesn't provide the operational consistency requested. Option C (SMS) is deprecated in favor of Application Migration Service (MGN), and while it provided incremental replication, it still requires immediate conversion to EC2. Option D (Import/Export) requires VM export/import for each migration and doesn't support incremental replication or zero-downtime migration. VMware Cloud on AWS is specifically designed for customers with VMware expertise who want to maintain operational consistency while moving to AWS."
    },
    {
      "id": "NEW-Q50",
      "question": "A SaaS company needs to migrate 50 microservices currently running in on-premises Kubernetes to AWS. Each service has different scaling requirements, dependencies, and release cycles. They use Helm charts for deployment, Prometheus for monitoring, and GitOps workflows. The team wants to minimize operational overhead while maintaining their existing tooling and workflows. What is the MOST appropriate AWS service?",
      "options": [
        "Amazon EKS with managed node groups, deploy existing Helm charts, and integrate with AWS services using IAM roles for service accounts",
        "Amazon ECS with Fargate, convert Kubernetes manifests to ECS task definitions, and use AWS native monitoring with CloudWatch",
        "Amazon EKS Anywhere to replicate the on-premises Kubernetes environment in AWS EC2 for consistency",
        "Amazon EKS with Fargate for serverless pods, use EKS add-ons for monitoring, and maintain Helm charts and GitOps workflows"
      ],
      "correctAnswer": 3,
      "explanation": "Amazon EKS with Fargate provides the best combination of minimizing operational overhead while maintaining existing tooling. EKS Fargate eliminates the need to manage Kubernetes worker nodes - AWS manages the underlying infrastructure. The team can: (1) migrate existing Helm charts with minimal changes, (2) maintain GitOps workflows using tools like ArgoCD or Flux, (3) use Kubernetes-native monitoring (Prometheus can run as a pod or use Amazon Managed Prometheus), (4) leverage IAM roles for service accounts (IRSA) for AWS service integration, (5) avoid node management overhead (patching, scaling, capacity planning). Fargate's per-pod pricing also aligns well with microservices where different services have different scaling needs. Option A (EKS with managed node groups) is good but requires more operational overhead for node management, patching, and capacity planning compared to Fargate. Option B (ECS) requires converting all Kubernetes manifests to ECS task definitions, abandoning Helm charts and Kubernetes-native tooling, which contradicts the requirement to maintain existing tooling. Option C (EKS Anywhere) is designed for running Kubernetes on-premises or at the edge, not for AWS cloud migration - it doesn't reduce operational overhead. EKS Fargate provides the serverless Kubernetes experience that minimizes operations while maintaining full Kubernetes compatibility."
    },
    {
      "id": "NEW-Q51",
      "question": "A media company has 10 PB of video content stored on-premises in a network-attached storage (NAS) system using NFS. They need to migrate this data to S3 for a new cloud-native video processing pipeline. The data center has a 1 Gbps internet connection. Transfer via internet would take 2+ years. They need the migration completed within 3 months. What is the MOST efficient migration approach?",
      "options": [
        "Order multiple AWS Snowball Edge devices (100 TB each) to transfer data in parallel, then use AWS DataSync for final synchronization",
        "Use AWS Storage Gateway File Gateway to gradually tier data to S3 while maintaining on-premises access during migration",
        "Deploy AWS DataSync agent on-premises to accelerate transfer over internet with parallel multi-threaded transfers",
        "Order AWS Snowmobile (100 PB capacity) to transfer all data in a single shipment"
      ],
      "correctAnswer": 0,
      "explanation": "Using multiple AWS Snowball Edge devices in parallel is the most efficient approach for migrating 10 PB within 3 months given the network bandwidth constraint. The math: 10 PB = 10,240 TB. With 1 Gbps connection operating at 80% efficiency (realistic overhead), transfer rate is ~0.1 TB/hour or 2.4 TB/day, requiring ~4,270 days (11.7 years), not 2 years. Snowball Edge devices (80-100 TB usable capacity each) can be deployed in parallel: order 10-15 devices, load them simultaneously on-premises (multiple NAS connections), ship to AWS where data is transferred to S3 in parallel. Total time: device delivery (1-2 weeks) + data loading in parallel (few weeks with multiple devices) + shipping (1 week) + AWS processing (1 week) + final DataSync sync for changed data (days). This completes within 3 months. Option B (Storage Gateway) is for gradual, ongoing hybrid cloud scenarios, not bulk one-time migration - 10 PB over 1 Gbps still takes years. Option C (DataSync over internet) doesn't solve the fundamental bandwidth problem - even with acceleration, 1 Gbps can't transfer 10 PB in 3 months. Option D (Snowmobile) has 100 PB capacity but is overkill and expensive for 10 PB, designed for 10+ PB datasets; also, Snowmobile requires special logistics and longer lead times. Multiple Snowball Edge devices provide the right balance of capacity, parallelism, cost, and timeline."
    },
    {
      "id": "NEW-Q52",
      "question": "A company is migrating an Oracle database (20 TB) to AWS and wants to minimize licensing costs by migrating to open-source PostgreSQL. The database has complex PL/SQL stored procedures, Oracle-specific data types (like NUMBER, VARCHAR2), and applications expecting Oracle SQL syntax. The team has a 6-month timeline and limited PostgreSQL expertise. What migration path minimizes risk and licensing costs?",
      "options": [
        "Migrate to Amazon RDS for PostgreSQL, manually convert PL/SQL to PL/pgSQL, and update applications for PostgreSQL syntax",
        "Migrate to Amazon Aurora PostgreSQL with Babelfish which provides Oracle compatibility and allows gradual application migration",
        "Use AWS Schema Conversion Tool (SCT) to automatically convert database schema and code, then migrate using DMS to RDS PostgreSQL",
        "Migrate to RDS for Oracle first to eliminate infrastructure management, then use SCT and DMS for gradual conversion to Aurora PostgreSQL"
      ],
      "correctAnswer": 1,
      "explanation": "Amazon Aurora PostgreSQL with Babelfish is the optimal solution for minimizing risk while achieving license cost reduction. Babelfish is a SQL Server and Oracle-compatible endpoint for Aurora PostgreSQL that allows applications to connect using Oracle SQL syntax (TDS protocol for SQL Server, Oracle protocol via foreign data wrapper). With Babelfish: (1) applications can continue using Oracle SQL syntax and drivers with minimal changes, (2) the database backend is PostgreSQL (eliminating Oracle licensing costs), (3) you get Aurora's performance and scalability benefits, (4) migration risk is reduced because applications don't require immediate rewrite, (5) you can gradually refactor applications to native PostgreSQL syntax over time. For PL/SQL procedures, Babelfish provides compatibility for common Oracle syntax and functions. Option A (manual conversion) is high-risk for a team with limited PostgreSQL expertise and complex PL/SQL procedures - likely to miss nuances and exceed the 6-month timeline. Option C (SCT automated conversion) helps but SCT's automated conversion typically achieves 80-90% conversion for complex databases; the remaining 10-20% requires manual work, and complex PL/SQL may not convert cleanly. Option D (RDS Oracle first) doesn't eliminate Oracle licensing costs initially and adds an extra migration step. Babelfish provides the best balance of risk reduction, license cost savings, and timeline feasibility."
    },
    {
      "id": "NEW-Q53",
      "question": "A healthcare provider has a DICOM medical imaging application that stores patient scans on a Windows file server using SMB protocol. The application requires low-latency access to recent images (last 30 days) and regulatory compliance requires 7-year retention. Storage is 500 TB and growing 10 TB monthly. They want to migrate to AWS while maintaining SMB access for the application. What is the MOST cost-effective solution?",
      "options": [
        "Deploy Amazon FSx for Windows File Server with 500 TB storage and use data deduplication to reduce costs",
        "Implement AWS Storage Gateway File Gateway with S3 backend, configure caching for recent files, and use S3 Intelligent-Tiering for long-term retention",
        "Use Amazon FSx for Windows File Server with 30 days of data and AWS DataSync to archive older data to S3 Glacier Deep Archive",
        "Deploy Amazon FSx for Lustre with S3 backend integration for high-performance access and automatic tiering"
      ],
      "correctAnswer": 2,
      "explanation": "Using Amazon FSx for Windows File Server for active data (30 days) with AWS DataSync to archive older data to S3 Glacier Deep Archive is the most cost-effective solution. FSx for Windows File Server provides native SMB access with Active Directory integration, maintaining full compatibility with the DICOM application. For the cost optimization: FSx stores only recent 30 days of data (~10 TB, costing ~$1,300/month), while DataSync automatically archives data older than 30 days to S3 Glacier Deep Archive (~470 TB at $0.00099/GB/month = ~$465/month). Total: ~$1,765/month. Option A (500 TB FSx) would cost ~$65,000/month (500 TB Ã— $0.13/GB/month) which is extremely expensive. Option B (Storage Gateway File Gateway) could work but has limitations: local cache sizing for 10 TB of active data requires significant on-premises or EC2 storage, and latency for accessing cached files is higher than native FSx; S3 Intelligent-Tiering costs more than Glacier Deep Archive for data that's clearly infrequently accessed (older than 30 days). Option D (FSx for Lustre) is for high-performance computing workloads and doesn't provide SMB protocol - it uses POSIX-compliant file system access, so the DICOM application would need modification. The FSx + DataSync + Glacier Deep Archive approach provides the best balance of performance for active data (FSx), SMB compatibility, and cost optimization for long-term retention (Glacier)."
    },
    {
      "id": "NEW-Q54",
      "question": "A company wants to migrate their Jenkins-based CI/CD pipelines to AWS. They have 200 Jenkins jobs, custom plugins, and extensive Groovy scripts for build automation. The infrastructure team manages 20 Jenkins master and worker instances. The team wants to reduce infrastructure management overhead while maintaining their existing job definitions and scripts. What is the MOST operationally efficient migration path?",
      "options": [
        "Migrate Jenkins to EC2 instances with Auto Scaling groups for workers to reduce manual infrastructure management",
        "Containerize Jenkins masters and workers, deploy on EKS with Kubernetes-based auto-scaling for workers",
        "Migrate to AWS CodePipeline and CodeBuild, converting Jenkins jobs to CloudFormation templates and buildspec.yml files",
        "Refactor to AWS CodeCatalyst which provides managed CI/CD with workflow definitions similar to Jenkins"
      ],
      "correctAnswer": 1,
      "explanation": "Containerizing Jenkins and deploying on Amazon EKS provides the best balance of reduced operational overhead while maintaining existing job definitions and scripts. This approach: (1) maintains full Jenkins compatibility with existing jobs, plugins, and Groovy scripts, (2) eliminates manual infrastructure management - Kubernetes handles orchestration, (3) provides dynamic scaling of Jenkins workers using Kubernetes pods, (4) uses the Kubernetes plugin for Jenkins to spawn workers on-demand, (5) reduces costs by scaling workers to zero when not in use. The team can use Jenkins Configuration as Code (JCasC) for version-controlled configuration. Containerization is straightforward with official Jenkins Docker images. Option A (EC2 with Auto Scaling) reduces some overhead but still requires managing EC2 instances, patching, capacity planning, and doesn't provide the same level of automation as Kubernetes. Option C (CodePipeline/CodeBuild) requires converting all 200 Jenkins jobs to AWS-native format - this is a complete rewrite of build definitions and Groovy scripts, taking months and not maintaining existing job definitions as required. Option D (CodeCatalyst) is AWS's newer managed CI/CD service and could reduce overhead significantly, but similar to option C, it requires migrating/converting existing Jenkins job definitions to CodeCatalyst workflows. Containerized Jenkins on EKS maintains compatibility while reducing operational burden."
    },
    {
      "id": "NEW-Q55",
      "question": "A company has a mainframe application processing insurance claims using COBOL code (500,000 lines) and a DB2 database (5 TB). The application processes 100,000 claims daily with complex business logic. They want to modernize to AWS to reduce mainframe costs ($2M annually) but maintain business continuity. The COBOL team is retiring and new hires prefer modern languages. What is the MOST practical modernization strategy?",
      "options": [
        "Rehost the mainframe to AWS using AWS Mainframe Modernization with Micro Focus runtime to run COBOL on AWS",
        "Refactor COBOL code to Java using automated conversion tools, deploy on ECS, and migrate DB2 to Aurora PostgreSQL",
        "Use AWS Mainframe Modernization with automated refactoring to convert COBOL to Java and DB2 to Aurora PostgreSQL",
        "Retire the mainframe application and implement a new claims processing system using modern AWS services like Lambda and DynamoDB"
      ],
      "correctAnswer": 2,
      "explanation": "AWS Mainframe Modernization with automated refactoring provides the most practical path for this scenario. AWS Mainframe Modernization offers two patterns: (1) replatform (running COBOL on AWS) and (2) refactor (automated conversion to Java). For this situation where the COBOL team is retiring and the goal is to reduce mainframe costs while modernizing, the refactor option is optimal. The service: (1) uses automated tools (like Blu Age from AWS) to convert COBOL to Java with business logic preservation, (2) converts DB2 to Aurora PostgreSQL, (3) deploys on managed AWS infrastructure (ECS or similar), (4) provides validation tools to verify business logic correctness, (5) includes migration support and professional services. This addresses the retiring COBOL expertise issue while maintaining business continuity through automated conversion. Option A (rehost with Micro Focus) maintains COBOL code, which doesn't solve the retiring expertise problem and doesn't fully modernize the application. Option B (manual refactor to Java) is extremely risky and time-consuming - 500,000 lines of COBOL with complex business logic would take years to manually convert and test, with high risk of business logic errors. Option D (retire and rebuild) is even higher risk - rewriting a complex claims processing system from scratch typically takes 3-5 years and risks losing embedded business logic accumulated over decades. AWS Mainframe Modernization's automated refactoring provides a middle ground: modernize the language and infrastructure while preserving battle-tested business logic."
    },
    {
      "id": "NEW-Q56",
      "question": "A financial services company is migrating their on-premises Active Directory (AD) with 50,000 users and 10,000 Windows workstations to AWS. They need to maintain AD for authentication, group policies, and domain join for EC2 Windows instances. Some users will remain on-premises for 2 years during gradual migration. The solution must support hybrid access and minimize operational overhead. What is the BEST AD architecture?",
      "options": [
        "Deploy AWS Managed Microsoft AD in AWS, establish a two-way trust with on-premises AD, and use AWS Directory Service for hybrid access",
        "Replicate on-premises AD to EC2-based domain controllers in AWS, configure site-to-site VPN for replication",
        "Use AWS Managed Microsoft AD as primary, migrate all users to cloud AD, and use AWS Client VPN for on-premises access",
        "Deploy Simple AD in AWS for EC2 instances and maintain on-premises AD separately without trust relationship"
      ],
      "correctAnswer": 0,
      "explanation": "Deploying AWS Managed Microsoft AD with a two-way trust relationship to on-premises AD is the best architecture for this hybrid scenario. This approach provides: (1) AWS Managed Microsoft AD handles all the operational overhead (patching, backups, replication, monitoring) for the cloud AD, (2) two-way trust allows users in on-premises AD to authenticate to AWS resources and vice versa, (3) supports gradual migration - users can remain in on-premises AD during the 2-year transition, (4) EC2 Windows instances can domain-join to AWS Managed AD, (5) group policies can be managed in either directory, (6) AWS Managed AD integrates with AWS services like Amazon WorkSpaces, RDS for SQL Server, and FSx for Windows File Server. The trust relationship enables seamless authentication across both environments during the migration period. Option B (EC2 domain controllers) requires managing domain controllers yourself (patching, backups, high availability), which adds operational overhead compared to the managed service. Option C (migrate all users immediately) contradicts the requirement that some users remain on-premises for 2 years. Option D (Simple AD) is a standalone directory based on Samba 4, not actual Microsoft AD - it doesn't support trust relationships with on-premises AD, doesn't support all AD features, and is limited to 5,000 users (this scenario has 50,000). AWS Managed Microsoft AD with trust relationships is the recommended pattern for hybrid Windows authentication scenarios."
    },
    {
      "id": "NEW-Q57",
      "question": "A company is migrating a stateful legacy application that stores session state in local disk files on application servers. The application has 20 web servers behind a load balancer using sticky sessions. Sessions last up to 4 hours and contain user data that must persist across application deployments and server failures. They want to modernize session management on AWS. What is the MOST appropriate solution?",
      "options": [
        "Store sessions in Amazon ElastiCache for Redis with session data backup enabled for persistence across failures",
        "Use Amazon EFS mounted to all EC2 instances to share session files across servers, maintaining compatibility with file-based session storage",
        "Configure Application Load Balancer with sticky sessions and store session files on local instance store volumes",
        "Store sessions in DynamoDB with TTL configured for automatic session expiration after 4 hours"
      ],
      "correctAnswer": 0,
      "explanation": "Amazon ElastiCache for Redis is the optimal solution for modernizing session management. Redis provides: (1) in-memory performance with sub-millisecond latency for session reads/writes, (2) persistence options (AOF and RDB snapshots) to survive failures and maintain sessions across deployments, (3) automatic failover with Multi-AZ for high availability, (4) seamless scalability by adding read replicas or cluster mode sharding, (5) automatic eviction policies to manage memory, (6) native TTL support for session expiration. Many application frameworks have built-in Redis session store libraries, making integration straightforward. ElastiCache for Redis is a managed service, eliminating operational overhead. Option B (EFS) maintains file compatibility but introduces significant performance overhead - EFS latency is milliseconds vs microseconds for Redis; file I/O is much slower than in-memory cache; concurrent file access from 20 servers could cause locking issues. Option C (ALB sticky sessions with local storage) doesn't solve the persistence problem - instance store volumes are ephemeral and lost on instance termination, and sticky sessions fail when instances are replaced during deployments or failures. Option D (DynamoDB) is a valid alternative and provides good performance (single-digit millisecond latency) with strong durability, but for session use cases where reads/writes happen on every request, Redis's sub-millisecond performance provides better user experience. Redis is the industry-standard solution for session management and recommended by AWS for this use case."
    },
    {
      "id": "NEW-Q58",
      "question": "A company wants to migrate their Hadoop cluster (200 nodes, 5 PB data) running Spark jobs for ETL processing to AWS. Jobs run nightly for 6 hours processing data in HDFS and output to S3. The cluster is idle for 18 hours daily. On-premises costs are $500K annually. They want to reduce costs while maintaining Spark compatibility. What is the MOST cost-effective approach?",
      "options": [
        "Migrate to Amazon EMR with EC2 on-demand instances, store data in S3 instead of HDFS for durability and cost savings",
        "Deploy a persistent EMR cluster with EC2 Spot instances to reduce compute costs by 70-90% while maintaining cluster availability",
        "Use Amazon EMR with auto-scaling and transient clusters that terminate after job completion, storing all data in S3",
        "Migrate to AWS Glue for ETL processing with automatic scaling and serverless architecture"
      ],
      "correctAnswer": 2,
      "explanation": "Using Amazon EMR with transient clusters that terminate after job completion provides the greatest cost savings. The key insight is the cluster is idle 18 hours daily (75% of the time). With transient clusters: (1) launch EMR cluster from S3 data before nightly job (15 minutes), (2) run Spark jobs for 6 hours, (3) terminate cluster after completion, (4) pay only for 6.25 hours of compute (vs 24 hours with persistent cluster), reducing compute costs by 74%. Storing data in S3 instead of HDFS provides: (5) durability without HDFS replication overhead, (6) separation of storage and compute, (7) lower storage costs ($23/TB/month for S3 vs $50+/TB for EBS with replication), (8) ability to share data with other AWS services. For 5 PB, S3 costs ~$115K/year vs HDFS on EBS ~$250K/year. Total savings: 74% on compute + 54% on storage = ~70% total cost reduction. Option A (on-demand persistent cluster) reduces some cost vs on-premises but pays for idle compute 18 hours daily. Option B (Spot persistent cluster) provides savings but still pays for idle capacity; also, Spot interruptions could cause data loss in HDFS (though EMR supports Spot best practices). Option D (AWS Glue) could work but has limitations: Glue is serverless and cost-effective, but some complex Spark jobs may need modification to run on Glue, and Glue has higher per-DPU-hour costs than EMR Spot instances. EMR transient clusters are the classic cost-optimization pattern for batch ETL workloads."
    },
    {
      "id": "NEW-Q59",
      "question": "A media company is migrating their video rendering farm (1000 servers) that processes 4K and 8K video files. Rendering jobs take 2-8 hours each and are compute-intensive (CPU and GPU). Jobs are submitted via a job queue, and the current on-premises farm costs $3M annually with 60% average utilization. They need to maintain the existing job submission API. What is the MOST cost-effective AWS architecture?",
      "options": [
        "Deploy AWS Batch with EC2 Spot instances (g5 for GPU, c6i for CPU) in compute environments with automatic scaling based on job queue depth",
        "Use EC2 Auto Scaling groups with mixed instance types (Spot and on-demand) and integrate with existing job queue via SQS",
        "Migrate to Amazon ECS with Fargate Spot for containerized rendering jobs and use EventBridge to trigger jobs",
        "Deploy ParallelCluster with mixed Spot and on-demand instances for HPC workload management"
      ],
      "correctAnswer": 0,
      "explanation": "AWS Batch with EC2 Spot instances provides the most cost-effective solution for this batch processing workload. AWS Batch: (1) is specifically designed for batch computing with job queues, dependencies, and prioritization, (2) automatically scales compute environments based on job queue depth (zero idle capacity costs), (3) supports multiple compute environments (GPU-optimized g5 instances for GPU rendering, compute-optimized c6i for CPU rendering), (4) integrates EC2 Spot instances for up to 90% cost savings vs on-demand, (5) handles Spot interruptions gracefully with automatic job retry, (6) provides job submission APIs that can integrate with existing systems. For the cost: if on-premises is $3M at 60% utilization, jobs use $1.8M of effective compute. With Batch + Spot, AWS cost could be $200K-400K annually (70-90% savings) by: eliminating idle capacity through auto-scaling to zero, using Spot instances at 70-90% discount, and optimizing instance types per job requirements. Option B (Auto Scaling + SQS) requires building custom job management logic that AWS Batch provides natively - more development and maintenance overhead. Option C (Fargate Spot) is limited because Fargate doesn't support GPU instances (needed for 4K/8K rendering), and Fargate has higher per-vCPU costs than EC2. Option D (ParallelCluster) is for HPC workloads requiring tight coupling and MPI, not for embarrassingly parallel batch rendering jobs; it also requires more manual management. AWS Batch is purpose-built for this exact use case and provides the best cost optimization and operational simplicity."
    },
    {
      "id": "NEW-Q60",
      "question": "A SaaS company is migrating their multi-tenant application serving 10,000 customers from a single on-premises database to AWS. Each customer's data is isolated using tenant_id column. As they scale, they want to improve tenant isolation, enable per-customer performance optimization, and support tiered pricing (some customers willing to pay for dedicated resources). The database is PostgreSQL (10 TB total). What is the MOST flexible migration architecture?",
      "options": [
        "Migrate to a single Aurora PostgreSQL cluster with read replicas, continue using tenant_id for isolation, and use Aurora Serverless v2 for auto-scaling",
        "Implement a database-per-tenant architecture using Aurora PostgreSQL Serverless v2 with one cluster per customer for maximum isolation",
        "Use a hybrid approach: high-value customers get dedicated Aurora clusters, mid-tier customers share multi-tenant Aurora clusters, low-tier on Aurora Serverless v2",
        "Migrate to DynamoDB with partition keys based on tenant_id for automatic scaling and tenant isolation"
      ],
      "correctAnswer": 2,
      "explanation": "A hybrid approach with tiered database architecture provides the most flexibility for a multi-tenant SaaS application. This architecture: (1) dedicates Aurora PostgreSQL clusters to high-value/enterprise customers who pay premium pricing, providing complete isolation, customizable performance (instance sizing, IOPS), independent upgrades, and dedicated security controls, (2) groups mid-tier customers (10-50 per cluster) on shared Aurora multi-tenant clusters with tenant_id isolation, balancing cost and performance, (3) uses Aurora Serverless v2 for low-tier/trial customers with unpredictable usage, automatically scaling capacity and minimizing costs. This supports the business model with infrastructure aligned to pricing tiers and customer requirements. For 10,000 customers, you might have: 100 enterprise on dedicated clusters, 2,000 mid-tier on 40 shared clusters (50 per cluster), 7,900 low-tier on Serverless v2. Option A (single cluster) doesn't provide tenant isolation improvements or per-customer optimization - all customers share resources and performance can't be individually optimized. Option B (database per tenant for all) would create 10,000 Aurora clusters, which is operationally complex and expensive - even with Serverless v2, managing 10,000 clusters is impractical. Option D (DynamoDB) requires complete application rewrite from relational PostgreSQL to NoSQL, which is extremely high effort and risk. The hybrid tiered approach is a recognized SaaS architecture pattern that balances isolation, performance, cost, and operational complexity while supporting business model requirements."
    }
  ]
}
