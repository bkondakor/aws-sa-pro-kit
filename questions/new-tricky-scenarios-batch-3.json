{
  "domain": "Mixed Domains - Advanced Scenarios",
  "task": "Batch 3: Performance & Optimization",
  "question_count": 15,
  "questions": [
    {
      "id": "NEW-Q31",
      "question": "A social media application uses DynamoDB with on-demand billing. During peak hours (2 hours daily), they experience read throttling on their Posts table (100 GB) despite having sufficient capacity. CloudWatch shows 80% of reads target a small set of celebrity user posts. Average item size is 4 KB. They need sub-millisecond read performance during peak hours. What is the MOST cost-effective solution?",
      "options": [
        "Switch to provisioned capacity mode with auto-scaling and configure higher base capacity for peak hours",
        "Implement DynamoDB Accelerator (DAX) with a cluster of 3 nodes to cache hot items and reduce read load on the table",
        "Create a Global Secondary Index (GSI) on the user_id attribute and query the GSI instead of the main table",
        "Enable DynamoDB auto scaling with scheduled scaling to increase capacity before peak hours"
      ],
      "correctAnswer": 1,
      "explanation": "DynamoDB Accelerator (DAX) is the optimal solution for this hot partition problem. DAX is an in-memory cache specifically designed for DynamoDB that provides microsecond read latency and significantly reduces read load on the underlying table. Since 80% of reads target a small set of items (celebrity posts), these will be cached in DAX, eliminating throttling caused by hot partitions. DAX is fully managed, automatically handles cache invalidation, and is more cost-effective than over-provisioning capacity for the entire table. A 3-node DAX cluster (minimum for production) costs approximately $0.32/hour while providing millions of cached reads per second. Option A (provisioned capacity with higher base) would cost more because you'd pay for capacity across all partitions even though only some are hot, and doesn't solve the underlying hot partition issue - DynamoDB partitions can still throttle if a single partition receives too many reads. Option C (GSI) doesn't solve the problem; the GSI would have the same hot partition issue if the same celebrity posts are queried. Option D (scheduled scaling) is similar to option A - it increases overall capacity but doesn't address the hot partition problem. DAX is purpose-built for caching hot items and provides the best combination of performance improvement and cost efficiency."
    },
    {
      "id": "NEW-Q32",
      "question": "A video streaming platform serves 4K video content globally using CloudFront. Users in Asia-Pacific report buffering during peak hours despite CloudFront distribution. Origin is S3 in us-east-1. CloudWatch shows increased origin fetch latency from Asia during peak (800ms vs 200ms off-peak). Videos are 2-8 GB each, and the catalog changes weekly. What solution provides the BEST user experience improvement?",
      "options": [
        "Enable CloudFront Origin Shield in an Asia-Pacific region to reduce origin load and improve cache hit ratio",
        "Create a secondary S3 bucket in ap-southeast-1 with S3 Cross-Region Replication and configure CloudFront origin failover",
        "Increase CloudFront TTL values to reduce origin requests and enable compression for video content",
        "Implement AWS Global Accelerator in front of S3 to optimize network path from Asia to us-east-1"
      ],
      "correctAnswer": 0,
      "explanation": "CloudFront Origin Shield is the best solution for this scenario. Origin Shield acts as an additional caching layer between CloudFront edge locations and the origin. When enabled in a region close to Asia-Pacific, it centralizes origin requests from multiple edge locations in that region, significantly improving cache hit ratios (often by 10-50%) and reducing the number of requests that reach the S3 origin in us-east-1. During peak hours when multiple edge locations are serving similar content, Origin Shield ensures that each object is only fetched once from origin instead of each edge location fetching separately. This reduces origin load and latency. For a weekly-changing catalog, Origin Shield's caching is effective. Option B (S3 CRR with origin failover) provides redundancy but not performance improvement - failover is for handling origin failures, not for reducing latency. You could use it as a multi-origin setup, but that requires more complex management. Option C (higher TTL) could help but TTL is already likely optimized for video content, and compression isn't effective for already-compressed video files. Option D (Global Accelerator for S3) doesn't integrate with CloudFront and would require changing the architecture entirely; Global Accelerator is better for dynamic content, not cached static video content served via CloudFront. Origin Shield is specifically designed to solve this exact problem of regional origin load and cache efficiency."
    },
    {
      "id": "NEW-Q33",
      "question": "An e-commerce company uses Aurora PostgreSQL for their product catalog (5 TB database). During sales events, read traffic increases 10x and the single writer instance (db.r6g.16xlarge) shows CPU at 90% despite having 15 read replicas. Most queries are simple lookups by product_id which rarely change. The read replicas show low utilization (<20% CPU). What is the PRIMARY bottleneck and solution?",
      "options": [
        "The write instance is handling all read traffic; configure Aurora custom endpoints to direct read traffic to read replicas",
        "Replication lag is causing reads to hit the writer; enable Aurora Advanced Auditing to identify slow queries",
        "The writer instance is experiencing lock contention; implement Aurora Write-Through Cache in Aurora PostgreSQL to offload reads",
        "Connection pooling is insufficient; implement RDS Proxy to manage database connections and reduce overhead on the writer"
      ],
      "correctAnswer": 2,
      "explanation": "The Aurora Write-Through Cache (Aurora PostgreSQL 14.8+ and 15.3+) is the optimal solution for this specific scenario. Write-Through Cache is an in-memory cache integrated directly into Aurora PostgreSQL that caches frequently accessed data (like product catalog lookups by primary key) with microsecond latency. The key insight is that even though there are 15 read replicas with low utilization, the writer instance is at 90% CPU, which suggests the application is not properly configured to use read replicas OR there's another bottleneck. Write-Through Cache offloads simple key-value lookups from the database engine itself, reducing CPU usage on both writers and replicas. It's particularly effective for read-heavy workloads with hot data that rarely changes (like product catalogs during sales). Option A (custom endpoints) would be correct if read replicas showed high utilization, but they're at 20%, suggesting read traffic IS going to replicas or the traffic pattern doesn't parallelize well. Option B (replication lag) doesn't match the symptoms - Aurora typically has sub-second replication lag, and this wouldn't cause 90% CPU on the writer. Option D (RDS Proxy) helps with connection management and failover but won't reduce CPU if the queries themselves are the bottleneck. The 90% CPU on writer with underutilized replicas suggests the workload is write-heavy or has poor read distribution, and Write-Through Cache specifically addresses high read throughput on simple lookups."
    },
    {
      "id": "NEW-Q34",
      "question": "A financial analytics application runs complex SQL queries on a 20 TB data warehouse in Redshift. Queries that previously ran in 5 minutes now take 30+ minutes as data grows. EXPLAIN plans show queries scan full tables. The tables store historical trade data and are rarely updated but queried with filters on trade_date and symbol columns. Queries typically filter last 90 days. What provides the MOST significant performance improvement?",
      "options": [
        "Create materialized views for commonly queried date ranges and use automatic query rewrite",
        "Implement table partitioning by trade_date and define sort keys on symbol and trade_date columns",
        "Enable Redshift Concurrency Scaling to add cluster capacity during high query load",
        "Upgrade to Redshift RA3 node types with managed storage and enable automatic table optimization"
      ],
      "correctAnswer": 1,
      "explanation": "Implementing the correct distribution style with sort keys on frequently filtered columns (trade_date and symbol) provides the most significant performance improvement for this workload. Redshift is a columnar database that relies heavily on proper data distribution and sort keys for query performance. Sort keys physically order data on disk, allowing Redshift's zone maps to skip entire blocks of data that don't match query predicates. For queries filtering on trade_date (last 90 days) and symbol, defining a compound sort key on (trade_date, symbol) or interleaved sort key allows Redshift to skip scanning the majority of the 20 TB dataset, reducing I/O by 70-95% depending on query selectivity. Additionally, choosing the right distribution key (likely symbol for this access pattern) minimizes data movement during joins. Option A (materialized views) can help but requires maintaining multiple views for different date ranges and doesn't address the underlying inefficiency of full table scans on the base tables. Option C (Concurrency Scaling) adds compute capacity for concurrent queries but doesn't make individual queries faster - a 30-minute query will still take 30 minutes. Option D (RA3 nodes with managed storage) provides better price-performance and automatic optimization but doesn't fundamentally change query execution like proper sort keys do. Proper data distribution and sort keys are foundational to Redshift performance and should be addressed first."
    },
    {
      "id": "NEW-Q35",
      "question": "A SaaS platform uses Lambda functions (1 GB memory, average 2-second execution) processing events from SQS queues. During traffic spikes, they see increased errors and latency. CloudWatch shows Lambda throttling errors and concurrent executions hitting account limit (1000). Reserved concurrency is not configured. Processing 50,000 events during peak hours takes 45 minutes versus 10 minutes during off-peak. What is the MOST effective solution to reduce processing time?",
      "options": [
        "Request AWS Support to increase account concurrent execution limit from 1000 to 3000",
        "Configure reserved concurrency of 3000 for the Lambda function to guarantee capacity during spikes",
        "Implement SQS FIFO queues with message grouping to reduce concurrent Lambda invocations and prevent throttling",
        "Optimize Lambda function code to reduce execution time from 2 seconds to 1 second, doubling processing throughput"
      ],
      "correctAnswer": 0,
      "explanation": "Requesting an increase to the account-level concurrent execution limit from AWS Support is the most direct solution. The account limit (default 1000) is a soft limit that can be increased. With 50,000 events and 2-second execution time, achieving 10-minute processing requires (50,000 events × 2 seconds) / 600 seconds = ~167 concurrent executions in ideal conditions, but with SQS polling batching and overhead, you likely need more. The current 45-minute processing time suggests the 1000 limit isn't the bottleneck - wait, let me recalculate: 50,000 events × 2 sec = 100,000 execution seconds. At 1000 concurrent, that's 100 seconds minimum. The 45-minute (2700 seconds) suggests other issues like SQS visibility timeout, Lambda cold starts, or batch size configuration. However, if we're hitting throttling errors, increasing the limit removes that bottleneck. Option B (reserved concurrency of 3000) is incorrect - you cannot set reserved concurrency higher than your account limit; reserved concurrency carves out a portion of the account limit for a specific function. Option C (FIFO queues) actually reduces throughput because FIFO queues have lower TPS (3000 vs standard queue's unlimited) and message grouping serializes processing within a group. Option D (code optimization) would help but requires development time and may not be feasible depending on the processing requirements. While option A is the direct answer, the real-world solution likely involves both limit increase AND optimizing batch size, memory allocation, and SQS configuration to maximize throughput."
    },
    {
      "id": "NEW-Q36",
      "question": "A machine learning application processes images from S3 using Lambda functions (3 GB memory, 5-minute execution). During large batch jobs (10,000 images), total cost is $800 and processing takes 8 hours. Each Lambda invocation downloads a 50 MB image, processes it (CPU-intensive resizing and feature extraction), and uploads results to S3. What is the MOST cost-effective way to reduce costs by at least 50%?",
      "options": [
        "Reduce Lambda memory to 1 GB to lower costs per invocation while accepting longer execution time",
        "Migrate the workload to AWS Batch with Fargate Spot instances for long-running, batch processing jobs",
        "Implement Lambda function batching to process 10 images per invocation instead of 1, reducing total invocations",
        "Use Lambda with Graviton2 processors (arm64) which provide better price-performance for compute-intensive workloads"
      ],
      "correctAnswer": 1,
      "explanation": "Migrating to AWS Batch with Fargate Spot instances provides the most significant cost reduction for this workload. Lambda pricing includes both GB-seconds of compute and per-request charges. For long-running batch processing (5-minute executions, 10,000 images, 8 hours total), Lambda is not cost-optimal. AWS Batch with Fargate Spot instances can reduce costs by 70-90% compared to Lambda because: (1) Batch jobs pay per-second vCPU and memory without per-request charges, (2) Spot instances provide up to 70% discount vs on-demand, (3) batch processing can optimize resource utilization better than many short Lambda invocations, (4) no 15-minute execution limit like Lambda. For this workload: Lambda costs ~$0.08 per image ($800/10,000), while Fargate Spot might cost ~$0.02 per image, achieving >75% cost reduction. Option A (reduce memory) might save some cost but significantly increases execution time, potentially increasing total GB-seconds and cost. Option C (batching) helps by reducing request charges from 10,000 to 1,000, but request charges are minimal ($0.20 per million) compared to compute charges. Option D (Graviton2) provides ~20% better price-performance, which is good but doesn't achieve 50% cost reduction. AWS Batch is purpose-built for long-running, resource-intensive batch jobs and provides much better economics than Lambda for this use case."
    },
    {
      "id": "NEW-Q37",
      "question": "A real-time bidding platform uses API Gateway with Lambda backend processing bid requests. They receive 10,000 requests per second with <100ms latency requirement. Current implementation shows CloudWatch API Gateway latency at 150ms (50ms over requirement). Lambda execution time is 30ms. API Gateway REST API uses regional endpoint with no caching. What optimization provides the GREATEST latency reduction?",
      "options": [
        "Switch from REST API to HTTP API which has lower latency (~20-30ms reduction) for simple proxy integrations",
        "Enable API Gateway caching with 300-second TTL to cache identical requests and reduce Lambda invocations",
        "Implement Lambda Provisioned Concurrency to eliminate cold starts and reduce invocation latency",
        "Deploy the API Gateway edge-optimized endpoint to use CloudFront's global network for lower latency"
      ],
      "correctAnswer": 0,
      "explanation": "Switching from API Gateway REST API to HTTP API provides the greatest latency reduction for this use case. HTTP APIs are optimized for low-latency, high-performance scenarios with simpler proxy integrations to Lambda. HTTP APIs typically have 20-30ms lower latency than REST APIs because they have a lighter-weight protocol and fewer features (no request validation, models, or SDK generation overhead). For the real-time bidding platform where latency is critical and features like request validation aren't needed, HTTP API can reduce the 150ms total latency to ~120-130ms, getting closer to the 100ms requirement. Option B (API Gateway caching) doesn't work for real-time bidding - each bid request is unique based on user, context, and auction parameters, so cache hit rate would be nearly 0%. Option C (Provisioned Concurrency) eliminates Lambda cold starts, but the question states Lambda execution is 30ms, suggesting cold starts aren't the issue; also, Provisioned Concurrency affects Lambda initialization, not Lambda execution time. Option D (edge-optimized endpoint) uses CloudFront to route requests to the nearest edge location, but for a regional service where most traffic comes from specific regions, this might not help and could even add latency due to extra hop through CloudFront. HTTP APIs are specifically designed for low-latency, high-throughput use cases and are the right choice for real-time bidding platforms."
    },
    {
      "id": "NEW-Q38",
      "question": "A media company transcodes videos using EC2 instances (c5.9xlarge) in an Auto Scaling group. Jobs take 1-3 hours each. During scaling down, instances are terminated mid-job, causing job failures. They use SQS for job queue with 4-hour visibility timeout. Current implementation shows 15% of jobs fail due to instance termination. They need zero job failures during scale-down while maintaining cost optimization. What is the BEST solution?",
      "options": [
        "Implement lifecycle hooks in Auto Scaling to delay instance termination, query SQS for in-flight messages, and wait for job completion before allowing termination",
        "Switch to Spot Instances with Spot Instance interruption notices, implement checkpointing to save progress, and resume jobs on new instances",
        "Configure Auto Scaling with custom health checks that mark instances as unhealthy if processing jobs, preventing termination",
        "Use AWS Batch instead of Auto Scaling groups to manage job processing with automatic job retry on instance termination"
      ],
      "correctAnswer": 0,
      "explanation": "Implementing Auto Scaling lifecycle hooks is the best solution for graceful shutdown of instances processing long-running jobs. Lifecycle hooks allow you to perform custom actions before instances are terminated during scale-down. The implementation would: (1) create a lifecycle hook for instance termination, (2) trigger a Lambda function or script on the instance when the hook is invoked, (3) check if the instance is processing a SQS job, (4) if yes, wait for job completion (up to hook timeout of 2 hours by default, extendable to 48 hours), (5) complete the lifecycle action to allow termination. This ensures zero job failures during scale-down while still allowing cost optimization through scaling. Option B (Spot Instances with checkpointing) is a good practice for cost savings (60-90% discount) but requires significant application changes to implement checkpointing for video transcoding, and Spot interruptions are independent of scaling decisions. Option C (custom health checks) doesn't prevent termination - unhealthy instances are terminated faster by Auto Scaling; health checks determine replacement, not protection from termination. Option D (AWS Batch) is a valid alternative that handles job management and instance lifecycle, but requires migrating the entire architecture; lifecycle hooks provide a solution within the existing Auto Scaling infrastructure. Lifecycle hooks are specifically designed for this use case of graceful shutdown during scale-down."
    },
    {
      "id": "NEW-Q39",
      "question": "A gaming company stores player session data in ElastiCache for Redis (cache.r6g.xlarge cluster mode disabled) for real-time game state. They experience memory pressure with evictions during peak hours (200,000 concurrent players). Dataset is 25 GB and growing. Read-heavy workload (read:write ratio 100:1) with sub-millisecond latency requirement. What provides the BEST performance and scalability?",
      "options": [
        "Enable cluster mode with 3 shards and 2 replicas per shard to distribute data and scale read capacity",
        "Upgrade to a larger instance type (cache.r6g.2xlarge) to double memory capacity and reduce evictions",
        "Implement application-level caching with lazy loading strategy to reduce Redis memory usage",
        "Enable Multi-AZ with automatic failover and add read replicas to scale read capacity without changing cluster mode"
      ],
      "correctAnswer": 0,
      "explanation": "Enabling Redis cluster mode with sharding provides the best performance and scalability for this growing dataset. Cluster mode allows you to partition data across multiple shards (up to 500), distributing the 25 GB dataset and allowing horizontal scaling. With 3 shards, each shard holds ~8 GB, reducing memory pressure. Each shard can have read replicas (up to 5 per shard), so 3 shards × 2 replicas = 6 additional read endpoints to handle the read-heavy workload (100:1 read:write ratio). This architecture scales both storage (through sharding) and read capacity (through replicas) while maintaining sub-millisecond latency. As the dataset grows beyond 25 GB, you can add more shards without downtime. Option B (larger instance) provides short-term relief but doesn't solve the long-term scalability issue - you're limited by the largest instance size (cache.r6g.16xlarge with 327 GB), and it doesn't scale read capacity. Option C (application caching) adds complexity and latency - you already have Redis for caching; adding another caching layer doesn't solve the memory pressure issue. Option D mentions adding read replicas without cluster mode - in non-clustered mode, you can only have 1 primary and up to 5 replicas, but all replicas contain the full 25 GB dataset, so you can't solve memory pressure by adding replicas, only read capacity. Cluster mode is the right solution for both memory and read scaling."
    },
    {
      "id": "NEW-Q40",
      "question": "A data analytics company runs Athena queries on 500 TB of compressed Parquet files in S3 organized by date (s3://bucket/year=2024/month=01/day=01/). Queries filter by date and user_id. Average query scans 50 TB and costs $250 (at $5/TB scanned). Most queries filter by last 7 days. They run 1000 queries monthly costing $250,000. What provides the GREATEST cost reduction?",
      "options": [
        "Convert Parquet files to ORC format with better compression and predicate pushdown, reducing scan volume by 10-15%",
        "Implement partition projection in Athena to eliminate partition metadata queries and improve query performance",
        "Create a partitioned table by date and user_id with smaller file sizes (128 MB target) to improve partition pruning and reduce scanned data by 60-80%",
        "Enable Athena Query Result Reuse with 24-hour caching to avoid re-scanning data for identical queries"
      ],
      "correctAnswer": 2,
      "explanation": "Creating a properly partitioned table by date and user_id with optimized file sizes provides the greatest cost reduction. The current implementation partitions only by date, so queries filtering by user_id still scan all data within the date partition. By partitioning on both date and user_id (or using a composite partition key), Athena can prune partitions more effectively. Additionally, many small files or very large files hurt Athena performance - 128 MB is the optimal file size for Parquet in S3. If queries typically filter to last 7 days and specific users, proper partitioning can reduce scanned data from 50 TB to 5-10 TB (80-90% reduction), reducing query cost from $250 to $25-50, saving $200+ per query and potentially $200,000+ monthly. Option A (ORC format) provides marginal compression improvement over Parquet (10-15%) but both are columnar formats with similar performance; the savings don't justify the migration effort. Option B (partition projection) improves query planning time by eliminating the need to query Glue catalog for partition metadata, but doesn't reduce data scanned - it's a performance optimization, not cost optimization. Option D (query result reuse) only helps if queries are identical and run within the cache TTL - for analytics workloads with varying parameters, cache hit rate is typically low. The key insight is that partitioning by query predicates (date AND user_id) dramatically reduces scanned data volume, which is the primary cost driver in Athena."
    },
    {
      "id": "NEW-Q41",
      "question": "A financial application uses Step Functions to orchestrate a workflow with 15 Lambda functions processing loan applications. The workflow takes 5 minutes end-to-end with most time spent in credit checks (Lambda calls external API with 3-minute response time). They process 1 million applications monthly. State transitions cost $0.025 per 1000 transitions. Each workflow has 30 state transitions costing $0.75 per execution. What reduces costs by at least 60%?",
      "options": [
        "Switch from Standard Workflows to Express Workflows which charge by execution duration instead of state transitions",
        "Combine multiple Lambda functions into fewer functions to reduce state transitions from 30 to 10",
        "Implement parallel states to run independent Lambda functions concurrently, reducing workflow duration",
        "Replace Lambda functions with ECS tasks to eliminate per-request Lambda charges"
      ],
      "correctAnswer": 0,
      "explanation": "Switching from Step Functions Standard Workflows to Express Workflows provides the greatest cost reduction. Express Workflows are designed for high-volume, short-duration workloads and charge based on execution duration ($1.00 per million requests + $0.00001667 per GB-second), not state transitions. For a 5-minute workflow: Express Workflow cost = $0.000001 (request) + (300 seconds × $0.00001667) = ~$0.005 per execution vs Standard Workflow cost of $0.75 (30 transitions × $0.025/1000). This is a 99.3% cost reduction! For 1 million executions: Express = $5,000 vs Standard = $750,000. The tradeoff is Express Workflows have a maximum duration of 5 minutes (this workflow is exactly at the limit) and don't support all features like waiting for human approval, but for automated workflows with external API calls, Express Workflows are ideal. Option B (combining Lambda functions) reduces transitions from 30 to 10, saving $0.50 per execution (33% reduction, not 60%), and increases complexity. Option C (parallel states) reduces duration but doesn't reduce state transition count or cost. Option D (ECS tasks) might reduce Lambda costs but adds ECS overhead and doesn't address the Step Functions state transition costs. Express Workflows are specifically designed for this use case and provide massive cost savings."
    },
    {
      "id": "NEW-Q42",
      "question": "A content delivery application serves static website assets (HTML, CSS, JS, images) from S3 via CloudFront. The index.html file (entry point) is updated every 15 minutes with new content references. Users report seeing stale content for up to 24 hours after deployments. Current CloudFront TTL is 24 hours for all objects. Creating CloudFront invalidations for every deployment costs $0.005 per path invalidated. They deploy 100 times daily with 50 files per deployment. What is the MOST cost-effective solution?",
      "options": [
        "Reduce CloudFront TTL to 15 minutes for all objects to match the update frequency",
        "Implement versioned filenames (e.g., app.v123.js) for static assets and use short TTL (60 seconds) for index.html only",
        "Create CloudFront invalidations for the 50 changed files after each deployment, accepting the $0.25 per deployment cost",
        "Use CloudFront Functions to modify cache keys based on file modification time in S3"
      ],
      "correctAnswer": 1,
      "explanation": "Implementing versioned filenames for static assets with a short TTL only for index.html is the most cost-effective and performant solution. This is a best practice for cache invalidation: static assets (CSS, JS, images) get unique versioned filenames (app.v123.js, app.v124.js), allowing 24+ hour caching since each version is unique. The index.html file references the new versioned assets and has a short TTL (60 seconds or less), ensuring users get the latest references quickly. When you deploy: (1) upload new versioned assets to S3, (2) update index.html to reference new versions, (3) users fetch index.html (short TTL means it refreshes within 60 seconds), (4) index.html references new versioned assets which are fetched fresh. No invalidations needed! CloudFront invalidation costs for 100 deployments × 50 files = $25/day ($750/month). Option A (15-minute TTL for all) causes cache efficiency to drop dramatically, increasing origin requests from S3 by 96× (from 1 request per 24 hours to 96 per 24 hours), increasing latency and S3 GET request costs significantly. Option C (invalidations) works but costs $750/month unnecessarily. Option D (cache keys based on modification time) is overly complex and CloudFront Functions can't query S3 metadata. The versioned filename approach is how major CDNs and frameworks (React, Vue, webpack) handle cache invalidation - it's the industry standard for good reason."
    },
    {
      "id": "NEW-Q43",
      "question": "A microservices application uses Application Load Balancer with target groups pointing to ECS Fargate tasks. During deployment of new task versions (rolling update), users experience intermittent 502 errors for 2-3 minutes. The tasks have a health check (HTTP GET /health) with 30-second interval, 2 consecutive successes required to be healthy, and 2 failures to be unhealthy. Tasks take 60 seconds to fully start and be ready to serve traffic. What resolves the 502 errors?",
      "options": [
        "Increase the deregistration delay from default 300 seconds to 600 seconds to ensure connections drain properly",
        "Configure health check interval to 10 seconds and reduce healthy threshold to 1 successful check to detect healthy tasks faster",
        "Implement a longer health check grace period (60 seconds) and configure slow start mode on target group to gradually increase traffic",
        "Change the health check path to a lightweight endpoint (/ping) that responds immediately without application initialization"
      ],
      "correctAnswer": 2,
      "explanation": "Implementing a health check grace period combined with slow start mode resolves the 502 errors during deployments. The root cause is: tasks take 60 seconds to start, but health checks begin immediately with 30-second intervals requiring 2 successes (60 seconds to be marked healthy). During rolling deployment, old tasks are deregistered while new tasks are still initializing, creating a gap where ALB has no healthy targets, causing 502 errors. Health check grace period (not an ALB feature but an ECS service feature) delays health check failures during task startup, but the key is the target group slow start mode. Slow start gradually increases the share of traffic sent to newly registered targets over a specified duration (60-300 seconds), preventing new tasks from being overwhelmed while still initializing. This ensures smooth traffic shift during deployments. Option A (deregistration delay) affects how long ALB waits before deregistering a target, which helps drain connections but doesn't help new targets become healthy faster. Option B (faster health checks) might help slightly (30 seconds to healthy vs 60) but doesn't solve the fundamental issue that tasks need 60 seconds to initialize - they'd still fail health checks during startup. Option D (lightweight health check) defeats the purpose if it doesn't verify application readiness - the health check would pass before the application is ready, sending traffic to unready tasks causing errors. The combination of proper health check configuration and slow start is the correct solution for zero-downtime deployments."
    },
    {
      "id": "NEW-Q44",
      "question": "A mobile gaming company uses DynamoDB with Global Tables in 3 regions (us-east-1, eu-west-1, ap-southeast-1) for player profiles. They observe that players in Asia frequently experience write conflicts where their profile updates are overwritten by the last-write-wins conflict resolution. A player's session in Asia writes profile updates while background jobs in us-east-1 update player statistics, causing data loss. What is the BEST solution to prevent write conflicts?",
      "options": [
        "Implement conditional writes using expected attribute values to detect conflicts and retry with exponential backoff",
        "Configure DynamoDB Streams with Lambda to implement custom conflict resolution logic based on timestamps",
        "Route all write traffic to the us-east-1 region using Route 53 latency-based routing to ensure sequential writes",
        "Implement optimistic locking using a version attribute, incrementing it with each write and using conditional expressions to prevent overwrites"
      ],
      "correctAnswer": 3,
      "explanation": "Implementing optimistic locking with a version attribute is the best solution for preventing write conflicts in DynamoDB Global Tables. Optimistic locking works by: (1) adding a version number attribute to each item, (2) when reading an item, noting its current version, (3) when writing, using a conditional expression like 'SET data = :new_data, version = :new_version WHERE version = :expected_version', (4) if the version doesn't match (another write occurred), the update fails and the application can retry with fresh data. This prevents lost updates from concurrent writes across regions while allowing Global Tables replication to work. Unlike application-level last-write-wins, optimistic locking ensures that writes are only applied if the item hasn't been modified since it was read. Option A (conditional writes with expected values) is similar but less systematic than version-based locking; checking multiple attribute values is more complex than a single version number. Option B (DynamoDB Streams with custom logic) implements pessimistic locking or custom conflict resolution after the fact, but DynamoDB Global Tables already replicate via streams - adding custom logic here is complex and creates consistency issues. Option C (routing all writes to one region) defeats the purpose of Global Tables which is to provide low-latency writes in multiple regions; it also creates a single point of failure. Optimistic locking is the standard pattern for handling concurrent updates in eventually consistent distributed databases."
    },
    {
      "id": "NEW-Q45",
      "question": "A data processing pipeline uses Kinesis Data Streams with 50 shards processing 50 MB/sec (1 MB/shard/sec) of clickstream data. Lambda consumers (100 concurrent executions) process events with 5-second execution time. CloudWatch shows high IteratorAge (up to 5 minutes) during peak hours indicating processing lag. Increasing Lambda concurrency to 200 doesn't reduce IteratorAge. What is the PRIMARY bottleneck and solution?",
      "options": [
        "Kinesis shard throughput limit (2 MB/sec read per shard) is the bottleneck; increase shard count to 100 to double read capacity",
        "Lambda is not polling efficiently; enable enhanced fan-out on Kinesis to push records to Lambda with dedicated 2 MB/sec per consumer",
        "Lambda concurrent execution limit is the bottleneck; request limit increase from AWS Support",
        "Kinesis record batch size is too small; increase Lambda batch size to process more records per invocation"
      ],
      "correctAnswer": 1,
      "explanation": "Enabling Kinesis enhanced fan-out is the solution. The key insight is that increasing Lambda concurrency from 100 to 200 didn't help, indicating Lambda concurrency isn't the bottleneck. With standard iteration (polling mode), each Lambda consumer uses GetRecords API which has a limit of 5 GetRecords calls per second per shard and 2 MB/sec aggregate across all consumers per shard. With multiple Lambda functions polling the same shard, they compete for the 2 MB/sec limit, creating a bottleneck regardless of Lambda concurrency. Enhanced fan-out gives each consumer a dedicated 2 MB/sec throughput per shard using HTTP/2 push (SubscribeToShard API), eliminating the shared throughput bottleneck. With 50 shards at 1 MB/sec each, enhanced fan-out easily handles the load. This also reduces IteratorAge because data is pushed immediately rather than polled every second. Option A (more shards) would help if individual shards were over 1 MB/sec write limit, but they're not - they're at 1 MB/sec which is within limits. Option C (Lambda concurrency) was already tested by increasing to 200 with no improvement. Option D (larger batch size) might help slightly by reducing number of Lambda invocations, but doesn't address the fundamental read throughput bottleneck from shared polling. Enhanced fan-out is specifically designed to solve this multi-consumer bottleneck on Kinesis streams."
    }
  ]
}
