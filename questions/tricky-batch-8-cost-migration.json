{
  "domain": "Mixed Domains: Advanced Scenarios",
  "task": "Tricky Batch 8: Cost Optimization & Migration Strategies",
  "question_count": 15,
  "questions": [
    {
      "question": "A company has a portfolio of 500 EC2 instances running 24/7 across multiple instance families (m5, c5, r5) in us-east-1. They purchased Standard Reserved Instances (RIs) for 300 instances one year ago with 3-year terms. Due to a major architecture change, they've migrated to containerized workloads on ECS Fargate and no longer need the EC2 instances. They have 2 years remaining on the RI commitment. What is the MOST cost-effective approach to minimize waste from the unused RIs?",
      "options": [
        "Sell the unused Reserved Instances on the AWS Reserved Instance Marketplace to recover a portion of the remaining committed cost",
        "Modify the Reserved Instances to Convertible RIs, which allows changing instance families, and apply them to the closest matching instance types still in use",
        "Contact AWS Support to cancel the Reserved Instance commitment early with a termination fee of 50% of the remaining commitment",
        "Keep the Reserved Instances active and launch EC2 instances to match the RI commitment, even if not needed, because the RI cost is already paid upfront"
      ],
      "correctAnswer": 0,
      "explanation": "Option A is correct. The Reserved Instance Marketplace allows you to sell Standard Reserved Instances that you no longer need. Key details: 1) Only Standard RIs can be sold (not Convertible RIs), 2) RIs must be active for at least 30 days before selling, 3) You receive payment for the remaining term at a market-determined price (typically 20-40% of the original cost, depending on remaining term and instance type demand), 4) AWS charges a 12% service fee on the sale price, 5) Only All Upfront and Partial Upfront RIs can be sold. This recovers some value from the unused commitment. The remaining loss is a sunk cost. Alternative: If you have other AWS accounts in your organization, you can share RI benefits across accounts using Consolidated Billing - if any account runs matching EC2 instances, they'll receive the RI discount automatically. Option B is incorrect - you CANNOT convert Standard RIs to Convertible RIs. The conversion only goes one way: you can exchange Convertible RIs for other Convertible RIs (different instance families, tenancy, or operating systems), but Standard RIs cannot be converted to Convertible. Standard RIs can be modified (change AZ within the same region, change instance size within the same family, change network from EC2-Classic to VPC), but this doesn't help if you've completely migrated away from EC2. Option C is incorrect - AWS does not allow early termination or cancellation of Reserved Instance commitments, even with a penalty fee. The commitment is binding for the full term. This is why it's critical to plan RI purchases carefully and consider starting with Convertible RIs (which offer more flexibility) or Savings Plans (which apply to compute usage across EC2, Fargate, and Lambda). Option D is incorrect and wasteful - launching EC2 instances just to 'use' the RI billing benefit doesn't make sense if you don't actually need the compute capacity. You're paying for the RI commitment AND the operational costs (data transfer, storage, etc.). Better to sell the RIs and cut losses. Key principle: RI cost optimization: 1) Use RI utilization reports in Cost Explorer to monitor unused RIs, 2) Sell unused Standard RIs on the marketplace, 3) Use Convertible RIs for workloads with uncertain future requirements (higher cost but more flexibility), 4) Consider Savings Plans instead of RIs - Savings Plans apply to Fargate and Lambda in addition to EC2, providing flexibility for containerized and serverless architectures, 5) For predictable workloads, use All Upfront payment for maximum discount (up to 72% vs On-Demand), 6) Implement RI management processes: forecast capacity needs, right-size before purchasing, review utilization quarterly."
    },
    {
      "type": "multiple",
      "question": "A SaaS company runs a multi-tenant application with varying compute usage patterns. Some customers have predictable 24/7 usage, others have seasonal spikes, and some are small with unpredictable usage. Current monthly cost is $200,000 for EC2 compute (mix of m5, c5, r5 instances across us-east-1, eu-west-1, ap-southeast-1). They want to reduce costs by 30-40% while maintaining performance. Which THREE pricing strategies should they implement? (Select THREE)",
      "options": [
        "Purchase Compute Savings Plans for 50% of baseline compute usage with 1-year All Upfront commitment, providing up to 66% discount compared to On-Demand while maintaining flexibility across instance types and regions",
        "Use EC2 Instance Savings Plans for the most predictable workloads tied to specific instance families (m5, c5), which provides higher discounts (up to 72%) than Compute Savings Plans but less flexibility",
        "Implement Auto Scaling with Spot Instances for 30-40% of the workload capacity, using diversified instance types and On-Demand as fallback to handle Spot interruptions",
        "Purchase Regional Standard Reserved Instances for all predictable workloads in each region separately, using 3-year All Upfront terms for maximum discount",
        "Use AWS Cost Anomaly Detection with automated SNS alerts to identify and terminate unused resources, reducing waste by 20-30%",
        "Migrate all workloads to Graviton2-based instances (m6g, c6g, r6g) which provide 20% better price-performance than x86 instances"
      ],
      "correctAnswer": [0, 2, 5],
      "explanation": "Options 0, 2, and 5 are correct. Option 0: Compute Savings Plans provide the best balance of savings and flexibility for this scenario. Key benefits: 1) Applies to EC2 (any instance family, size, region, OS, tenancy), Fargate, and Lambda, 2) 1-year commitment provides ~40-54% discount (depending on upfront payment), 3-year provides ~50-66% discount, 3) Commit to 50% of baseline usage to cover predictable load, use On-Demand or Spot for variable load, 4) Cross-region flexibility helps for multi-region deployments. For $200K/month spend, committing $100K/month in Compute Savings Plans (50% of usage) at 50% discount saves $50K/month ($600K/year), achieving the 25% overall cost reduction goal. Option 2: Spot Instances provide 70-90% discount vs On-Demand for interruptible workloads. For SaaS applications, implement: 1) Use Spot for stateless web/app tiers with Auto Scaling, 2) Diversify across instance types and AZs to minimize interruption risk, 3) Use On-Demand or Savings Plans as baseline, Spot for scaling, 4) Implement graceful handling of Spot interruptions. Using Spot for 30% of workload at 80% discount saves $200K × 30% × 80% = $48K/month. Combined with Savings Plans, this achieves 30-40% total savings. Option 5: AWS Graviton2 instances (ARM-based) provide 20% better price-performance than comparable x86 instances. For example: m6g.xlarge costs ~10% less than m5.xlarge but provides similar or better performance. Benefits: 1) Lower cost per hour, 2) Better performance per dollar, 3) More sustainable (20% better energy efficiency). For many workloads (web servers, containerized apps, microservices), Graviton2 is drop-in compatible. Migration effort: recompile applications for ARM64 (many languages/frameworks support this natively), test compatibility. Savings: ~15-20% cost reduction through Graviton migration. Combined strategy: Savings Plans (25% reduction) + Spot (12% reduction from 30% of workload) + Graviton (10% reduction) ≈ 40% total cost reduction. Option 1 is valid but less optimal than Compute Savings Plans for this multi-region, variable workload scenario. EC2 Instance Savings Plans lock you into specific instance families (m5, c5, r5) in specific regions, reducing flexibility. If workload patterns change or you want to switch to Graviton or Fargate, Instance Savings Plans don't apply. Use Instance Savings Plans only when you're certain about instance family and region long-term. Option 3 (Regional Standard RIs) is similar to Instance Savings Plans but even less flexible - RIs lock you into specific instance types and sizes within a region. With Standard RIs, you can't change instance family or region. Given the variable workload and multi-tenant nature, this is too rigid. Option 4 (Cost Anomaly Detection) helps identify waste but doesn't provide the 30-40% savings target. It's useful for detecting anomalies (misconfigurations, unused resources), but the question asks for pricing strategies for active workloads. Key principle: Modern AWS cost optimization strategy: 1) Savings Plans (not RIs) for baseline predictable usage - offers flexibility for evolving architectures, 2) Spot Instances for variable/interruptible workloads, 3) Architecture optimization (Graviton, right-sizing, serverless), 4) Continuous optimization (Cost Explorer, Compute Optimizer, Trusted Advisor). For SaaS specifically: 1) Tag resources by customer/tenant for cost allocation, 2) Use Cost Allocation Tags and Cost Categories, 3) Implement chargeback/showback models, 4) Monitor cost per customer to identify unprofitable customers."
    },
    {
      "question": "A financial services company is migrating a 50 TB Oracle database to AWS. The database supports a trading application that requires < 1 second failover time and < 100ms latency. They're evaluating RDS for Oracle vs Aurora PostgreSQL (with database migration). The Oracle database uses Oracle RAC with active-active nodes, Advanced Compression, and Oracle Advanced Security. Which approach provides the MOST cost-effective solution while meeting technical requirements?",
      "options": [
        "Use RDS for Oracle Multi-AZ with Provisioned IOPS (io2) storage. This provides automatic failover in < 1 minute and supports all Oracle features including RAC, Advanced Compression, and Advanced Security",
        "Migrate to Aurora PostgreSQL using AWS DMS for ongoing replication and AWS SCT for schema conversion. Aurora provides < 30 second failover, and PostgreSQL extensions can replace most Oracle-specific features at 1/10th the cost",
        "Use Aurora PostgreSQL-Compatible with Babelfish, which provides Oracle compatibility layer allowing Oracle SQL and PL/SQL to run on PostgreSQL without code changes, at a fraction of RDS Oracle cost",
        "Deploy Oracle RAC on EC2 with Oracle Real Application Clusters across multiple AZs, using Amazon FSx for Oracle RAC shared storage. This provides < 1 second failover with active-active nodes"
      ],
      "correctAnswer": 3,
      "explanation": "Option D is correct. The requirements specify < 1 second failover time, which rules out managed database services (RDS, Aurora) that typically have failover times of 30-120 seconds. Oracle RAC on EC2 provides: 1) Active-active clustering with sub-second failover (client connection automatically redirects to surviving node), 2) Amazon FSx for Oracle RAC (launched 2024) provides shared storage compatible with Oracle RAC, 3) Support for all Oracle Enterprise Edition features (Advanced Compression, Advanced Security, Partitioning), 4) Full control over Oracle configuration and tuning. Trade-offs: 1) Higher operational overhead (you manage OS, Oracle installation, patching), 2) Higher cost than Aurora but lower than RDS Oracle Enterprise Edition with comparable licensing, 3) Requires Oracle RAC licenses (BYOL or License Included). Cost comparison for 50 TB: RDS Oracle EE ~$15-20K/month (instance + storage + licensing), Aurora PostgreSQL ~$3-5K/month (instance + storage, no licensing), EC2 Oracle RAC ~$8-12K/month (instances + FSx + Oracle BYOL licensing). While Aurora is cheaper, it doesn't meet the < 1 second failover requirement. Option A is incorrect - RDS for Oracle Multi-AZ does NOT support Oracle RAC. RDS Multi-AZ uses synchronous replication to a standby instance with automatic failover, but failover takes 60-120 seconds (DNS propagation, connection re-establishment). RDS also doesn't meet the < 1 second failover requirement. Additionally, Oracle Advanced Security and Advanced Compression are available in RDS Oracle EE but significantly increase cost. Option B is incorrect - while Aurora PostgreSQL is cost-effective and provides excellent performance, the failover time is 30-120 seconds (promoting a read replica to master), not < 1 second. More critically, migrating from Oracle to PostgreSQL is a significant undertaking: 1) Schema conversion using AWS SCT (may require manual adjustments for complex PL/SQL, Oracle-specific features), 2) Application code changes (SQL dialect differences, driver changes), 3) Testing and validation (months of effort), 4) Risk of migration issues for a critical trading application. Unless there's a strategic reason to migrate (avoiding Oracle licensing long-term), this is too risky for the immediate requirement. Option C is incorrect - Aurora PostgreSQL-Compatible with Babelfish is designed for SQL Server compatibility, not Oracle compatibility. Babelfish allows SQL Server T-SQL applications to run on Aurora PostgreSQL. For Oracle compatibility, AWS offers no direct equivalent. You would need to use Oracle-to-PostgreSQL migration with AWS SCT and DMS, which involves the challenges mentioned in Option B. Key principle: Database migration decision framework: 1) Technical requirements (failover time, latency, features) - eliminate options that don't meet requirements, 2) Risk tolerance (migration complexity, downtime, testing effort), 3) Cost (licensing, infrastructure, operational overhead), 4) Strategic direction (vendor lock-in, future flexibility). For Oracle workloads on AWS: 1) RDS Oracle for managed service with moderate performance requirements (failover 60-120s acceptable), 2) Oracle on EC2 with RAC for ultra-low failover requirements (< 1s), 3) Migrate to Aurora PostgreSQL for long-term cost savings (but significant migration effort), 4) Consider Aurora PostgreSQL for new applications to avoid Oracle licensing. For trading applications specifically: sub-second failover typically requires active-active or clustering solutions (Oracle RAC, SQL Server AlwaysOn Failover Cluster), not active-passive managed services."
    },
    {
      "question": "A media company uses Amazon S3 to store video files (500 TB total). Access patterns: 80% of requests are for videos uploaded in the last 30 days, 15% are for videos 30-90 days old, and 5% are for videos over 90 days old. Videos are kept for 5 years for compliance. Currently, all videos are in S3 Standard storage class, costing $11,500/month. What is the MOST cost-effective S3 lifecycle policy to reduce storage costs?",
      "options": [
        "Transition objects to S3 Intelligent-Tiering immediately upon upload. Intelligent-Tiering automatically moves objects between access tiers based on usage patterns, optimizing cost without lifecycle policies",
        "Transition objects to S3 Standard-IA after 30 days, S3 Glacier Instant Retrieval after 90 days, and S3 Glacier Deep Archive after 1 year. This aligns storage class with access patterns",
        "Transition objects to S3 One Zone-IA after 30 days and S3 Glacier Flexible Retrieval after 90 days. One Zone-IA provides 20% cost savings over Standard-IA for non-critical data",
        "Use S3 Lifecycle policy to transition all objects to S3 Glacier Deep Archive after 90 days since 95% of access is in the first 90 days, maximizing storage cost savings"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct and optimally aligns storage classes with access patterns and retrieval requirements. Here's the cost analysis (for 500 TB): 1) S3 Standard (first 30 days): $0.023/GB = $11,500/month for 500 TB, 2) S3 Standard-IA (days 30-90): $0.0125/GB = $6,250/month, 3) S3 Glacier Instant Retrieval (90 days - 1 year): $0.004/GB = $2,000/month, 4) S3 Glacier Deep Archive (1-5 years): $0.00099/GB = $495/month. With a lifecycle policy transitioning data through these tiers, the blended cost is significantly lower. Assuming even distribution of data age: (100TB × $23) + (100TB × $12.50) + (150TB × $4) + (150TB × $0.99) = $2,300 + $1,250 + $600 + $148.50 = $4,298.50/month, a 63% cost reduction. Key considerations: 1) S3 Standard-IA has minimum storage duration of 30 days and minimum object size of 128 KB - suitable for video files, 2) S3 Glacier Instant Retrieval provides millisecond retrieval (same as Standard) but costs 1/5th of Standard - perfect for 90-day to 1-year archive with occasional access, 3) S3 Glacier Deep Archive is cheapest ($1/TB/month) for long-term archive with rare access (retrieval takes 12-48 hours), 4) Lifecycle transitions are automatic and free (no retrieval or transition fees for lifecycle actions). Option A is incorrect for this scenario - S3 Intelligent-Tiering is beneficial when access patterns are unpredictable or change over time. For this media company, access patterns are predictable (time-based), making lifecycle policies more cost-effective. Intelligent-Tiering charges $0.0025 per 1,000 objects monitoring fee, which for millions of video files adds significant cost. Intelligent-Tiering tiers: Frequent Access ($0.023/GB, same as Standard), Infrequent Access ($0.0125/GB, same as Standard-IA), Archive Instant Access ($0.004/GB), and optional Archive/Deep Archive tiers. Since the access pattern is clearly time-based, explicit lifecycle policies provide better control and avoid monitoring fees. Option C is incorrect - S3 One Zone-IA stores data in a single AZ, reducing cost by 20% vs Standard-IA but losing the 99.99% availability and multi-AZ durability of Standard-IA (One Zone-IA is 99.5% availability, 99.999999999% durability but only if the AZ is available). For compliance and important video assets, multi-AZ durability is recommended. Also, S3 Glacier Flexible Retrieval (formerly Glacier) has retrieval times of 1-5 minutes (Expedited), 3-5 hours (Standard), or 5-12 hours (Bulk), which may be too slow if the 5% of old video access needs quick retrieval. Option D is incorrect - transitioning all objects to Glacier Deep Archive after 90 days would save maximum storage costs, but Deep Archive retrieval takes 12-48 hours, which is unacceptable for media serving. If a user requests a 4-month-old video, they'd have to wait 12+ hours. This violates user experience requirements. Deep Archive is only suitable for true archival with very rare access. Key principle: S3 storage class selection: 1) S3 Standard - active data, frequent access, 2) S3 Intelligent-Tiering - unpredictable access patterns, 3) S3 Standard-IA - infrequent access but immediate retrieval, 4) S3 One Zone-IA - non-critical, reproducible data, 5) S3 Glacier Instant Retrieval - archive with occasional immediate retrieval, 6) S3 Glacier Flexible Retrieval - archive with retrieval in minutes/hours, 7) S3 Glacier Deep Archive - long-term archive, retrieval in hours, 8) Use lifecycle policies for time-based transitions, Intelligent-Tiering for access-based transitions. For compliance retention: enable S3 Object Lock (WORM), S3 Versioning, and appropriate lifecycle policies to move old versions to cheaper storage."
    },
    {
      "type": "multiple",
      "question": "A manufacturing company wants to migrate their on-premises VMware environment (200 VMs, 50 TB total storage) to AWS. VMs run a mix of Windows Server 2016/2019 and Red Hat Enterprise Linux 7/8. They need to minimize downtime during migration (< 4 hours) and maintain application compatibility. Some applications have dependencies on specific on-premises services (Active Directory, DNS, NFS file shares). Which THREE components should be part of the migration strategy? (Select THREE)",
      "options": [
        "Use AWS Application Migration Service (MGN) for continuous block-level replication of VMs to AWS, allowing testing in AWS while on-premises systems remain running, then cutover with minimal downtime",
        "Implement AWS VM Import/Export to convert VMware VMDK files to AMIs, launch EC2 instances from AMIs, and configure networking and application dependencies",
        "Establish AWS Direct Connect or Site-to-Site VPN between on-premises and AWS VPC to maintain connectivity to on-premises Active Directory and NFS shares during and after migration",
        "Use AWS Server Migration Service (SMS) to automate VM replication and migration, creating AMIs for each VM that can be launched as EC2 instances",
        "Deploy AWS Managed Microsoft AD in AWS VPC with a trust relationship to on-premises Active Directory, allowing migrated Windows servers to authenticate against AWS AD while maintaining on-premises integration",
        "Migrate all VMs to VMware Cloud on AWS (VMC), which provides a native VMware environment in AWS, eliminating the need for VM conversion or application changes"
      ],
      "correctAnswer": [0, 2, 4],
      "explanation": "Options 0, 2, and 4 are correct. Option 0: AWS Application Migration Service (MGN, formerly CloudEndure Migration) is the current AWS-recommended tool for large-scale migrations. Key features: 1) Continuous replication of source servers (VMware VMs, physical servers) to AWS staging area, 2) Non-disruptive testing - launch test instances in AWS while replication continues, 3) Cutover with minimal downtime (minutes, not hours) - stop source server, final data sync, launch production instance in AWS, 4) Automated conversion from source to AWS-optimized instances, 5) No impact on source systems during replication. MGN replaced AWS SMS as the primary migration tool in 2022. For 200 VMs, MGN provides orchestrated migration waves, rollback capabilities, and detailed monitoring. Option 2: Hybrid connectivity is essential for dependencies on on-premises services. Direct Connect provides dedicated 1-10 Gbps connectivity (lower latency, more reliable than VPN), while Site-to-Site VPN provides encrypted connectivity over internet (faster to set up, lower cost). During migration: 1) Migrated VMs in AWS access on-premises AD, DNS, NFS over Direct Connect/VPN, 2) After migration, gradually migrate these shared services to AWS (AD using AWS Managed AD, NFS using Amazon FSx for NetApp ONTAP or EFS). This phased approach reduces risk - you don't have to migrate everything simultaneously. Option 4: AWS Managed Microsoft AD (AWS Directory Service for Microsoft Active Directory) is a managed AD service in AWS. For hybrid scenarios: 1) Deploy AWS Managed AD in AWS VPC, 2) Create a two-way trust relationship with on-premises AD, 3) Windows servers in AWS can join AWS Managed AD domain, 4) Users authenticate against AWS AD, which can forward authentication to on-premises AD if needed, 5) Gradually migrate users and resources to AWS AD. Benefits: reduced latency for authentication, resilience if on-premises AD is unavailable, and path to fully cloud-based AD. Alternative: AD Connector (lightweight proxy to on-premises AD, but less resilient). Option 1 is outdated - AWS VM Import/Export is a manual, one-time migration tool suitable for small migrations (< 10 VMs). Process: 1) Export VMDK from VMware, 2) Upload to S3, 3) Use VM Import to create AMI, 4) Launch EC2 instance. Limitations: 1) Manual process for each VM, 2) Downtime during export/import (hours per VM), 3) No continuous replication or testing capability, 4) Network and application configuration is manual post-migration. For 200 VMs, this is impractical. Use MGN instead. Option 3 mentions AWS Server Migration Service (SMS), which was deprecated in March 2022 and replaced by Application Migration Service (MGN). While SMS was suitable for VMware migrations, AWS now recommends MGN for all migration scenarios. SMS only created AMIs from replicated VMs; MGN provides more features (testing, orchestration, automation). Option 5 (VMware Cloud on AWS) is a valid migration path but not cost-optimal for most scenarios. VMC provides: 1) Native VMware environment (vSphere, vSAN, NSX) running on bare-metal EC2 instances, 2) Zero application changes - lift-and-shift VMs directly, 3) Use VMware tools and skills. However: 1) High cost - you pay for dedicated hosts and VMware licensing, 2) Best for organizations with large VMware investments, regulatory requirements for VMware, or hybrid cloud strategies, 3) For this scenario (200 VMs with no specific VMware requirement), migrating to EC2 with MGN is more cost-effective. Use VMC when: staying on VMware long-term, need hybrid on-premises/cloud VMware environment, or have VMware-specific tooling dependencies. Key principle: AWS migration strategy (6 R's): 1) Rehost (lift-and-shift) - MGN for VM migration to EC2, 2) Replatform (lift-tinker-shift) - migrate to managed services (RDS, ECS), 3) Refactor/Re-architect - modernize to serverless/containers, 4) Retire - decommission unused applications, 5) Retain - keep on-premises, 6) Repurchase - move to SaaS. For VMware migrations specifically: MGN for most scenarios, VMC for VMware-committed environments. Always establish hybrid connectivity first, migrate in waves (test with non-critical apps), validate functionality, then migrate critical apps."
    },
    {
      "question": "A global e-commerce company uses AWS Organizations with 50 accounts across Development, Staging, and Production environments. They receive a monthly AWS bill of $500,000 and want to implement cost allocation and chargeback to individual business units and teams. Cost Explorer shows that 40% of costs lack proper tagging. What is the MOST effective strategy to implement comprehensive cost allocation?",
      "options": [
        "Enable AWS Cost Categories to group costs by business unit and team based on account IDs, even without tags. Use Cost Category rules to allocate untagged resources based on account ownership",
        "Implement a Service Control Policy (SCP) that prevents launching any resources without required tags (CostCenter, BusinessUnit, Team), ensuring all future resources are properly tagged",
        "Use AWS Tag Policies in AWS Organizations to define required tag keys (CostCenter, BusinessUnit, Team) and enforce tag compliance across all accounts, combined with AWS Config Rules to detect non-compliant resources",
        "Export detailed billing data to S3 using AWS Cost and Usage Reports (CUR), import into Amazon QuickSight or Athena, and create custom allocation logic based on account, resource type, and usage patterns"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct and provides the most comprehensive solution. Here's how it works: 1) AWS Tag Policies (part of AWS Organizations) allow you to define required tag keys and allowed values. Example policy: require tags 'CostCenter', 'BusinessUnit', 'Team' with specific allowed values like 'CostCenter': ['CC-1001', 'CC-1002'], 2) Attach tag policies to OUs or accounts in your organization, 3) Tag policies are enforced during resource creation - if a user tries to create an EC2 instance without the required tags, the creation fails, 4) AWS Config Rules (managed rule 'required-tags') continuously monitor existing resources and flag non-compliant resources, 5) Use AWS Systems Manager Automation or custom Lambda to remediate non-compliant resources (add missing tags). This approach ensures: new resources are properly tagged (tag policies enforce at creation), existing resources are identified and remediated (Config Rules + automation), and compliance is continuously monitored. After implementing, enable Cost Allocation Tags in the billing console to include these tags in cost reports. Then use Cost Explorer to break down costs by tag (BusinessUnit, Team), enabling accurate chargeback. For the 40% of existing untagged resources: 1) Use AWS Resource Groups Tagging API to bulk-tag resources, 2) Use AWS Tag Editor (console tool) to find and tag resources, 3) Implement automation (Lambda + EventBridge) to tag resources based on naming conventions or account ownership. Option A is helpful but incomplete - AWS Cost Categories allow creating custom cost groupings based on account, tags, service, or other dimensions. You can create categories like 'Business Unit - Marketing' that includes specific accounts and tagged resources. This helps organize costs even when tagging is incomplete (you can use account ID as a proxy for business unit). However, it doesn't solve the root problem of untagged resources - it's a workaround. Cost Categories are best used IN ADDITION to proper tagging, not as a replacement. Option B is incorrect - SCPs control IAM permissions and API actions, but they CANNOT enforce tag requirements during resource creation. SCPs work at the account/OU level to allow or deny API actions (e.g., 'Deny ec2:RunInstances if tags are missing'), but the syntax is complex and doesn't work for all resource types. Tag Policies (Option C) are specifically designed for tag governance and enforcement. Option D is valid for analysis but doesn't solve the tagging problem - AWS Cost and Usage Reports (CUR) provide detailed billing data with resource IDs, tags, and usage metrics. You can import into Athena for SQL queries or QuickSight for visualization. This allows sophisticated cost analysis and allocation, even for untagged resources (allocate based on account, naming patterns, etc.). However, this is reactive analysis, not proactive governance. You're analyzing costs after they've been incurred, not preventing untagged resources from being created. Key principle: AWS cost allocation best practices: 1) Tag governance - use Tag Policies to enforce required tags, 2) Cost allocation tags - enable tags in billing console, 3) Cost Categories - create business-meaningful groupings, 4) Cost and Usage Reports - detailed analysis and chargeback, 5) Cost Explorer - visualize costs by dimension (tag, account, service), 6) Budgets and Alerts - prevent cost overruns. For chargeback specifically: 1) Define cost allocation methodology (direct attribution via tags, shared cost allocation for common services), 2) Automate reporting (QuickSight dashboards, automated emails), 3) Review and adjust (monthly reconciliation, tag compliance audits). For Organizations with many accounts: 1) Use account structure to reflect business units (one OU per business unit), 2) Tag policies at OU level for consistent enforcement, 3) Consolidated billing provides volume discounts, 4) RI/Savings Plan sharing across accounts in organization."
    },
    {
      "question": "A company is migrating a 20 TB SQL Server 2016 database from on-premises to AWS. The database supports a critical ERP application that must remain available 24/7. Maximum acceptable downtime is 1 hour. The database has 5% daily change rate (1 TB of changes per day). They're migrating to RDS SQL Server Enterprise Edition. Which migration approach provides the LEAST downtime?",
      "options": [
        "Create a database backup on-premises, upload to S3 using AWS DataSync, restore the backup to RDS SQL Server, then apply transaction log backups to catch up to current state",
        "Use AWS DMS with ongoing replication: configure DMS task with source endpoint (on-premises SQL Server) and target endpoint (RDS SQL Server), perform full load and CDC (Change Data Capture) for continuous replication, then cutover when synchronized",
        "Use SQL Server native backup/restore: create a full backup, upload to S3, restore to RDS using native restore feature, then use SQL Server transactional replication to keep on-premises and RDS synchronized until cutover",
        "Use AWS Snowball Edge to ship 20 TB database backup to AWS, restore to RDS SQL Server in AWS, then use DMS CDC to sync changes made during Snowball transit time"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct and provides the least downtime (minutes, not hours). AWS Database Migration Service (DMS) supports continuous replication for minimal downtime migrations: 1) Full Load Phase - DMS copies the entire 20 TB database from on-premises SQL Server to RDS SQL Server (this can take days, but application continues running on-premises), 2) Change Data Capture (CDC) Phase - DMS continuously replicates ongoing changes (using SQL Server transaction logs) from source to target, keeping them synchronized, 3) Testing Phase - application team tests against RDS while on-premises remains active, 4) Cutover - when ready, stop application, wait for final changes to replicate (typically seconds to minutes), change connection string to point to RDS, restart application. Total downtime is the time to replicate final changes plus DNS/application restart (< 30 minutes typically). DMS supports SQL Server 2008 and later, works with SQL Server Standard and Enterprise editions, handles schema and data migration, and monitors lag (tracking source vs target state). For a 20 TB database with 1 TB/day changes: initial full load might take 3-7 days (depending on bandwidth), then CDC keeps up with 1 TB/day (easy for DMS). Option A is insufficient for the 1-hour downtime requirement - the process: 1) Create full backup on-premises (hours for 20 TB), 2) Upload to S3 using DataSync or AWS CLI (hours to days depending on bandwidth - 20 TB at 1 Gbps = 44 hours), 3) Restore to RDS (hours for 20 TB), 4) Apply transaction log backups (additional hours). Total downtime would be 2-5 days, far exceeding the 1-hour limit. This approach is suitable for migrations where extended downtime is acceptable or for initial seeding before setting up replication. Option C is partially correct but more complex than DMS - SQL Server native backup/restore to RDS is supported (backup to S3, restore using RDS native restore), and SQL Server transactional replication can sync on-premises to RDS. However: 1) Transactional replication requires manual setup and configuration (publisher, distributor, subscriber), 2) Not all SQL Server editions support transactional replication (Standard has limitations), 3) Operational overhead to manage replication, 4) DMS is purpose-built for migrations and provides better monitoring, easier setup, and supports heterogeneous migrations (if you later want to migrate to Aurora PostgreSQL). For SQL Server to SQL Server migrations with minimal downtime, DMS is simpler. Option D adds unnecessary complexity - Snowball Edge is useful for transferring hundreds of TB when network bandwidth is limited (e.g., 100 TB at 100 Mbps would take 92 days; Snowball ships in 1 week). For 20 TB, direct network transfer is feasible: 20 TB at 1 Gbps = 44 hours, 20 TB at 10 Gbps (Direct Connect) = 4.4 hours. Using Snowball for initial load + DMS CDC for synchronization is a valid pattern for very large databases (> 100 TB), but for 20 TB, direct DMS full load + CDC is simpler. Also, Snowball adds time (order, ship, load, ship back) and cost. Key principle: Database migration strategies by downtime tolerance: 1) Minimal downtime (< 1 hour) - Use DMS with CDC, or native replication (SQL Server transactional replication, PostgreSQL logical replication, MySQL binlog replication), 2) Moderate downtime (hours) - Backup/restore with transaction log shipping, 3) Extended downtime (days) - Simple backup/restore, 4) Heterogeneous migrations (SQL Server to Aurora PostgreSQL) - Use AWS SCT + DMS. For large databases (> 10 TB): 1) Seed with Snowball or initial backup/restore, then use CDC to catch up, 2) Increase DMS replication instance size for performance (dms.r5.4xlarge or larger), 3) Use multiple DMS tasks in parallel (table-level parallelization), 4) Monitor DMS task metrics (CDCLatencySource, CDCLatencyTarget). For SQL Server specifically on AWS: 1) RDS SQL Server for managed service (limited to 16 TB storage, max instance size), 2) SQL Server on EC2 for larger databases or AlwaysOn Availability Groups, 3) Consider Aurora PostgreSQL with Babelfish for long-term cost savings (but significant migration effort)."
    },
    {
      "type": "multiple",
      "question": "A fintech startup runs their application on AWS with variable workload (100 EC2 instances during business hours, 20 instances overnight). Monthly EC2 cost is $50,000 (On-Demand pricing). They're early-stage with uncertain growth trajectory and may pivot product direction in 6 months. They want to reduce costs by 30% without long-term commitments. Which THREE cost optimization strategies are MOST appropriate for this scenario? (Select THREE)",
      "options": [
        "Use Instance Scheduler on AWS to automatically stop non-production instances overnight and on weekends, reducing runtime by 65% for development and testing environments",
        "Implement Auto Scaling with Spot Instances for 50% of the production fleet, using On-Demand as baseline and Spot for peak demand with diversified instance types to minimize interruption risk",
        "Purchase 1-year Compute Savings Plans with No Upfront payment option for baseline capacity (20 instances), providing ~40% discount with flexibility to change instance types as the product evolves",
        "Use EC2 Instance Savings Plans with All Upfront payment for 100 instances to maximize discount (up to 72%), reducing costs while locking in capacity for current architecture",
        "Enable AWS Compute Optimizer recommendations and right-size instances based on CloudWatch metrics, moving oversized instances to smaller types (e.g., m5.2xlarge to m5.xlarge)",
        "Containerize applications and migrate to ECS Fargate with AWS Fargate Spot, reducing costs by 70% for interruptible tasks while eliminating instance management overhead"
      ],
      "correctAnswer": [0, 1, 4],
      "explanation": "Options 0, 1, and 4 are correct for this early-stage, uncertain-growth scenario. Option 0: Instance Scheduler is a serverless solution (Lambda + CloudWatch Events) that automatically starts and stops instances on a schedule. For non-production environments (dev, test, staging): 1) Run only during business hours (9 AM - 6 PM on weekdays = 45 hours/week vs 168 hours/week = 73% reduction in runtime), 2) For 30 non-production instances at $500/month each On-Demand, running 45 hours/week vs 168 hours/week saves $500 × 30 × 73% = $10,950/month. This requires no long-term commitment and can be changed instantly. Use AWS Instance Scheduler solution or custom Lambda functions. Ensure development teams are aware of schedule and can manually start instances if needed. Option 1: Spot Instances provide 70-90% discount vs On-Demand for interruptible workloads. For stateless application tiers (web servers, API servers, batch processing): 1) Use Auto Scaling Groups with mixed instance types (m5, m5a, m5n, c5, c5a) to maximize Spot availability, 2) Set On-Demand baseline (50% of capacity) and Spot for scaling (50% of capacity), 3) Configure ASG to handle Spot interruptions gracefully (2-minute warning, drain connections, rebalance). For 100 instances, using 50 Spot at 80% discount saves $50,000 × 50% × 80% = $20,000/month. No long-term commitment, and you can adjust mix anytime. Option 4: AWS Compute Optimizer analyzes CloudWatch metrics (CPU, memory, network, disk) and recommends right-sized instances. Common finding: many instances are oversized (running at 10-20% CPU utilization). Downsizing from m5.2xlarge ($0.384/hour) to m5.xlarge ($0.192/hour) saves 50%. For 30 oversized instances, this saves $0.192 × 30 × 730 hours/month = $4,204/month. This optimization requires no commitment and can be reversed if workload increases. Also consider: using AWS Graviton2 instances (m6g, c6g) for 20% better price-performance. Option 2 is risky for this scenario - while Compute Savings Plans with No Upfront payment provide flexibility (can change instance types, regions), committing to 1-year term when the company may pivot in 6 months is dangerous. If they pivot and reduce infrastructure or change architecture (serverless, containers), they're still paying for the commitment. For uncertain scenarios, avoid commitments > 6 months. Option 3 is worse - EC2 Instance Savings Plans lock you into specific instance families and regions, and All Upfront payment requires paying the full 1-year cost upfront. If the company pivots or shuts down infrastructure, the prepaid amount is wasted (non-refundable). This is the opposite of what an early-stage startup with uncertain growth should do. Option 5 is beneficial but not optimal for the 'no long-term commitment' constraint. Containerizing applications and migrating to Fargate is a significant engineering effort (weeks to months), and Fargate Spot provides great savings, but this is a strategic architectural change, not a quick cost optimization. Consider this for long-term roadmap, but for immediate cost reduction without commitment, Options 0, 1, 4 are faster to implement. Combined savings: Instance Scheduler ($10,950) + Spot Instances ($20,000) + Right-sizing ($4,204) = $35,154/month savings = 70% cost reduction, exceeding the 30% target. Key principle: Cost optimization by company stage: 1) Early-stage startup (uncertain growth, may pivot) - Use On-Demand, Spot, right-sizing, schedulers; avoid long-term commitments, 2) Growth-stage (predictable growth, product-market fit) - Use 1-year Savings Plans, moderate Reserved Instances, reserved capacity for baseline, 3) Mature enterprise (stable workloads, long-term planning) - Use 3-year Savings Plans, All Upfront for maximum discount, capacity reservations. For startups specifically: 1) AWS Activate credits (up to $100K for VC-backed startups), 2) Focus on variable costs (pay for what you use), 3) Automate cost optimization (schedulers, Auto Scaling, right-sizing), 4) Monitor burn rate (AWS Budgets with alerts), 5) Use managed services to reduce operational overhead (RDS, Lambda, Fargate)."
    },
    {
      "question": "A media processing company uses AWS Glue to run ETL jobs that transform video metadata (extracting thumbnails, analyzing content). Jobs run daily and process 10 TB of data stored in S3. Current monthly Glue cost is $15,000 (using 100 DPUs for 5 hours/day). AWS Cost Optimizer recommends reducing DPUs, but when they tested with 50 DPUs, job duration increased to 12 hours, exceeding the daily batch window. What is the MOST cost-effective optimization?",
      "options": [
        "Keep 100 DPUs but enable Glue Auto Scaling, which dynamically adjusts DPUs based on workload, reducing DPUs during low-processing phases of the job",
        "Migrate from Glue to AWS Batch with Spot Instances running custom Docker containers with Apache Spark, reducing compute costs by 70% compared to Glue DPUs",
        "Implement Glue job optimization: partition input data, enable Glue job bookmarks to avoid reprocessing, use columnar format (Parquet) for intermediate data, and optimize Spark configuration to reduce job duration with fewer DPUs",
        "Use Glue Flex execution class (introduced 2022), which uses Spot pricing for Glue jobs and can reduce costs by up to 35% compared to standard DPUs, with minimal code changes"
      ],
      "correctAnswer": 3,
      "explanation": "Option D is correct. AWS Glue Flex is a new execution class (introduced in late 2022) designed for cost-sensitive, non-urgent data integration workloads. Key features: 1) Uses Spot-like pricing (~35% cheaper than standard Glue DPUs), 2) Jobs may take longer to start (up to several minutes delay) due to resource provisioning, 3) Jobs may be interrupted (rarely, but possible), requiring retry logic, 4) Ideal for batch ETL jobs with flexible completion times (not time-critical), 5) Simple to enable - just set ExecutionClass parameter to 'FLEX' in job configuration. For this scenario: $15,000/month with standard DPUs becomes ~$9,750/month with Flex (35% reduction), with no code changes required. The daily batch window constraint (job must complete in 24 hours) is still met since Flex affects startup time, not necessarily execution time. For 5-hour jobs with some startup delay, Flex still completes within the batch window. Trade-offs: 1) Longer start time (jobs may wait for capacity), 2) Potential interruptions (rare, but implement retry logic), 3) Not suitable for real-time or time-critical jobs. This is the easiest, most cost-effective solution for this scenario. Option A is partially helpful - Glue Auto Scaling dynamically adjusts DPUs during job execution based on workload. However, it's designed to scale UP when needed (adding DPUs during high parallelism phases), not necessarily to reduce costs. For batch jobs with consistent workload, Auto Scaling may not reduce costs significantly. Auto Scaling is more beneficial for workloads with variable processing phases (e.g., large data shuffle followed by light aggregation). Option B is valid but requires significant engineering effort - AWS Batch with Spot Instances provides cost savings (70% discount for Spot vs On-Demand EC2), but: 1) You manage Spark infrastructure (Docker images, Spark configuration, cluster management), 2) Glue abstracts this complexity with managed Spark clusters, 3) Development and operational overhead increases. For a team already using Glue, migrating to Batch is a strategic decision requiring time and resources. For immediate cost reduction, Glue Flex is simpler. Use Batch when: you need more control over Spark configuration, have Spark expertise in-house, or have very large-scale processing where custom optimization outweighs managed service benefits. Option C is best practice optimization but doesn't directly address the cost constraint given - they already tested with fewer DPUs and it didn't meet the batch window. However, these optimizations are valuable: 1) Partitioning input data (S3 prefix partitioning) allows Spark to parallelize reads, 2) Glue job bookmarks track processed data to avoid reprocessing, reducing data volume, 3) Parquet format provides columnar storage and compression, reducing I/O, 4) Spark configuration tuning (shuffle partitions, executor memory, parallelism) can improve performance. These optimizations can reduce job duration, allowing use of fewer DPUs, but they require development and testing effort. Implement these AFTER enabling Flex for quick wins. Key principle: AWS Glue cost optimization: 1) Use Glue Flex for batch jobs (35% cost reduction), 2) Enable job bookmarks to process only new data, 3) Partition data for efficient processing, 4) Right-size DPUs based on job requirements, 5) Use G.1X workers for general workloads, G.2X for memory-intensive, 6) Monitor job metrics (ExecutionTime, MaxCapacity) and optimize, 7) Schedule jobs during off-peak hours if applicable. Glue pricing: Standard DPU = $0.44/hour, Flex DPU = ~$0.29/hour. For large-scale ETL (> 50 TB/day), consider: 1) AWS Glue with optimized Spark jobs, 2) Amazon EMR with Spot Instances for more control and lower cost, 3) Amazon Athena for query-based transformations (serverless, pay per query), 4) AWS Lake Formation for data lake management. For video metadata processing specifically: consider Amazon Rekognition for content analysis (managed service), AWS Elemental MediaConvert for video processing, and Glue/EMR for metadata ETL."
    },
    {
      "question": "A healthcare provider migrated their PACS (Picture Archiving and Communication System) to AWS, storing 800 TB of medical images in S3 Standard. Compliance requires keeping images for 7 years. Access pattern: 90% of image retrievals are for images < 1 year old, and retrieval must be immediate (< 1 second). Older images are accessed rarely (1-2 times per year) but must be available within 1 hour when needed. Current storage cost is $18,400/month. What is the MOST cost-effective S3 lifecycle strategy?",
      "options": [
        "Transition to S3 Intelligent-Tiering immediately, which automatically moves images to Infrequent Access tier after 30 days and Archive Instant Access tier after 90 days, with immediate retrieval for all tiers",
        "Transition images to S3 Standard-IA after 90 days and S3 Glacier Flexible Retrieval after 1 year. Use Expedited retrieval (1-5 minutes) for urgent access to old images",
        "Transition images to S3 Glacier Instant Retrieval after 1 year, which provides millisecond retrieval like S3 Standard but at 68% lower storage cost",
        "Transition images to S3 One Zone-IA after 1 year to reduce storage costs by 20% compared to Standard-IA, since PACS images can be regenerated from source systems if an AZ failure occurs"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct and optimally balances cost with retrieval requirements. S3 Glacier Instant Retrieval provides: 1) Immediate retrieval (milliseconds, same as S3 Standard and Standard-IA), 2) 68% lower storage cost than S3 Standard ($0.004/GB vs $0.023/GB for S3 Standard), 3) Minimum storage duration of 90 days, 4) Retrieval fee of $0.03 per GB (but for 1-2 retrievals per year of old images, this is minimal). Cost breakdown for 800 TB with 1-year lifecycle: 1) Year 1 images (120 TB, assuming even distribution): S3 Standard at $0.023/GB = $2,760/month, 2) Years 2-7 images (680 TB): S3 Glacier Instant Retrieval at $0.004/GB = $2,720/month, 3) Total: $5,480/month vs current $18,400/month = 70% cost reduction. Retrieval costs: If 1% of old images are retrieved per month (6.8 TB), cost = 6,800 GB × $0.03 = $204/month, still far cheaper than keeping all images in Standard. This meets both requirements: immediate retrieval for recent images (< 1 year in Standard) and immediate retrieval for old images when needed (Glacier Instant Retrieval). Option A is incorrect for this use case - S3 Intelligent-Tiering is beneficial for unpredictable access patterns, but this PACS system has predictable time-based access (age-based). Intelligent-Tiering costs: 1) Storage: $0.023/GB (Frequent Access tier, same as Standard), 2) Monitoring fee: $0.0025 per 1,000 objects, 3) For millions of medical images, monitoring fees add up. Tiers: Frequent Access ($0.023/GB), Infrequent Access ($0.0125/GB, after 30 days of no access), Archive Instant Access ($0.004/GB, after 90 days of no access). While Archive Instant Access tier pricing is good, the monitoring fees and the unpredictability of tier placement make explicit lifecycle policies more cost-effective for time-based access patterns. Option B has a critical flaw - S3 Glacier Flexible Retrieval (formerly S3 Glacier) does NOT provide immediate retrieval. Retrieval times: 1) Expedited: 1-5 minutes, $0.03/GB, limited capacity, 2) Standard: 3-5 hours, $0.01/GB, 3) Bulk: 5-12 hours, $0.0025/GB. The requirement is retrieval within 1 hour, and while Expedited meets this (1-5 minutes), Expedited retrievals are not guaranteed (capacity-limited) and cost more. Also, users would need to wait minutes, not immediate access. For medical imaging where clinicians need immediate access, this is unacceptable. Option D is incorrect and risky - S3 One Zone-IA stores data in a single Availability Zone, providing 99.5% availability (vs 99.99% for multi-AZ storage classes) and 99.999999999% durability ONLY if the AZ remains available. If the AZ is lost (rare but possible), data is lost. For medical images required for compliance and patient care, multi-AZ durability is essential. The statement 'images can be regenerated from source systems' is false for PACS - images are captured from medical devices (X-ray, MRI, CT scan) and cannot be regenerated. Loss of medical images violates HIPAA and patient care standards. Never use One Zone-IA for irreplaceable data. Key principle: S3 storage class selection for archival with retrieval requirements: 1) Immediate retrieval (milliseconds) - S3 Standard (frequent access), S3 Standard-IA (infrequent access), S3 Glacier Instant Retrieval (rare access, long retention), 2) Retrieval in minutes to hours - S3 Glacier Flexible Retrieval (archive with occasional access), 3) Retrieval in hours - S3 Glacier Deep Archive (long-term archive, very rare access), 4) Use lifecycle policies for time-based transitions, 5) Always use multi-AZ storage classes for critical data. For HIPAA compliance specifically: 1) Encrypt at rest (S3 default encryption with KMS), 2) Encrypt in transit (TLS), 3) Access logging (S3 server access logs or CloudTrail data events), 4) Versioning and MFA Delete for data protection, 5) Retain audit logs for 7 years, 6) Use S3 Object Lock for immutable archives (WORM compliance mode). S3 Glacier Instant Retrieval is specifically designed for medical imaging, compliance archives, and media assets requiring long-term retention with rare but immediate access."
    },
    {
      "type": "multiple",
      "question": "A financial services company is migrating a complex application to AWS. The application consists of: 50 Windows VMs (web/app tiers), 10 Linux VMs (batch processing), 5 Oracle RAC databases (500 GB each), and dependencies on NetApp NFS storage (20 TB). They want to modernize during migration to reduce operational overhead and costs. Which THREE migration and modernization strategies should they implement? (Select THREE)",
      "options": [
        "Migrate Windows VMs to Windows containers on ECS Fargate, eliminating VM management overhead while maintaining Windows compatibility for legacy .NET Framework applications",
        "Migrate Linux batch processing VMs to AWS Lambda functions triggered by EventBridge schedules, reducing costs by 60% and eliminating server management",
        "Migrate Oracle RAC databases to Amazon RDS for Oracle Multi-AZ, which provides managed Oracle with automated backups, patching, and failover, reducing operational overhead",
        "Migrate Oracle databases to Aurora PostgreSQL using AWS DMS and AWS SCT, eliminating Oracle licensing costs (saving ~70%) while modernizing to a cloud-native database",
        "Migrate NetApp NFS storage to Amazon FSx for NetApp ONTAP, providing fully managed NFS with enterprise features (snapshots, replication, data tiering) and seamless integration with existing applications",
        "Migrate all workloads to VMware Cloud on AWS to maintain existing VMware operational model and tools, then gradually modernize individual components over 2-3 years"
      ],
      "correctAnswer": [2, 3, 4],
      "explanation": "Options 2, 3, and 4 provide balanced modernization while reducing operational overhead. Option 2: RDS for Oracle Multi-AZ provides managed Oracle database service with: 1) Automated backups (point-in-time recovery up to 35 days), 2) Automated patching (with configurable maintenance windows), 3) Multi-AZ failover (automatic failover in 60-120 seconds), 4) Monitoring with Performance Insights, 5) Encryption at rest and in transit. Trade-offs: 1) RDS Oracle does NOT support Oracle RAC (no active-active clustering), 2) Failover time is 60-120 seconds (vs sub-second for RAC), 3) RDS has storage limits (64 TB max), 4) No OS-level access. If the application can tolerate 60-120 second failover (many financial applications can with proper application retry logic), RDS significantly reduces operational overhead compared to managing Oracle RAC on EC2 or on-premises. Oracle licensing: RDS supports both License Included and BYOL (Bring Your Own License). This is a 'replatform' strategy (lift-tinker-shift). Option 3: Migrating from Oracle to Aurora PostgreSQL is a significant modernization effort but provides maximum long-term benefits: 1) Cost savings: eliminate Oracle licensing (which can be $10K-50K per database per year), 2) Aurora PostgreSQL pricing: pay for storage and compute, no licensing fees, 3) Cloud-native features: Aurora Global Database, automated backups, fast cloning, performance at scale, 4) Open-source: PostgreSQL is open-source, avoiding vendor lock-in. Migration process: 1) Use AWS Schema Conversion Tool (SCT) to convert Oracle schema (tables, views, procedures, functions) to PostgreSQL, 2) Manual review and adjustment for complex PL/SQL (SCT converts 80-95% automatically), 3) Use AWS DMS for data migration with CDC (continuous replication), 4) Application code changes for SQL dialect differences (minimal for well-architected apps using ORMs), 5) Testing and validation (critical - months of effort). This is a 'refactor' strategy with high effort but high reward. For 5 Oracle databases at 500 GB each (small databases), migration effort is moderate. Option 4: Amazon FSx for NetApp ONTAP provides fully managed NetApp file storage in AWS: 1) Native NFS and SMB protocols (drop-in replacement for NetApp), 2) Enterprise features: snapshots, replication, data tiering (to S3), deduplication, compression, 3) Integration with existing applications (no code changes), 4) Managed service: AWS handles patching, backups, availability, 5) Multi-AZ for high availability. This is a 'replatform' strategy - maintain NFS functionality but on managed infrastructure. For 20 TB NFS, FSx for NetApp ONTAP provides cost-effective, managed storage. Alternative: Amazon EFS (simpler, lower cost) if advanced NetApp features are not needed. Option 0 is technically possible but adds significant complexity - while Windows containers are supported on ECS Fargate (launched 2021), migrating legacy .NET Framework applications to containers requires: 1) Containerizing applications (Dockerfile, base images), 2) Testing for compatibility (not all .NET Framework apps work in containers), 3) Refactoring for 12-factor app principles (stateless, externalized configuration), 4) Operational changes (container orchestration, logging, monitoring). For legacy Windows apps, the easier modernization path is: 1) Rehost to EC2 Windows (using Application Migration Service), 2) Gradually refactor to .NET Core / .NET 6+ (cross-platform), 3) Then containerize with ECS or EKS. Trying to containerize during initial migration adds too much risk and effort. Option 1 is incorrect and oversimplified - while Lambda is excellent for event-driven computing, not all batch processing workloads are suitable for Lambda: 1) Lambda has 15-minute maximum execution time (batch jobs may run hours), 2) Lambda has memory limits (10 GB max), 3) Lambda is best for short-duration, stateless tasks. For long-running batch processing, better options: 1) AWS Batch with Spot Instances (managed batch computing), 2) ECS Fargate with scheduled tasks, 3) Step Functions orchestrating multiple Lambda functions for long workflows. Assess batch job characteristics before choosing Lambda. Option 5 (VMware Cloud on AWS) maintains status quo and delays modernization - while VMC provides familiarity, it's expensive and doesn't reduce operational overhead. Use VMC only for: massive VMware estate (thousands of VMs), regulatory requirements for VMware, hybrid cloud strategy. For this scenario (50 Windows VMs + 10 Linux VMs), migrating to native AWS services provides better long-term value. Key principle: Cloud migration strategies (6 R's): 1) Rehost (lift-and-shift) - fastest migration, minimal changes (Application Migration Service to EC2), 2) Replatform (lift-tinker-shift) - minimal changes with cloud benefits (RDS, FSx), 3) Refactor - re-architect for cloud-native (serverless, containers, Aurora), 4) Retire - decommission unused apps, 5) Retain - keep on-premises for now, 6) Repurchase - move to SaaS. Balance: quick wins with rehost/replatform (80% of apps), strategic refactor for high-value apps (20%). For financial services specifically: prioritize security (encryption, compliance), availability (multi-AZ, backups), and auditability (CloudTrail, logging)."
    },
    {
      "question": "A data analytics company uses Amazon Redshift with 20 dc2.8xlarge nodes (160 vCPUs, 1280 GB RAM total) for their data warehouse. Monthly Redshift cost is $40,000. Usage pattern: heavy querying during business hours (9 AM - 6 PM), minimal usage overnight and weekends. They want to reduce costs by 40-50% while maintaining query performance during business hours. What is the MOST effective approach?",
      "options": [
        "Use Redshift Pause and Resume feature to manually pause the cluster overnight and on weekends, reducing costs by 65% (cluster only billed when running)",
        "Migrate to Redshift Serverless (preview/GA), which automatically scales compute based on workload and pauses during idle periods, providing pay-per-query pricing instead of paying for 24/7 cluster",
        "Implement Redshift Concurrency Scaling, which adds transient compute capacity only during peak hours for read queries, reducing the need for a large base cluster",
        "Use Redshift Reserved Nodes with 1-year All Upfront commitment to get 40% discount on the existing cluster size, maintaining 24/7 availability at lower cost"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct. Amazon Redshift Serverless (GA in July 2022) is specifically designed for this use case: variable, intermittent workloads. Key features: 1) No cluster provisioning - AWS manages compute capacity automatically, 2) Auto-scaling - scales compute up during peak hours, down during low usage, 3) Auto-pause - automatically pauses during idle periods (no queries for a configurable duration), 4) Pay-per-use pricing - charged for RPUs (Redshift Processing Units) consumed, not for 24/7 cluster, 5) Instant resume - queries trigger automatic resume from paused state. Cost model: ~$0.36 per RPU-hour. For this scenario: business hours usage (9 AM - 6 PM weekdays = 45 hours/week) vs 24/7 cluster (168 hours/week) = 73% reduction in runtime. Even accounting for peak-hour scaling, costs would be 50-60% lower than a 24/7 provisioned cluster. Serverless also eliminates cluster management (sizing, scaling, pause/resume automation). Trade-offs: 1) Paused clusters take 10-30 seconds to resume (first query after idle period has latency), 2) Less control over cluster configuration compared to provisioned clusters, 3) May cost more than Reserved Nodes for 24/7 workloads. For intermittent analytics, Serverless is optimal. Option A is valid but requires automation and has operational overhead - Redshift Pause/Resume allows manually pausing clusters to stop billing for compute (storage still billed). However: 1) Manual pause/resume is operationally intensive (need to remember to pause/resume daily), 2) Requires automation (Lambda + EventBridge to pause at 6 PM, resume at 9 AM), 3) Resume takes 3-5 minutes, delaying morning queries, 4) If someone forgets to pause, costs continue. While this can achieve 65% cost savings (billing only during 45 hours/week), Redshift Serverless provides the same benefit with automatic pause/resume and no operational overhead. Option C is incorrect for this scenario - Redshift Concurrency Scaling adds temporary capacity for read queries during peak usage to handle concurrent user queries without queuing. It's designed for workload spikes (Black Friday, month-end reporting), not for reducing costs on underutilized clusters. Concurrency Scaling is billed per-second for additional capacity used, so it adds cost rather than reducing it. You'd still need the base 20-node cluster running 24/7. Concurrency Scaling is valuable for improving query performance during peaks, not for cost reduction on idle clusters. Option D reduces cost but doesn't address the core issue - Reserved Nodes provide 40-75% discount (depending on term and payment option) for committing to 1-year or 3-year terms. 1-year All Upfront provides ~40% discount, 3-year All Upfront provides ~75% discount. However, you're still paying for a 24/7 cluster even during idle periods. For $40K/month with 40% discount = $24K/month. Compare to Serverless at 50-60% reduction = $16-20K/month. Reserved Nodes make sense for workloads with 24/7 usage, not intermittent usage. Key principle: Redshift cost optimization by usage pattern: 1) Intermittent usage (business hours only) - Use Redshift Serverless for automatic scaling and pause, 2) 24/7 usage with predictable load - Use Reserved Nodes for maximum discount, 3) Variable load with peak spikes - Use Concurrency Scaling for peak queries + right-sized base cluster, 4) Development/test clusters - Use pause/resume or Serverless. Other Redshift cost optimizations: 1) Use RA3 nodes with managed storage (separate compute and storage, cheaper for large datasets), 2) Implement Redshift Spectrum to query S3 directly without loading data (reduce cluster size), 3) Optimize table design (sort keys, distribution keys, compression), 4) Use Materialized Views for frequently accessed aggregations, 5) Implement Automatic Table Optimization (ATO) for automated maintenance. For analytics workloads with intermittent usage, also consider: 1) Amazon Athena (serverless SQL on S3, pay per query), 2) Amazon EMR with Spot Instances (managed Hadoop/Spark, cost-effective for batch processing), 3) Redshift Serverless for structured data warehousing. Redshift Serverless pricing: Base capacity (e.g., 128 RPUs) costs ~$5.76/hour. For 45 hours/week usage = $1,036/month vs $40K/month for 24/7 cluster."
    },
    {
      "question": "A global logistics company operates AWS workloads in us-east-1, eu-west-1, and ap-southeast-1 with monthly data transfer costs of $80,000. Analysis shows: 40% of data transfer is between regions (cross-region), 30% is to the internet (CloudFront to users), and 30% is within the same region (inter-AZ). They want to reduce data transfer costs by 30-40%. Which combination of strategies would provide the MOST cost savings?",
      "options": [
        "Enable VPC endpoints (PrivateLink) for all AWS service access (S3, DynamoDB, SQS) to eliminate data transfer charges for accessing these services over the internet",
        "Deploy AWS Global Accelerator to reduce internet data transfer costs by routing traffic through AWS's private network instead of the public internet",
        "Use CloudFront with Regional Edge Caches and Origin Shield to reduce origin fetches from S3/ALB, and consolidate workloads to fewer regions to minimize cross-region data transfer",
        "Implement AWS Transit Gateway with centralized internet egress in one region (us-east-1) and route all outbound internet traffic through that region to take advantage of volume discounts"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct and addresses both major cost drivers: cross-region transfer and internet egress. Strategy 1: CloudFront optimization: 1) CloudFront caching reduces origin fetches - with good cache hit ratios (80-90%), you reduce data transfer from origins (S3, ALB) by 80-90%, 2) CloudFront data transfer pricing is tiered and cheaper than direct S3/EC2 internet egress ($0.085/GB for CloudFront vs $0.09/GB for EC2 internet egress in us-east-1), 3) Regional Edge Caches (intermediate caches between edge locations and origins) further reduce origin fetches for less popular content, 4) Origin Shield (CloudFront feature) provides an additional caching layer before origins, collapsing requests from multiple edge locations into single origin requests. Impact: 30% internet egress ($24K/month) with 80% cache hit ratio reduces origin fetches by 80%, saving ~$19K/month. Strategy 2: Reduce cross-region transfer: 1) Consolidate workloads - if eu-west-1 and ap-southeast-1 are serving small user bases, consider serving them from us-east-1 via CloudFront (global CDN), 2) Architect applications to minimize cross-region calls (e.g., use regional data stores, replicate data instead of fetching cross-region), 3) Use S3 Transfer Acceleration for uploads (reduces cross-region data movement), 4) Implement caching layers (ElastiCache, CloudFront) to reduce repeated data fetches. Impact: Reducing 40% cross-region transfer ($32K/month) by 50% saves $16K/month. Total savings: $19K + $16K = $35K/month = 44% reduction. Option A is incorrect in its premise - VPC endpoints (PrivateLink) provide private connectivity to AWS services, but data transfer pricing doesn't change significantly: 1) VPC endpoint pricing: $0.01/hour + $0.01/GB processed, 2) S3 and DynamoDB have Gateway VPC endpoints (free, no data transfer charges within the same region), 3) Most other services use Interface VPC endpoints (PrivateLink) with per-GB charges. The benefit of VPC endpoints is security/privacy (no internet gateway), not cost savings. For S3/DynamoDB, Gateway endpoints can eliminate NAT Gateway costs ($0.045/GB for NAT Gateway processing), but the question states data transfer is between regions and to internet, not within-region to S3/DynamoDB. Option B is incorrect - AWS Global Accelerator routes traffic through AWS's private global network for performance (lower latency, better availability), but it does NOT reduce data transfer costs. In fact, Global Accelerator adds cost: $0.025/hour per accelerator + $0.015/GB data transfer premium (in addition to standard data transfer costs). Global Accelerator is for improving user experience (latency, failover), not for cost reduction. Option D is incorrect and would likely increase costs - centralized internet egress (routing all outbound traffic through one region) creates complexity and adds costs: 1) Cross-region data transfer from eu-west-1/ap-southeast-1 to us-east-1 costs $0.02/GB, 2) Then internet egress from us-east-1 costs $0.09/GB, 3) Total: $0.11/GB vs direct internet egress from each region ($0.09/GB). This increases costs by 22%. Volume discounts for data transfer exist but require very high volumes (petabytes/month) and don't offset the cross-region transfer costs. Key principle: AWS data transfer cost optimization: 1) Cross-region transfer ($0.02/GB) - minimize by: architecting for regional autonomy, caching, replication instead of fetching, 2) Internet egress ($0.09/GB for first 10 TB, tiered down to $0.05/GB for > 500 TB) - minimize by: CloudFront caching, compression, efficient data formats, 3) Inter-AZ transfer ($0.01/GB in/out per AZ) - minimize by: single-AZ for non-critical workloads (not recommended for production), use VPC endpoints for AWS services, 4) Inbound data transfer (free in most cases). Specific tactics: 1) Use CloudFront aggressively (high TTLs, large cache, Origin Shield), 2) Compress data (Gzip, Brotli) for internet transfer, 3) Use efficient formats (Parquet instead of JSON for large datasets), 4) Architect for regional data locality (users in Europe access eu-west-1 resources, not us-east-1), 5) Monitor data transfer with Cost Explorer (filter by usage type: DataTransfer-Regional, DataTransfer-Out, etc.). For global applications: 1) Multi-region active-active with regional data stores, 2) CloudFront for content delivery, 3) Route 53 latency-based routing to direct users to nearest region, 4) Minimize cross-region API calls (replicate data instead). CloudFront pricing tiers (per GB): $0.085 (first 10 TB), $0.080 (10-50 TB), $0.060 (50-150 TB), $0.040 (150-500 TB), decreasing further for higher volumes."
    },
    {
      "type": "multiple",
      "question": "A SaaS company is rightsizing their AWS infrastructure to reduce costs. AWS Compute Optimizer recommends downsizing 100 EC2 instances from m5.2xlarge to m5.xlarge (50% cost reduction per instance). However, the DevOps team is concerned about performance degradation and wants to validate recommendations before implementing across all instances. Which THREE approaches provide safe, data-driven rightsizing implementation? (Select THREE)",
      "options": [
        "Implement CloudWatch metric-based Auto Scaling with target tracking on CPU utilization (70%) and Application Load Balancer request count, allowing instances to auto-scale if downsized instances become resource-constrained",
        "Create a canary deployment: downsize 5% of instances (5 instances) in production, monitor performance metrics (latency, error rate, CPU) for 1 week, compare against baseline, then proceed with full rollout if metrics are acceptable",
        "Use AWS Compute Optimizer's 'Inferred' recommendations level (based on 14 days of metrics) instead of 'Low' or 'Medium' confidence recommendations to ensure accuracy",
        "Enable Enhanced Monitoring and Application Performance Monitoring (APM) with detailed metrics (CPU, memory, disk I/O, network, application latency) to identify bottlenecks before and after rightsizing",
        "Schedule rightsizing during maintenance windows with immediate rollback plan: use Auto Scaling Groups to downsize instances, monitor for 30 minutes, and rollback to original size if performance degrades",
        "Implement a monthly rightsizing review cycle using AWS Cost Explorer Rightsizing Recommendations and Trusted Advisor to continuously identify new optimization opportunities as workload patterns change"
      ],
      "correctAnswer": [1, 3, 5],
      "explanation": "Options 1, 3, and 5 are correct for safe, data-driven rightsizing. Option 1: Canary deployment is a proven best practice for infrastructure changes: 1) Select a small subset of instances (5-10%) representing production traffic, 2) Downsize these instances (m5.2xlarge → m5.xlarge), 3) Monitor key metrics for 1-2 weeks: CPU/memory utilization, application latency (p50, p99), error rates (5xx errors), throughput, 4) Compare metrics against baseline (same metrics for instances that weren't downsized), 5) If metrics are within acceptable thresholds (e.g., latency increase < 10%, CPU < 80%), proceed with phased rollout, 6) If metrics degrade, rollback the canary instances and investigate. This approach provides real-world validation with minimal risk - only 5% of traffic is impacted if there are issues. Gradual rollout (5% → 25% → 100%) further reduces risk. Option 3: Enhanced Monitoring and APM provide detailed visibility: 1) CloudWatch Enhanced Monitoring (1-second granularity) for CPU, memory, disk, network at the OS level, 2) APM tools (AWS X-Ray, Datadog, New Relic, Dynatrace) for application-level metrics (API latency, database query time, cache hit rates), 3) Baseline metrics before rightsizing (capture 2-4 weeks of data), 4) After rightsizing, compare metrics to baseline, 5) Identify specific bottlenecks (CPU-bound, memory-bound, I/O-bound) for targeted optimization. This data-driven approach ensures you understand performance implications and can identify issues quickly. Option 5: Continuous rightsizing is essential because workload patterns change over time: 1) AWS Cost Explorer Rightsizing Recommendations analyzes resource utilization and suggests downsizing/upsizing, 2) Trusted Advisor (Cost Optimization checks) identifies idle resources, underutilized instances, 3) Schedule monthly reviews (first Friday of each month), 4) Prioritize recommendations by potential savings (focus on high-cost, low-utilization instances), 5) Track savings over time. Rightsizing is not a one-time activity - new instances are launched, application usage changes, and periodic reviews ensure ongoing optimization. Use AWS Cost Anomaly Detection to identify sudden usage spikes or drops. Option 0 is beneficial but doesn't address validation of rightsizing - Auto Scaling with target tracking allows instances to scale out if demand increases, which is a good safety net. However, if individual instances are undersized, scaling out adds more small instances, which may be less efficient than having appropriately-sized instances. Auto Scaling complements rightsizing but doesn't validate that downsizing is appropriate. Also, if instances are CPU-bound at 90% constantly after downsizing, Auto Scaling will add more instances, potentially increasing costs rather than reducing them. Option 2 is incorrect - Compute Optimizer recommendation confidence levels (High, Medium, Low) are based on the amount of available data, not the level to choose. You should use High confidence recommendations first (most data, most reliable), then Medium confidence if you want to be more aggressive. There's no 'Inferred' level. Compute Optimizer requires CloudWatch metrics for 14 days minimum, 30+ days for higher confidence. Option 4 describes a valid rollback plan but 30 minutes is too short for validation - performance issues may not manifest immediately (e.g., memory leaks, cache cold starts, gradual degradation under load). Also, implementing all 100 instances simultaneously in a maintenance window is risky - if there are issues, you impact 100% of production. The canary approach (Option 1) is safer. Key principle: Safe rightsizing process: 1) Analyze - Use Compute Optimizer, Cost Explorer, CloudWatch metrics to identify candidates, 2) Validate - Start with non-production environments, then canary in production, 3) Monitor - Enhanced monitoring, APM, compare metrics to baseline, 4) Rollout - Phased deployment (5% → 25% → 100%), 5) Rollback plan - Ability to quickly revert if issues arise, 6) Continuous improvement - Monthly reviews, track savings. For AWS Compute Optimizer specifically: 1) Enable at organization level for all accounts, 2) Requires CloudWatch metrics (enable detailed monitoring for EC2), 3) Recommendations for EC2, EBS, Lambda, Auto Scaling Groups, 4) Export recommendations to S3 for analysis, 5) Savings estimates include compute cost reduction but not indirect benefits (lower data transfer, lower network costs). Rightsizing vs Auto Scaling: Rightsizing optimizes instance size for efficiency, Auto Scaling adjusts instance count for capacity. Both should be used together: right-sized instances + Auto Scaling for variable load."
    }
  ]
}
