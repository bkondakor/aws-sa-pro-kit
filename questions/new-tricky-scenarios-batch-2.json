{
  "domain": "Mixed Domains - Advanced Scenarios",
  "task": "Batch 2: Security & Compliance",
  "question_count": 15,
  "questions": [
    {
      "id": "NEW-Q16",
      "question": "A healthcare company must comply with HIPAA and requires all data at rest to be encrypted with customer-managed keys that can be rotated and audited. They have data in S3, RDS, DynamoDB, and EBS volumes across 50 AWS accounts. The security team needs centralized key management with automatic key rotation every 90 days and the ability to immediately revoke access to all encrypted data. What is the MOST operationally efficient solution?",
      "options": [
        "Use AWS KMS with customer managed keys (CMK) in each account, enable automatic rotation, and use CloudTrail for audit logging",
        "Deploy AWS KMS with a centralized CMK in a security account, share keys across accounts using key policies, enable automatic rotation, and use CloudWatch Events to track rotation",
        "Implement AWS CloudHSM cluster with custom key rotation scripts, use HSM-backed keys for all services, and enable CloudTrail logging",
        "Use AWS KMS with customer managed keys, AWS Organizations SCPs to enforce encryption, AWS Config to monitor compliance, and cross-account key sharing via resource policies"
      ],
      "correctAnswer": 3,
      "explanation": "The most comprehensive and operationally efficient solution uses AWS KMS with customer managed keys combined with AWS Organizations SCPs and AWS Config. This approach provides: (1) KMS CMKs with automatic rotation every year (AWS-managed rotation) or manual rotation for 90-day requirements, (2) SCPs to enforce encryption requirements across all 50 accounts, preventing non-compliant resource creation, (3) AWS Config rules to continuously monitor encryption compliance and key usage, (4) cross-account key sharing through key policies allowing centralized management while maintaining service integration. Option A (CMKs in each account) creates management overhead with 50 separate keys to manage and doesn't provide centralized control. Option B suggests a single centralized CMK, but this creates a single point of failure and potential performance bottleneck; while you can share KMS keys, having one key for all services across 50 accounts isn't recommended. Option C (CloudHSM) provides HSM-level security but requires significant operational overhead for managing the cluster and custom rotation scripts, and doesn't integrate as seamlessly with AWS services as KMS. Option D provides the right balance: centralized policy enforcement via SCPs, continuous compliance monitoring via Config, and flexible key management via KMS."
    },
    {
      "id": "NEW-Q17",
      "question": "A financial institution has a regulatory requirement to detect and prevent any S3 buckets from being made publicly accessible across 200 AWS accounts. They also need to automatically remediate any violations within 15 minutes and maintain an audit trail. What solution provides real-time protection with automatic remediation?",
      "options": [
        "Enable S3 Block Public Access at the organization level and use AWS Config rules with automatic remediation to enforce the policy",
        "Deploy AWS Firewall Manager with S3 policies and use Lambda functions triggered by CloudWatch Events to remediate violations",
        "Use AWS Security Hub with CIS benchmark controls and create EventBridge rules to trigger remediation via Systems Manager Automation",
        "Implement Service Control Policies (SCPs) to prevent s3:PutBucketPublicAccessBlock actions and use Config for monitoring"
      ],
      "correctAnswer": 0,
      "explanation": "Enabling S3 Block Public Access at the AWS Organizations level combined with AWS Config rules and automatic remediation provides the most robust solution. S3 Block Public Access at the organization level provides preventive controls that block public access settings on all buckets across all accounts, regardless of bucket policies or ACLs. This prevents violations before they occur. AWS Config rules (like s3-bucket-public-read-prohibited and s3-bucket-public-write-prohibited) provide detective controls and can trigger automatic remediation using AWS Systems Manager Automation documents to fix any violations within minutes. Option B (Firewall Manager) is primarily for managing WAF rules and security groups, not S3 bucket policies - it doesn't support S3 public access controls. Option C (Security Hub) aggregates findings from Config and other services but doesn't provide the preventive control that organization-level Block Public Access provides. Option D has the SCP logic backwards - you want to ENFORCE (not prevent) PutBucketPublicAccessBlock actions; also, SCPs alone don't provide automatic remediation. The combination of organization-level Block Public Access (preventive), Config rules (detective), and automatic remediation provides defense in depth with both prevention and detection/response capabilities."
    },
    {
      "id": "NEW-Q18",
      "question": "A company needs to implement a security solution that provides network threat detection, identifies malicious activity, and detects compromised EC2 instances and IAM credentials across 100 AWS accounts. The solution must analyze VPC Flow Logs, DNS logs, CloudTrail logs, and provide automated response capabilities. What combination of services should they implement?",
      "options": [
        "Amazon GuardDuty for threat detection, EventBridge for event routing, and Lambda functions for automated incident response",
        "AWS Security Hub with aggregated findings from Config, CloudTrail Insights, and custom Lambda analyzers for threat detection",
        "Amazon Macie for data discovery, AWS Config for compliance monitoring, and Systems Manager for automated remediation",
        "AWS Detective for investigation, CloudWatch Logs Insights for log analysis, and Step Functions for response orchestration"
      ],
      "correctAnswer": 0,
      "explanation": "Amazon GuardDuty combined with EventBridge and Lambda provides the most comprehensive solution for this requirement. GuardDuty is specifically designed for threat detection and continuously analyzes VPC Flow Logs, DNS logs, CloudTrail events, and Kubernetes audit logs to identify malicious activity, compromised instances, and credential compromises. It uses machine learning and threat intelligence to detect anomalies. GuardDuty operates across all accounts when enabled at the organization level and can be centrally managed. EventBridge (formerly CloudWatch Events) can route GuardDuty findings to Lambda functions for automated remediation actions like isolating compromised instances, revoking IAM credentials, or creating incident tickets. Option B (Security Hub) is a security findings aggregator that collects findings from multiple services but doesn't perform the actual threat detection on logs - it relies on other services like GuardDuty. Option C (Macie) is for data security and privacy, specifically for discovering sensitive data in S3, not for network threat detection or IAM credential compromise. Option D (Detective) is for security investigation and analysis AFTER an incident is detected, not for real-time threat detection. GuardDuty is the purpose-built service for continuous threat detection from VPC Flow Logs, DNS, and CloudTrail."
    },
    {
      "id": "NEW-Q19",
      "question": "A company stores sensitive customer data in S3 and must comply with GDPR data residency requirements ensuring data never leaves the EU. They have a global team, including developers in US and Asia who need read access to bucket metadata but must not be able to access objects. The compliance team requires proof that objects have never been accessed from outside eu-west-1 and eu-central-1. How should they configure this?",
      "options": [
        "Use S3 bucket policies with aws:RequestedRegion condition to deny GetObject from non-EU regions and enable CloudTrail data events for audit logs",
        "Implement S3 Access Points with VPC-only access in EU regions, use IAM policies with aws:SourceVpc conditions, and enable S3 server access logging",
        "Configure AWS Control Tower with preventive guardrails using SCPs to restrict S3 access by region and use CloudTrail for audit trails",
        "Enable S3 Object Lock in compliance mode, use bucket policies with aws:PrincipalOrgID conditions, and implement VPC endpoints in EU regions only"
      ],
      "correctAnswer": 0,
      "explanation": "Using S3 bucket policies with the aws:RequestedRegion condition key combined with CloudTrail data events provides the most direct and auditable solution. The bucket policy can explicitly deny s3:GetObject and s3:GetObjectVersion actions when aws:RequestedRegion is not in the list [eu-west-1, eu-central-1]. This ensures objects cannot be accessed from outside EU regions. For metadata access (ListBucket, GetBucketLocation), the policy can allow these globally since they don't expose object data. CloudTrail data events for S3 provide detailed audit logs of all object-level API calls, including the region from which they were made, providing the required compliance proof. Option B (S3 Access Points with VPC) is overly restrictive - it would require all access to come from VPCs, which may not be practical for all use cases, and VPC endpoints don't inherently restrict by region. Option C (Control Tower with SCPs) can restrict regions but SCPs affect all services, not just S3 object access, and are less granular. Option D (Object Lock) is for preventing object deletion/modification (WORM), not for regional access control. The bucket policy approach is most precise, allowing metadata access globally while restricting object access regionally, and CloudTrail provides the necessary audit trail."
    },
    {
      "id": "NEW-Q20",
      "question": "A SaaS company hosts multi-tenant applications where each customer's data must be cryptographically isolated. They have 5000 customers with data in DynamoDB and S3. Each customer needs their own encryption key that can be independently rotated and revoked. Key operations must be logged per customer. Managing 5000 separate KMS CMKs would exceed the KMS key quota (10,000 per region) and create cost issues. What is the MOST cost-effective and scalable solution?",
      "options": [
        "Use KMS with customer managed keys and implement envelope encryption with unique data keys per customer stored in DynamoDB",
        "Deploy AWS CloudHSM with custom key derivation to generate per-customer keys from a master key and implement envelope encryption",
        "Implement S3 bucket-per-customer architecture with customer managed CMKs, using S3 cross-region replication to manage key limits",
        "Use AWS KMS with a single CMK and implement application-layer encryption with per-customer encryption contexts in DynamoDB"
      ],
      "correctAnswer": 0,
      "explanation": "Using KMS customer managed keys with envelope encryption and storing unique data keys per customer in DynamoDB is the most scalable and cost-effective solution. With envelope encryption, you generate a unique data encryption key (DEK) for each customer using KMS GenerateDataKey API, encrypt the customer's data with this DEK, and then encrypt the DEK itself with a KMS CMK. The encrypted DEK is stored alongside the customer data in DynamoDB. This approach requires far fewer KMS CMKs (you might use one CMK per application or environment) while providing cryptographic isolation per customer through unique DEKs. Each customer's data can only be decrypted with their specific DEK. You can rotate customer keys by generating new DEKs, and 'revoking' access means deleting or disabling the encrypted DEK. KMS CloudTrail logs show which customer's key was used via the encryption context parameter. Option B (CloudHSM) provides strong security but significantly increases operational complexity and cost (CloudHSM costs $1/hour vs KMS $1/month). Option C (bucket-per-customer) creates massive operational overhead with 5000 S3 buckets and doesn't solve the KMS key quota issue. Option D (single CMK with encryption context) doesn't provide true cryptographic isolation since all customers share the same CMK - you cannot revoke access for a single customer without affecting all customers. The envelope encryption pattern is a well-established best practice for multi-tenant encryption at scale."
    },
    {
      "id": "NEW-Q21",
      "question": "A company must ensure that all EC2 instances are launched from approved AMIs only and non-compliant instances must be automatically terminated within 30 minutes. They have 80 AWS accounts with different teams deploying instances frequently. The solution must provide real-time compliance monitoring and maintain an audit trail. What is the MOST effective automated solution?",
      "options": [
        "Use AWS Config rules to detect non-compliant instances and AWS Systems Manager Automation to terminate them",
        "Implement Service Control Policies to prevent launching instances from non-approved AMIs at the organization level",
        "Deploy AWS Lambda functions triggered by CloudWatch Events for EC2 RunInstances API calls to validate and terminate non-compliant instances",
        "Use AWS Security Hub with custom insights and EventBridge rules to trigger Step Functions workflows for remediation"
      ],
      "correctAnswer": 1,
      "explanation": "Service Control Policies (SCPs) provide the strongest preventive control by blocking the launch of instances from non-approved AMIs at the API level before they're created. An SCP can use the ec2:ImageId condition key to allow RunInstances API calls only when the AMI ID is in an approved list. This prevents non-compliant instances from being created in the first place across all 80 accounts, which is more effective than detecting and terminating after creation. SCPs are enforced at the AWS Organizations level and apply to all accounts. For audit trails, CloudTrail logs all denied API calls with the SCP evaluation result. Option A (Config rules with remediation) is a good detective control but allows non-compliant instances to run for up to 30 minutes, which may not be acceptable for security. Option C (Lambda on RunInstances events) could work but requires custom code, has potential for race conditions, and the instance is still created before validation occurs. Option D (Security Hub) aggregates findings but doesn't provide preventive controls. The preventive approach with SCPs is superior because it enforces compliance at the permission layer, prevents violations rather than detecting and remediating them, and provides immediate blocking without the detection delay. This is a defense-in-depth best practice: prevent first, detect second."
    },
    {
      "id": "NEW-Q22",
      "question": "A global company needs to implement certificate management for 500 internal applications across AWS and on-premises environments. Certificates must be issued from a private Certificate Authority, automatically renewed before expiration, and certificate usage must be audited. They currently manually manage certificates using OpenSSL, which has led to outages due to expired certificates. What is the MOST operationally efficient solution?",
      "options": [
        "Deploy AWS Certificate Manager Private CA, use ACM for automatic certificate renewal for AWS services, and implement custom scripts for on-premises renewal",
        "Use AWS Certificate Manager Private CA with AWS Certificate Manager for AWS services and use ACME protocol support for on-premises environments",
        "Implement HashiCorp Vault on EC2 for certificate management with auto-renewal capabilities for both AWS and on-premises",
        "Deploy AWS Certificate Manager Private CA, export certificates for manual distribution, and use CloudWatch alarms for expiration notifications"
      ],
      "correctAnswer": 1,
      "explanation": "AWS Certificate Manager Private CA (ACM PCA) combined with ACM for AWS services and ACME protocol support for on-premises provides the most operationally efficient solution. ACM PCA creates and manages a private CA hierarchy. For AWS services (ALB, CloudFront, API Gateway), ACM integrates natively and handles automatic certificate renewal and deployment without manual intervention. For on-premises environments and non-AWS services, ACM PCA supports the ACME protocol (Automated Certificate Management Environment), which enables automated certificate issuance and renewal using standard tools like certbot. All certificate operations are logged in CloudTrail for audit purposes. Option A (custom scripts for on-premises) increases operational overhead and maintenance burden. Option C (Vault on EC2) requires managing additional infrastructure, licensing (for enterprise features), and has higher operational complexity compared to the managed ACM PCA service. Option D (manual export and distribution) defeats the purpose of automation and doesn't solve the expired certificate problem. The combination of ACM for AWS integration, ACME for on-premises automation, and CloudTrail for auditing provides comprehensive, automated certificate lifecycle management with minimal operational overhead."
    },
    {
      "id": "NEW-Q23",
      "question": "A company's security team needs to analyze and investigate security findings across 200 AWS accounts. They receive thousands of security findings daily from GuardDuty, Security Hub, and Macie. Analysts spend hours manually correlating findings, identifying affected resources, and determining the blast radius of security incidents. What AWS service should they implement to streamline security investigations?",
      "options": [
        "AWS Security Hub with custom insights and automated finding aggregation across accounts",
        "Amazon Detective with automatic data collection from VPC Flow Logs, CloudTrail, and GuardDuty to visualize security investigations",
        "AWS CloudTrail Insights to identify unusual API activity and correlate with security findings",
        "Amazon Athena with automated queries against CloudTrail and VPC Flow Logs stored in S3 for forensic analysis"
      ],
      "correctAnswer": 1,
      "explanation": "Amazon Detective is purpose-built for security investigation and analysis. It automatically collects log data from VPC Flow Logs, CloudTrail, and GuardDuty, and uses machine learning to create a unified, interactive view of resource behaviors and interactions over time. Detective enables security analysts to quickly investigate findings by automatically visualizing relationships between resources, API calls, and network traffic. It can show the full context of a security finding including what happened before and after an incident, which resources were affected, and the scope of impact - essentially determining the 'blast radius' automatically. This significantly reduces investigation time from hours to minutes. Option A (Security Hub) aggregates findings from multiple services but doesn't provide the investigation and visualization capabilities that Detective offers. Option C (CloudTrail Insights) identifies unusual API activity but doesn't provide comprehensive investigation tools or correlation across multiple data sources. Option D (Athena queries) can perform forensic analysis but requires manual query writing and doesn't provide the automatic correlation and visualization that Detective offers. Detective is specifically designed to solve the problem of time-consuming manual investigation and correlation of security findings."
    },
    {
      "id": "NEW-Q24",
      "question": "A healthcare provider must implement controls to ensure that PHI (Protected Health Information) in S3 buckets is never exposed publicly or shared with unauthorized AWS accounts. They have 1000+ S3 buckets across 50 accounts. The compliance team requires continuous monitoring, automatic detection of policy violations, and prevention of data exfiltration. What combination provides the MOST comprehensive protection?",
      "options": [
        "Enable S3 Block Public Access at organization level, use Amazon Macie to discover and classify PHI, implement S3 Access Analyzer, and use AWS Config for monitoring",
        "Implement bucket policies with aws:PrincipalOrgID conditions, enable S3 server access logging, and use CloudWatch Logs Insights for analysis",
        "Deploy AWS Control Tower with detective guardrails, use AWS Security Hub for findings aggregation, and implement automated remediation with Lambda",
        "Use AWS IAM Access Analyzer for S3, enable GuardDuty for threat detection, and implement Service Control Policies to restrict S3 actions"
      ],
      "correctAnswer": 0,
      "explanation": "The most comprehensive solution combines multiple layers of defense: (1) S3 Block Public Access at the organization level prevents public exposure across all buckets in all accounts, (2) Amazon Macie uses machine learning to automatically discover, classify, and protect sensitive data like PHI across all S3 buckets, providing continuous monitoring and alerts when sensitive data is detected or at risk, (3) IAM Access Analyzer for S3 continuously analyzes bucket policies and ACLs to identify buckets shared with external entities and provides findings for review, (4) AWS Config rules provide continuous compliance monitoring and can trigger automatic remediation. This multi-layered approach provides preventive controls (Block Public Access), detective controls (Macie for data classification, Access Analyzer for external sharing), and compliance monitoring (Config). Option B (bucket policies with PrincipalOrgID) helps but doesn't prevent public access or provide data classification capabilities. Option C (Control Tower) provides governance but doesn't include the data classification and external sharing analysis that Macie and Access Analyzer provide. Option D combines good services but GuardDuty is focused on threat detection (compromised credentials, malicious activity) rather than data classification and policy compliance. The combination in option A provides the most complete coverage for PHI protection."
    },
    {
      "id": "NEW-Q25",
      "question": "A financial services company needs to implement network segmentation to isolate their PCI DSS cardholder data environment (CDE) from other workloads. The CDE VPC must allow outbound internet access for software updates but must block all inbound internet traffic and restrict lateral movement from non-CDE VPCs. All traffic must be inspected. They currently have 5 CDE VPCs and 20 non-CDE VPCs connected via Transit Gateway. What is the MOST secure architecture?",
      "options": [
        "Create separate Transit Gateway route tables for CDE and non-CDE VPCs, deploy AWS Network Firewall in an inspection VPC, and route all CDE egress through the firewall",
        "Implement VPC peering between CDE VPCs only, use NACLs to block inter-VPC traffic from non-CDE, and deploy NAT Gateways for egress",
        "Use Transit Gateway with separate attachments for CDE VPCs, configure blackhole routes to prevent CDE-to-non-CDE traffic, and implement proxy servers for egress",
        "Deploy CDE VPCs without Transit Gateway attachment, implement AWS PrivateLink for required service access, and use AWS Network Firewall for egress filtering"
      ],
      "correctAnswer": 0,
      "explanation": "Creating separate Transit Gateway route tables for CDE and non-CDE VPCs with AWS Network Firewall in an inspection VPC provides the most secure and manageable architecture. This design uses Transit Gateway route table isolation to segment CDE and non-CDE networks - the CDE route table only contains routes to CDE VPCs and the inspection VPC, preventing any routing from non-CDE VPCs to CDE VPCs. All egress traffic from CDE VPCs is routed through the inspection VPC where AWS Network Firewall performs stateful inspection, URL filtering, and IPS/IDS before allowing traffic to the internet via NAT Gateways. The firewall can enforce allow-lists for software update URLs. This provides defense in depth with both network-layer isolation (routing) and application-layer inspection (firewall). Option B (VPC peering only) doesn't provide centralized inspection and is harder to manage at scale. Option C (blackhole routes) can work but proxy servers on EC2 add operational complexity compared to the managed Network Firewall service. Option D (no TGW attachment) completely isolates CDE VPCs but makes it difficult to provide required services and can complicate operations. The Transit Gateway with route table segmentation plus Network Firewall inspection is an AWS best practice for PCI DSS environments."
    },
    {
      "id": "NEW-Q26",
      "question": "A company operates a serverless application using Lambda, API Gateway, and DynamoDB. They need to implement authentication and authorization that supports both machine-to-machine (M2M) API access using client credentials and user authentication with social identity providers (Google, Facebook). The solution must support fine-grained authorization based on custom user attributes and API scopes. What is the MOST appropriate solution?",
      "options": [
        "Use Amazon Cognito User Pools for user authentication, Amazon Cognito Identity Pools for M2M, and implement custom authorizers in Lambda for fine-grained authorization",
        "Implement Amazon Cognito User Pools with social identity federation for users, OAuth 2.0 client credentials flow for M2M, and use Cognito groups with API Gateway resource policies for authorization",
        "Deploy AWS IAM roles for M2M access, Amazon Cognito User Pools for user authentication with social federation, and implement Lambda authorizers for fine-grained RBAC",
        "Use Auth0 or Okta on EC2 for both user and M2M authentication, integrate with API Gateway using JWT authorizers, and implement custom RBAC in Lambda functions"
      ],
      "correctAnswer": 2,
      "explanation": "The most appropriate solution uses AWS IAM roles for M2M access and Amazon Cognito User Pools with Lambda authorizers for fine-grained authorization. For M2M communication, IAM roles with temporary credentials provide secure, scalable authentication without managing client secrets. For users, Cognito User Pools supports federation with social identity providers (Google, Facebook) via SAML or OpenID Connect. Lambda authorizers (formerly custom authorizers) allow implementing fine-grained authorization logic based on custom user attributes, scopes, and complex business rules - they receive the JWT token, validate it, and return an IAM policy specifying what API resources the user can access. Option A suggests using Cognito Identity Pools for M2M, but Identity Pools are designed for providing temporary AWS credentials to users/devices, not for M2M API authentication. Option B (Cognito groups with resource policies) doesn't provide the fine-grained, custom attribute-based authorization required; Cognito groups are useful but less flexible than Lambda authorizers for complex authorization logic. Option D (third-party auth on EC2) adds infrastructure to manage, increases costs, and negates the serverless benefits; Lambda authorizers with Cognito provide equivalent functionality as a managed service. The combination of IAM for M2M, Cognito for user auth, and Lambda authorizers provides the right balance of security, flexibility, and operational simplicity."
    },
    {
      "id": "NEW-Q27",
      "question": "A company needs to grant temporary cross-account access to an external security auditor to review CloudTrail logs and AWS Config compliance data across 100 AWS accounts for a two-week audit period. The auditor should have read-only access and must not be able to modify or delete any logs. Access must automatically expire after the audit period. What is the MOST secure approach?",
      "options": [
        "Create IAM users in each of the 100 accounts with ReadOnlyAccess policy and manually delete them after two weeks",
        "Use AWS SSO with permission sets for read-only access to CloudTrail and Config, create a temporary user, and configure session duration limits",
        "Create an IAM role in each account with a trust policy allowing the auditor's AWS account to assume the role, add a condition for time-based access, and share role ARNs",
        "Centralize CloudTrail and Config logs in a dedicated audit account S3 bucket and grant the auditor's AWS account cross-account S3 read access with a bucket policy including time-based conditions"
      ],
      "correctAnswer": 3,
      "explanation": "Centralizing logs in a dedicated audit account and granting time-bound cross-account access is the most secure and operationally efficient approach. CloudTrail supports organization trails that automatically aggregate logs from all accounts to a central S3 bucket. AWS Config can also deliver configuration snapshots and compliance data to a central S3 bucket. A bucket policy can grant the auditor's AWS account (via aws:PrincipalAccount) read access (s3:GetObject, s3:ListBucket) with a condition using aws:CurrentTime or aws:EpochTime to enforce automatic access expiration after two weeks. This approach provides: (1) single point of access control, (2) automatic access revocation without manual intervention, (3) read-only access to logs without granting AWS account access, (4) centralized audit trail of auditor's access via CloudTrail. Option A (IAM users in 100 accounts) has massive operational overhead and relies on manual cleanup. Option B (AWS SSO) is good but session duration limits are typically hours, not weeks, requiring the auditor to re-authenticate frequently. Option C (IAM roles in each account) requires managing 100 separate roles and the auditor has to switch between accounts. The centralized logs approach is an AWS best practice for security audits and compliance."
    },
    {
      "id": "NEW-Q28",
      "question": "A media company stores encrypted videos in S3 with client-side encryption using KMS. Videos are large (5-50 GB each) and they need to securely share specific videos with external partners for a limited time (24-72 hours) without requiring partners to have AWS accounts or KMS access. Partners should be able to download videos directly. What is the MOST secure and user-friendly solution?",
      "options": [
        "Generate S3 pre-signed URLs with expiration times using an IAM role that has KMS decrypt permissions, and share URLs with partners",
        "Create an IAM user for each partner with KMS decrypt permissions and S3 GetObject access, provide temporary credentials with STS GetSessionToken",
        "Decrypt videos using a Lambda function, temporarily store decrypted versions in a separate bucket, generate pre-signed URLs, and delete after expiration",
        "Use Amazon CloudFront with signed URLs, configure CloudFront to access S3 using OAI with KMS permissions, and set URL expiration times"
      ],
      "correctAnswer": 0,
      "explanation": "Generating S3 pre-signed URLs with expiration times using an IAM role that has KMS decrypt permissions is the most secure and user-friendly solution. Pre-signed URLs allow temporary access to S3 objects without requiring the downloader to have AWS credentials. When the object is downloaded using a pre-signed URL, S3 performs the KMS decryption operation using the credentials of the IAM principal (role or user) that generated the pre-signed URL. The URL can be configured to expire after 24-72 hours, automatically revoking access. This requires no AWS account for partners and provides direct S3 download performance. Option B (IAM users for partners) requires partners to understand AWS authentication and manage credentials, adding complexity. Option C (decrypting to temporary bucket) is insecure because it stores unencrypted sensitive data, requires additional storage costs, and adds complexity with Lambda processing and cleanup. Option D (CloudFront signed URLs) could work but adds unnecessary complexity and costs; CloudFront is beneficial for global distribution and caching, but for one-time large file downloads directly to known partners, S3 pre-signed URLs are simpler and more cost-effective. The key insight is that pre-signed URLs inherit the KMS permissions of the URL generator, enabling seamless encrypted object access for partners."
    },
    {
      "id": "NEW-Q29",
      "question": "A company must implement data loss prevention (DLP) to prevent sensitive data (SSN, credit card numbers, API keys) from being uploaded to S3 buckets or stored in code repositories. They use S3 for data storage and CodeCommit for source code. The security team needs automated detection, blocking of violations in real-time for CodeCommit, and alerts for S3. What combination of services should they implement?",
      "options": [
        "Use Amazon Macie for S3 to detect sensitive data, implement CodeGuru Reviewer for code analysis, and use EventBridge for alerting",
        "Deploy AWS Lambda functions triggered by S3 PutObject events to scan objects, use CodeCommit approval rules with Lambda validators, and send alerts via SNS",
        "Implement Amazon Macie for S3 with real-time discovery, use CodeGuru Secrets Detector for CodeCommit, and integrate findings with Security Hub for alerting",
        "Use AWS Config rules to monitor S3 objects, deploy CodePipeline with CodeBuild to scan commits using custom security tools, and use CloudWatch for alerting"
      ],
      "correctAnswer": 2,
      "explanation": "Amazon Macie for S3 combined with CodeGuru Secrets Detector for CodeCommit provides the most comprehensive and automated DLP solution. Amazon Macie uses machine learning and pattern matching to automatically discover, classify, and protect sensitive data in S3, including SSNs, credit card numbers, and API keys. Macie can be configured for automated discovery jobs and publishes findings to EventBridge for alerting. Amazon CodeGuru Secrets Detector is specifically designed to identify hardcoded secrets (API keys, passwords, tokens) in source code and integrates directly with CodeCommit to provide recommendations during pull requests. Security Hub can aggregate findings from both Macie and CodeGuru for centralized security monitoring. Option A mentions CodeGuru Reviewer which does code quality analysis but not specifically secrets detection (CodeGuru has separate Reviewer and Secrets Detector features). Option B (custom Lambda functions) requires significant development and maintenance effort and won't match the accuracy of Macie's ML models; CodeCommit approval rules don't block commits, they control merge approvals. Option D (Config rules + custom tools) requires building custom scanning capabilities and doesn't provide the same level of ML-based detection as Macie. The combination of Macie (for S3 DLP) and CodeGuru Secrets Detector (for code DLP) provides AWS-native, automated, and accurate sensitive data detection."
    },
    {
      "id": "NEW-Q30",
      "question": "A company implements a defense-in-depth strategy for their web application. They want to protect against DDoS attacks, bot traffic, SQL injection, and XSS at multiple layers. The application uses CloudFront for content delivery, Application Load Balancer, and EC2 instances. They need automatic threat mitigation and want to minimize manual rule management. What is the MOST comprehensive security architecture?",
      "options": [
        "Enable AWS Shield Standard on CloudFront and ALB, implement AWS WAF with managed rule groups on both CloudFront and ALB, and use AWS Firewall Manager to centrally manage policies",
        "Subscribe to AWS Shield Advanced for DDoS protection with 24/7 DRT support, deploy AWS WAF on CloudFront with AWS Managed Rules and rate-based rules, and enable ALB security features",
        "Implement AWS WAF with custom rules on ALB, use CloudFront geo-restriction for traffic filtering, enable VPC Flow Logs for monitoring, and deploy AWS Network Firewall",
        "Use AWS Shield Advanced with AWS WAF on both CloudFront and ALB using AWS Managed Rules, enable automatic application layer DDoS mitigation, and implement Firewall Manager for central management"
      ],
      "correctAnswer": 3,
      "explanation": "The most comprehensive defense-in-depth security architecture uses AWS Shield Advanced with AWS WAF on both CloudFront and ALB, leveraging AWS Managed Rules and Firewall Manager for central management. Shield Advanced provides enhanced DDoS protection at network (L3/L4) and application layers (L7), includes access to the DDoS Response Team (DRT), cost protection against DDoS-related scaling charges, and automatic application layer DDoS mitigation. Deploying AWS WAF at both CloudFront (edge) and ALB (origin) provides layered protection - CloudFront WAF blocks threats at the edge, reducing load on origin, while ALB WAF provides additional protection. AWS Managed Rules for WAF (like Core Rule Set, Known Bad Inputs, SQL Database, Anonymous IP) provide continuously updated protection against OWASP top 10 vulnerabilities including SQL injection and XSS, with minimal manual management. Firewall Manager centrally manages WAF rules across resources and accounts. Option A uses Shield Standard (free but basic) instead of Shield Advanced, missing advanced DDoS features and cost protection. Option B only deploys WAF on CloudFront, not ALB, providing only single-layer protection. Option C uses custom WAF rules requiring manual management, and Network Firewall is for VPC-level inspection, not application-layer web protection. Option D provides the most complete coverage with automatic threat mitigation, managed rules, and centralized management."
    }
  ]
}
