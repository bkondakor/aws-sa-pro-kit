{
  "domain": "Mixed Domains: Advanced Scenarios",
  "task": "Tricky Batch 5: Hybrid Cloud & Multi-Region Architectures",
  "question_count": 15,
  "questions": [
    {
      "type": "single",
      "question": "A financial services company has a primary 10 Gbps AWS Direct Connect connection from their data center to us-east-1, with a VPN connection as backup. They've configured BGP to prefer Direct Connect by setting AS PATH prepending on the VPN. During a recent Direct Connect maintenance window, traffic failed to failover to VPN automatically. Investigation revealed that the VPN connection remained UP throughout the maintenance. What is the MOST likely root cause of the failover failure?",
      "options": [
        "The VPN connection's customer gateway BGP ASN did not match the Direct Connect virtual interface BGP ASN, preventing route propagation",
        "Direct Connect advertises routes with a longer AS PATH than VPN by default, so the AS PATH prepending on VPN made it even less preferred",
        "The Virtual Private Gateway has a built-in route preference (Direct Connect > VPN) that overrides BGP metrics, and routes are not withdrawn when Direct Connect BGP sessions go down if the link stays up",
        "The company needed to enable BFD (Bidirectional Forwarding Detection) on the Direct Connect connection to detect the maintenance window faster"
      ],
      "correctAnswer": 2,
      "explanation": "The correct answer is that VGW has built-in route preference where Direct Connect routes are always preferred over VPN routes regardless of BGP metrics like AS PATH. More critically, during Direct Connect maintenance, AWS may keep the BGP session up while blocking data plane traffic. This causes routes to remain in the routing table even though packets cannot flow. To handle this, you should: 1) Use BFD on Direct Connect for faster failure detection, 2) Monitor CloudWatch metrics for ConnectionState, 3) Consider using Transit Gateway instead of VGW, which has better routing control. Option A is incorrect because BGP ASN matching is not required between Direct Connect and VPN - they can have different ASNs. Option B is incorrect because AS PATH prepending on VPN actually makes VPN routes LESS preferred (longer AS PATH = less preferred), which is the opposite of what's needed for failover. Option D mentions BFD but doesn't explain the core issue - VGW's route preference behavior. AWS Direct Connect SLA only covers the AWS-side equipment, not the entire path."
    },
    {
      "type": "multiple",
      "question": "A global media company operates a multi-region active-active architecture across us-east-1, eu-west-1, and ap-southeast-1. They use Route 53 with geoproximity routing, Aurora Global Database, and DynamoDB Global Tables. Users in Asia report inconsistent data: sometimes they see content published 5 minutes ago, sometimes they don't see content published 30 seconds ago. The application uses DynamoDB for user preferences and Aurora for content metadata. Which THREE actions would improve read consistency for Asian users while maintaining low latency? (Select THREE)",
      "options": [
        "Configure Route 53 health checks to fail over to the next closest region if ap-southeast-1 Aurora read replicas have replication lag > 1 second",
        "Enable DynamoDB strong consistency reads in ap-southeast-1 for critical user preference queries",
        "Implement application-level read-after-write consistency by tracking the last write timestamp and waiting for replication before reading",
        "Switch from Aurora Global Database to Aurora Multi-Master to enable synchronous replication across all regions",
        "Use DynamoDB Global Tables' version 2019.11.21 with strongly consistent reads across all regions",
        "Implement a cache layer with ElastiCache Global Datastore to provide consistent reads with sub-millisecond latency"
      ],
      "correctAnswer": [0, 2, 5],
      "explanation": "Options 0, 2, and 5 are correct. Option 0: Route 53 health checks can monitor Aurora replica lag and fail over to another region if lag exceeds thresholds, ensuring users don't read stale data. Option 2: Application-level tracking of writes with timestamps allows the app to wait for replication before serving reads - this is a common pattern for critical consistency requirements. Option 5: ElastiCache Global Datastore provides active-active replication across regions with sub-second replication, and you can implement cache invalidation strategies to ensure consistency. Option 1 is incorrect because DynamoDB strong consistency only works within a single region - Global Tables use eventual consistency across regions. Option 3 is incorrect because Aurora Global Database already provides the best cross-region replication for Aurora (typically < 1 second lag), and Aurora Multi-Master is only available within a single region, not across regions. Option 4 is incorrect because DynamoDB Global Tables do not support strongly consistent reads across regions - they only support eventual consistency for cross-region reads. The key insight is that for global applications, you must combine multiple strategies: intelligent routing based on data freshness, application-level consistency guarantees, and caching layers with proper invalidation."
    },
    {
      "question": "A manufacturing company wants to establish private connectivity between their on-premises data center and AWS. They have three VPCs in us-east-1 (Production, Development, Test) and need to access on-premises databases from all VPCs. They're considering Transit Gateway with a single Direct Connect connection (1 Gbps) using transit VIFs. Security requires that Development and Test VPCs cannot communicate with each other, but both need access to on-premises and a shared services VPC. What is the MOST operationally efficient solution?",
      "options": [
        "Create a Transit Gateway with four VPC attachments and one Direct Connect Gateway attachment. Use separate route tables for Dev and Test VPCs that only contain routes to on-premises and shared services. Use association and propagation to control routing",
        "Create three Virtual Private Gateways (one per VPC) and attach them to a Direct Connect Gateway. Use route tables in each VPC to control traffic flow to on-premises and shared services VPC via VPC peering",
        "Create a Transit Gateway with four VPC attachments. Create a single route table and use security groups and NACLs to prevent Dev-Test communication while allowing on-premises access",
        "Create two Transit Gateways - one for Production and Shared Services, another for Development and Test. Connect both to Direct Connect Gateway and use route filtering to prevent Dev-Test communication"
      ],
      "correctAnswer": 0,
      "explanation": "Option A is correct. Transit Gateway supports advanced routing segmentation through route table associations and propagations. You would create separate route tables: Production VPCs can have their own route table, Dev and Test VPCs each get their own route table that only has routes propagated from the Direct Connect Gateway attachment (on-premises) and the Shared Services VPC attachment. By not propagating routes between Dev and Test route tables, you prevent communication between these VPCs while allowing both to reach on-premises and shared services. This is the operationally efficient approach because it uses built-in Transit Gateway routing features. Option B is incorrect because using three separate VGWs with Direct Connect Gateway creates operational complexity - you'd need VPC peering for shared services access, which doesn't scale well. Additionally, you can only associate up to 10 VPCs with a Direct Connect Gateway when using VGWs. Option C is incorrect because using security groups and NACLs to block traffic between VPCs is not operationally efficient and doesn't prevent the routes from existing - it's a data plane solution to a control plane problem. Option D is incorrect because creating two Transit Gateways is unnecessary complexity and cost - a single Transit Gateway with proper route table segmentation achieves the goal. Transit Gateway charges per attachment and per GB transferred, so minimizing Transit Gateways reduces cost."
    },
    {
      "question": "An enterprise has deployed a hub-and-spoke network architecture using Transit Gateway in us-east-1. The hub VPC contains shared services (Active Directory, DNS, proxy servers). Recently, they've noticed that DNS queries from spoke VPCs to the hub VPC's Route 53 Resolver endpoints are failing intermittently, but direct IP communication works fine. The Route 53 Resolver endpoints are configured with security groups allowing UDP/TCP port 53 from the spoke VPC CIDR ranges. What is the MOST likely cause of the DNS resolution failures?",
      "options": [
        "Transit Gateway does not support DNS traffic and requires VPC Peering for DNS resolution between VPCs",
        "The Route 53 Resolver endpoint security groups must allow traffic from the Transit Gateway ENI IP addresses, not the spoke VPC CIDR ranges",
        "DNS traffic is being blackholed because the Route 53 Resolver endpoint subnet route table doesn't have a route back to the spoke VPCs through the Transit Gateway",
        "Route 53 Resolver endpoints require enabling DNS resolution and DNS hostnames on both the hub and spoke VPCs' VPC settings"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct. This is a classic asymmetric routing problem. When DNS queries arrive from spoke VPCs through Transit Gateway, the Route 53 Resolver endpoint receives them and processes them. However, when sending responses, the Route 53 Resolver endpoint looks at the subnet route table. If this route table doesn't have routes pointing the spoke VPC CIDRs back through the Transit Gateway, the responses either go to the local VPC's IGW (if it exists) or are dropped. The fix is to ensure the Route 53 Resolver endpoint subnets have proper return routes to spoke VPCs via the Transit Gateway. This is different from the VPC's main route table - the endpoint subnet specifically needs these routes. Option A is incorrect because Transit Gateway fully supports DNS traffic - there's no protocol restriction. Option B is incorrect because Transit Gateway doesn't use its own IP addresses for forwarding traffic - it preserves the source IP of the original packet from the spoke VPC, so the security group rules for spoke VPC CIDRs are correct. Option D is incorrect because while DNS hostnames and resolution settings matter for Route 53 private hosted zones, they don't affect Route 53 Resolver endpoint functionality. The key lesson is that for any hub-and-spoke architecture with Transit Gateway, all subnets that receive traffic from spokes must have explicit return routes through the Transit Gateway."
    },
    {
      "type": "multiple",
      "question": "A healthcare company is implementing a disaster recovery solution with a primary region in us-east-1 and DR region in us-west-2. They have a requirement for RPO of 15 minutes and RTO of 1 hour. The architecture includes Aurora PostgreSQL (10 TB), S3 buckets with medical images (500 TB), EFS file systems (50 TB), and EC2-based application servers. Which THREE components should be part of their DR strategy? (Select THREE)",
      "options": [
        "Use Aurora Global Database with us-west-2 as a secondary region, and automate promotion of the secondary region using Lambda triggered by CloudWatch alarms",
        "Enable S3 Cross-Region Replication (CRR) with replication time control (RTC) to guarantee 15-minute replication SLA for medical images",
        "Use AWS Backup to create EFS backups every 15 minutes and copy them to us-west-2, then restore from backup during DR",
        "Implement AWS DataSync to replicate EFS data to us-west-2 EFS on a 15-minute schedule with verification enabled",
        "Deploy EC2 instances in us-west-2 in a stopped state with AMIs updated daily, and use Auto Scaling to launch instances during DR",
        "Use CloudFormation StackSets to maintain synchronized infrastructure templates across both regions, with EC2 launch templates ready for immediate deployment"
      ],
      "correctAnswer": [0, 1, 3],
      "explanation": "Options 0, 1, and 3 are correct. Option 0: Aurora Global Database provides automated replication with typical lag under 1 second, easily meeting the 15-minute RPO. Automated promotion can bring the secondary region online in under 1 minute for the database layer. Option 1: S3 CRR with Replication Time Control guarantees 99.99% of objects replicate within 15 minutes, with replication metrics and notifications when objects don't meet the SLA - this is specifically designed for compliance and DR scenarios. Option 3: DataSync can schedule synchronization tasks as frequently as every 15 minutes and includes verification to ensure data integrity. Unlike backup/restore, DataSync keeps a live copy in the DR region, significantly reducing RTO. Option 2 is incorrect because AWS Backup for EFS has a minimum backup frequency of 1 hour (not 15 minutes), and the restore process for 50 TB would likely exceed the 1-hour RTO requirement. Option 4 is incorrect because maintaining stopped EC2 instances incurs storage costs and requires manual AMI updates - it's operationally inefficient. Option 5 is incorrect as the sole EC2 strategy because CloudFormation alone doesn't provide fast enough deployment to meet a 1-hour RTO when you need to launch and configure instances from scratch. However, combining CloudFormation with warm standby (some instances running) or pilot light (core infrastructure pre-deployed) would work. The key is that for strict RPO/RTO requirements, you need automated, tested, and monitored replication mechanisms, not just backup/restore strategies."
    },
    {
      "question": "A company has a VPC in us-east-1 with three subnets: public (10.0.1.0/24), private-app (10.0.2.0/24), and private-db (10.0.3.0/24). They use a transit VIF on Direct Connect to connect to on-premises (172.16.0.0/16) via Transit Gateway. The on-premises network team reports they can reach EC2 instances in the private-app subnet but cannot reach RDS instances in the private-db subnet, even though security groups allow the traffic. Both subnets' route tables have 0.0.0.0/0 pointing to the Transit Gateway. What is the MOST likely cause?",
      "options": [
        "RDS instances are in a DB subnet group that has 'Publicly Accessible' set to No, which blocks Transit Gateway traffic",
        "The Transit Gateway route table doesn't have a route for 10.0.3.0/24, only for 10.0.2.0/24",
        "The Transit Gateway VPC attachment is configured with specific subnet associations that include private-app but not private-db",
        "RDS security groups are configured to allow traffic from the VPC CIDR (10.0.0.0/16) but not from the on-premises CIDR (172.16.0.0/16)"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct. When you attach a VPC to a Transit Gateway, you must specify which subnets the Transit Gateway uses for the attachment. This is often misunderstood - the subnet specification isn't about which subnets can send traffic (any subnet with a route to TGW can send), but rather which subnets the Transit Gateway places ENIs into for receiving traffic. If the private-db subnet isn't associated with the Transit Gateway attachment, the Transit Gateway cannot route return traffic into that subnet. The solution is to modify the Transit Gateway VPC attachment to include the private-db subnet. Option A is incorrect because the 'Publicly Accessible' setting only affects whether RDS gets a public IP address and whether it can be accessed from the internet - it doesn't affect private connectivity via Transit Gateway. Option B is incorrect because Transit Gateway route tables work with VPC attachments at the VPC level, not individual subnets. If traffic reaches the private-app subnet, the Transit Gateway route table has a route to the VPC. Option D seems plausible but is incorrect because the problem states security groups allow the traffic, and besides, if this were the issue, you'd see security group denials in VPC Flow Logs, not routing failures. The key lesson is that Transit Gateway subnet associations define where Transit Gateway can deliver traffic in your VPC, and all subnets that need to receive traffic from Transit Gateway must be included in the attachment configuration."
    },
    {
      "question": "A retail company operates a multi-region application using Route 53 with latency-based routing to direct users to the nearest region among us-east-1, eu-west-1, and ap-southeast-1. Each region runs an identical ALB + ECS Fargate setup. Users report that during peak times, they sometimes get routed to a distant region despite their local region being healthy. CloudWatch shows all ALB targets are healthy in all regions. What is the MOST likely cause of this behavior?",
      "options": [
        "Route 53 latency-based routing caches DNS responses for the TTL period (60 seconds default), so users may be directed to a previously-chosen region even after closer regions become available",
        "Route 53 latency-based routing uses network latency from AWS edge locations to the resource, not from the user to the resource, which can differ from the user's actual network path",
        "ALB target health checks only verify target health, not ALB capacity. During peak load, Route 53 cannot detect ALB throttling or elevated response times and continues routing traffic normally",
        "Route 53 charges extra for traffic flow policies, so the company likely uses simple routing instead of latency-based routing with health checks"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct. This is a subtle but important distinction. Route 53 health checks only determine if a resource is UP or DOWN (binary health status). They don't measure or react to performance degradation, increased latency, or throttling at the application level. During peak times, an ALB might be healthy and accepting connections, but if it's approaching its limits or the backend targets are slow, users experience poor performance. Route 53 will continue routing traffic to that region because the health check passes. To solve this, you need: 1) CloudWatch alarms monitoring ALB metrics like TargetResponseTime, HTTPCode_Target_5XX_Count, and RejectedConnectionCount, 2) Calculated health checks in Route 53 that fail based on CloudWatch alarms, not just TCP/HTTP checks, 3) Proper capacity planning for ALB (using Target Request Count Per Target metric). Option A is incorrect - while DNS caching exists, Route 53 latency routing evaluates latency in real-time for each query, and the default TTL is 60 seconds, which wouldn't explain consistent peak-time issues. Option B is incorrect - Route 53 latency-based routing does measure latency from AWS edge locations (which are close to users) to the target resources, providing a good approximation of user latency. Option D is incorrect because Traffic Flow is not required for latency-based routing - it's a built-in Route 53 routing policy. The key takeaway is that Route 53 health checks are binary (healthy/unhealthy) and don't automatically account for degraded performance or capacity issues."
    },
    {
      "type": "multiple",
      "question": "An IoT company processes sensor data from manufacturing facilities in multiple countries. Data must remain in the country of origin due to data residency regulations. They're designing a solution with regional processing in eu-central-1 (Germany), us-east-1 (USA), and ap-southeast-1 (Singapore). Each region has Amazon Kinesis Data Streams for ingestion, Lambda for processing, and DynamoDB for storage. The central compliance team in USA needs read-only access to aggregated metrics (not raw data) from all regions. Which THREE design decisions would meet these requirements? (Select THREE)",
      "options": [
        "Use DynamoDB Global Tables with replication to us-east-1, but configure the table with a custom replication filter that only replicates aggregated metrics tables, not raw data tables",
        "Implement Lambda functions in each region that publish aggregated metrics to CloudWatch Logs, then use CloudWatch Cross-Region Cross-Account functionality to centralize logs in us-east-1",
        "Deploy Amazon Athena in each region to query DynamoDB exports stored in regional S3 buckets, and use S3 Replication to copy only the aggregated query results to a central S3 bucket in us-east-1",
        "Use Amazon EventBridge to send custom events with aggregated metrics from each region to a central event bus in us-east-1, ensuring raw data never leaves the regional boundary",
        "Configure DynamoDB Streams in each region to trigger Lambda functions that calculate metrics and store them in a separate DynamoDB table with Global Tables enabled for replication to us-east-1",
        "Deploy AWS Lake Formation in us-east-1 with cross-region data catalogs pointing to regional data, using column-level security to expose only aggregated metrics"
      ],
      "correctAnswer": [3, 4, 5],
      "explanation": "Options 3, 4, and 5 are correct. Option 3: EventBridge supports cross-region event routing with custom event patterns. Regional Lambda functions can compute aggregated metrics and publish them as custom events to a central event bus in us-east-1. Raw data never leaves the region, and the centralized event bus can trigger analytics workflows. This is scalable and maintains data residency. Option 4: This is an elegant solution - DynamoDB Streams capture changes in the raw data tables, Lambda functions compute running aggregations, and a separate DynamoDB Global Table (containing only aggregated metrics) replicates to us-east-1. Raw data stays regional while aggregated metrics are globally available. Option 5: Lake Formation can create a federated view across regions using Glue Data Catalog federation. You can register S3 locations from multiple regions and use column-level security (via Lake Formation permissions) to expose only specific aggregated views to the compliance team. While more complex, this provides powerful query capabilities. Option 0 is incorrect because DynamoDB Global Tables do not support selective replication filters - you cannot choose which items or tables replicate. It's all-or-nothing at the table level. Option 1 is incorrect because CloudWatch doesn't have built-in cross-region log aggregation (though you can build it with Kinesis Data Firehose subscriptions), and it's not designed as a metrics storage system for business intelligence. Option 2 is partially viable but unnecessarily complex - you'd need to export DynamoDB to S3, run Athena queries, and selectively replicate results. The operational overhead is high compared to real-time streaming solutions. The key principle is to perform aggregation at the regional boundary before any cross-region data transfer to maintain compliance."
    },
    {
      "question": "A financial institution uses AWS Site-to-Site VPN to connect their on-premises data center to a VPC in us-east-1. The VPN connection uses a Virtual Private Gateway and has two tunnels for redundancy. They've noticed that tunnel 1 is always active and carries all traffic, while tunnel 2 remains idle in standby mode. They want to utilize both tunnels simultaneously to increase bandwidth from 1.25 Gbps to 2.5 Gbps. What is the correct approach to achieve active-active VPN tunnel usage?",
      "options": [
        "Configure equal-cost multi-path (ECMP) routing on the customer gateway device and ensure both tunnels advertise the same BGP routes with equal AS PATH length",
        "Modify the VPN connection to use a Transit Gateway instead of a Virtual Private Gateway, enable ECMP support, and configure the customer gateway to advertise identical BGP routes over both tunnels",
        "Change the VPN tunnel options to enable 'Active-Active Mode' in the AWS console, and configure the customer gateway to establish both tunnels with identical BGP attributes",
        "Create two separate VPN connections to the Virtual Private Gateway, each with two tunnels, and configure the customer gateway to load balance across all four tunnels"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct. Virtual Private Gateway does NOT support ECMP routing for Site-to-Site VPN - it always prefers one tunnel over the other based on BGP attributes, using the second tunnel only for failover. To achieve active-active VPN with ECMP, you must use Transit Gateway, which explicitly supports ECMP for VPN connections. With Transit Gateway: 1) Create a VPN attachment with two tunnels, 2) Enable ECMP support (which is default for Transit Gateway), 3) Configure the customer gateway to advertise identical BGP routes (same prefix, same AS PATH, same attributes) over both tunnels, 4) Transit Gateway will load-balance traffic across both tunnels using a hash-based algorithm (typically 5-tuple: source IP, dest IP, source port, dest port, protocol). This effectively doubles your bandwidth. Important: ECMP is per-flow, not per-packet, so a single large file transfer won't use both tunnels - but multiple flows will be distributed. Option A is incorrect because even if you configure ECMP on the customer gateway, the Virtual Private Gateway doesn't support it - it will still prefer one tunnel. Option C is incorrect because there is no 'Active-Active Mode' setting for VPN connections in the AWS console with Virtual Private Gateway. Option D is incorrect because creating multiple VPN connections doesn't help with Virtual Private Gateway - VGW will still prefer one VPN connection over others, and you cannot load balance across them without Transit Gateway. If you need more than 2.5 Gbps, consider AWS Direct Connect or multiple VPN connections to a Transit Gateway (Transit Gateway supports up to 50 Gbps of VPN bandwidth with proper architecture)."
    },
    {
      "question": "A media streaming company has a three-tier application running in multiple Availability Zones in us-east-1. The presentation layer uses CloudFront with ALB as the origin. The application layer runs on ECS Fargate behind the ALB. The data layer uses Aurora PostgreSQL. They want to implement a pilot light DR strategy in eu-west-1 with an RTO of 4 hours and RPO of 30 minutes. Which combination of services provides the MOST cost-effective solution while meeting the RTO/RPO requirements?",
      "options": [
        "Use Aurora Global Database for data replication, S3 + CloudFront for static assets with Cross-Region Replication, and maintain CloudFormation templates for ECS infrastructure. During DR, promote Aurora in eu-west-1, deploy ECS via CloudFormation, and update Route 53",
        "Use Aurora automated backups copied to eu-west-1 every 30 minutes using AWS Backup, store CloudFormation templates in S3 with versioning, and replicate ALB configuration using AWS Config. During DR, restore Aurora from backup, deploy infrastructure from CloudFormation, and update DNS",
        "Implement Aurora Global Database, run a single ECS Fargate task in eu-west-1 for warm standby, enable S3 CRR for static assets, and use Route 53 health checks with automatic failover to eu-west-1 ALB",
        "Use AWS Database Migration Service (DMS) with continuous replication from us-east-1 Aurora to eu-west-1 Aurora, maintain stopped ECS tasks in eu-west-1, enable CloudFront with eu-west-1 as a secondary origin with origin group failover"
      ],
      "correctAnswer": 0,
      "explanation": "Option A is correct and provides the most cost-effective pilot light DR solution. Here's why: 1) Aurora Global Database provides RPO of typically < 1 second (well under 30 minutes) with minimal replication lag, and the cost is only storage + replicated I/O (no compute cost in DR region until you promote). 2) S3 CRR ensures static assets are available in the DR region. 3) CloudFormation templates stored in S3 (or Git) allow rapid infrastructure deployment - with ECS Fargate, you can deploy and start tasks in minutes. 4) Promoting an Aurora Global Database secondary takes ~1 minute, and deploying ECS infrastructure takes 15-30 minutes, easily meeting the 4-hour RTO. 5) Cost is minimal: Aurora storage + replication I/O (~$0.20 per million replicated writes), S3 storage, and S3 replication costs. Option B is incorrect because Aurora automated backups have a minimum frequency of 5 minutes (via continuous backup/PITR), but more importantly, restoring a large Aurora database from backup can take hours and may not meet the 4-hour RTO depending on database size. AWS Backup for Aurora is better suited for compliance than DR. Option C is incorrect because running even a single ECS task makes this a warm standby (not pilot light), increasing costs. Pilot light means minimal resources running - only critical core infrastructure like replicated databases. Option D is incorrect because DMS continuous replication is more expensive than Aurora Global Database for this use case, and maintaining stopped ECS tasks still incurs storage costs. Additionally, CloudFront origin group failover requires health checks, adding complexity. The key distinction: Pilot light = minimal infrastructure (databases, maybe a bastion host), deployed infrastructure from templates during DR. Warm standby = some application servers always running at reduced capacity."
    },
    {
      "type": "multiple",
      "question": "A healthcare SaaS provider operates in a single AWS account with workloads across Development, Staging, and Production environments in us-east-1. Due to compliance requirements, they must migrate to a multi-account strategy using AWS Organizations with separate accounts for Dev, Staging, and Prod. They currently use: VPC peering between environment VPCs, a central VPC for shared services (DNS, AD, proxy), Route 53 private hosted zones associated with all VPCs, and CloudWatch Logs centralized via subscription filters. Which THREE changes are MOST important when migrating to the multi-account structure? (Select THREE)",
      "options": [
        "Replace VPC peering with Transit Gateway across accounts, using Resource Access Manager (RAM) to share the Transit Gateway and separate route tables for environment isolation",
        "Migrate to Route 53 Resolver endpoints in the shared services VPC and create Resolver rules shared via RAM to all accounts for centralized DNS resolution",
        "Reconfigure CloudWatch Logs subscription filters to use cross-account log delivery with destination access policies in the central logging account",
        "Implement AWS SSO with permission sets mapped to Active Directory groups, and use cross-account IAM roles for programmatic access between accounts",
        "Enable AWS Config aggregator in a central governance account to collect compliance data from all accounts, and use AWS Security Hub for centralized security findings",
        "Migrate shared services VPC to a central networking account and use VPC sharing via RAM to extend subnets into Dev, Staging, and Prod accounts"
      ],
      "correctAnswer": [0, 2, 3],
      "explanation": "Options 0, 2, and 3 are correct. Option 0: Transit Gateway is the recommended approach for multi-account networking at scale. You create the Transit Gateway in a central networking account and use AWS RAM to share it with Dev, Staging, and Prod accounts. Each account can attach their VPCs, and you use separate Transit Gateway route tables to maintain isolation between environments (Dev cannot reach Prod). This is more scalable than cross-account VPC peering. Option 2: CloudWatch Logs subscription filters support cross-account delivery, but the destination account (central logging) must have a destination access policy that allows the source accounts to write logs. You need to create CloudWatch Logs destinations in the central account and configure subscription filters in each workload account to send to that destination. This is essential for maintaining centralized logging. Option 3: AWS SSO (now IAM Identity Center) is the recommended way to manage human access across multiple accounts. Permission sets define what users can do in each account. For programmatic access between accounts (e.g., Dev account Lambda calling Prod account S3), use cross-account IAM roles with trust policies. This is a critical security and access management change. Option 1 is a valid architectural choice but not the MOST important - you could also use Route 53 private hosted zones with cross-account association, or Route 53 Resolver endpoints with rules shared via RAM (both work). The question asks for the most important changes. Option 4 is a good practice for governance and compliance but not strictly required for the migration to function - it's more of a best practice enhancement. Option 5 (VPC sharing) is an alternative to Transit Gateway where you create subnets in a central VPC and share them with other accounts. However, it's generally less flexible than Transit Gateway for complex multi-account scenarios, especially when you need strict environment isolation. The key principle is that multi-account architectures require: 1) Network connectivity strategy (Transit Gateway or VPC sharing), 2) Cross-account access controls (IAM roles, SSO), 3) Centralized logging and monitoring with cross-account configurations."
    },
    {
      "question": "A company has a hybrid architecture with an on-premises data center connected via Direct Connect (1 Gbps) to a VPC in us-east-1. They use an on-premises DNS server (bind9) for internal name resolution. EC2 instances in AWS need to resolve on-premises DNS names, and on-premises servers need to resolve AWS private hosted zone records. They've implemented Route 53 Resolver endpoints (inbound and outbound) and configured forwarding rules. However, DNS resolution fails intermittently. VPC Flow Logs show DNS queries (port 53) are accepted by the Route 53 Resolver endpoint security group, but on-premises logs show no DNS queries arriving from AWS. What is the MOST likely cause?",
      "options": [
        "Route 53 Resolver outbound endpoints do not support Direct Connect - they only work with Site-to-Site VPN connections due to DNS protocol limitations",
        "The VPC route table has a route for the on-premises CIDR (172.16.0.0/16) pointing to the Virtual Private Gateway, but the Route 53 Resolver endpoint subnet's route table doesn't have this route",
        "Route 53 Resolver forwarding rules have the wrong target IP address - they must point to the Virtual Private Gateway IP address, not directly to the on-premises DNS server IP",
        "The on-premises firewall is blocking DNS responses from Route 53 Resolver outbound endpoint IPs because they appear as unknown source IPs from the VPC CIDR range"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct, and this is a very common mistake. Route 53 Resolver endpoints are deployed into specific subnets in your VPC, and they use the route table associated with those subnets for routing decisions. Even if your VPC's main route table or other subnet route tables have routes to on-premises via Direct Connect, if the Route 53 Resolver endpoint subnet route table doesn't have these routes, the DNS queries will be dropped or misrouted. The solution: 1) Identify which subnets contain the Route 53 Resolver outbound endpoint ENIs (check the Route 53 Resolver console), 2) Modify the route table associated with those subnets to include routes to your on-premises CIDR via the Virtual Private Gateway or Transit Gateway, 3) Verify return routes exist on-premises to route DNS responses back to the VPC CIDR. Option A is incorrect - Route 53 Resolver works perfectly with Direct Connect; there's no technical limitation requiring VPN. Option C is incorrect - Route 53 Resolver forwarding rules require the IP address of the target DNS server (your on-premises bind9 server), not the VGW. The Route 53 Resolver endpoint handles the routing to reach that IP. Option D is partially plausible but less likely - the problem states VPC Flow Logs show queries are accepted, implying they're leaving the VPC. If the on-premises firewall was blocking them, you'd see the queries arrive but responses blocked. The scenario states on-premises logs show NO queries arriving, indicating a routing issue in AWS, not a firewall issue on-premises. The key lesson: Always check the route tables of the specific subnets where your AWS service endpoints (like Route 53 Resolver, Interface VPC Endpoints, NAT Gateway) are deployed - they need proper routes for bidirectional connectivity."
    },
    {
      "question": "A global logistics company uses a centralized Transit Gateway in us-east-1 connected to 15 VPCs across different business units. They've recently onboarded a new business unit that requires complete network isolation from all other VPCs for security compliance, but this new VPC still needs access to a shared services VPC and on-premises via Direct Connect. The current architecture uses a single Transit Gateway route table with full mesh connectivity between all VPCs. What is the MOST scalable and secure way to achieve this isolation while minimizing operational overhead?",
      "options": [
        "Create a second Transit Gateway in us-east-1 exclusively for the new business unit and the shared services VPC, and peer it with the first Transit Gateway to provide access to on-premises",
        "Create a new Transit Gateway route table for the isolated VPC that only has routes to the shared services VPC attachment and the Direct Connect Gateway attachment, and associate the new VPC attachment with this route table",
        "Implement security groups and NACLs on all VPCs to prevent traffic to/from the new business unit VPC, while allowing shared services and on-premises traffic",
        "Use VPC peering to connect the new business unit VPC directly to the shared services VPC, and create a separate Virtual Private Gateway for Direct Connect connectivity to on-premises"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct and demonstrates proper Transit Gateway route table segmentation. Transit Gateway supports advanced routing through multiple route tables with associations and propagations. Here's the solution: 1) Create a new Transit Gateway route table (e.g., 'Isolated-BU'), 2) Associate the new business unit VPC attachment with this route table, 3) Propagate only the routes from the shared services VPC attachment and the Direct Connect Gateway attachment into this route table, 4) In the shared services VPC's route table, propagate the new business unit VPC so shared services can reach it. This ensures the isolated VPC can only reach shared services and on-premises, but none of the other 15 VPCs. The other VPCs cannot reach the isolated VPC unless explicitly configured. This is scalable because you can add more isolated VPCs to the same route table or create additional route tables for different isolation tiers. Option A is incorrect because creating a second Transit Gateway is expensive (each TGW has a $36/month base cost plus attachment costs) and creates unnecessary complexity. Transit Gateway peering is useful for cross-region connectivity, not for isolation within the same region. Option C is incorrect because using security groups and NACLs to enforce network isolation is a data plane solution to a control plane problem - routes still exist, you're just blocking traffic. It's operationally complex (15 VPCs × security groups/NACLs = lots of rules) and error-prone. Option D is incorrect because it abandons the centralized Transit Gateway architecture, creating operational overhead with VPC peering (which doesn't scale - it's a 1:1 relationship) and a separate VGW just for one VPC. The key principle: Use Transit Gateway route table associations and propagations to implement network segmentation and isolation. Common patterns include: 1) Shared services route table (all VPCs can reach shared services), 2) Isolated route tables (only specific VPCs can be reached), 3) Environment-specific route tables (Dev, Staging, Prod), 4) Egress route tables (for centralized internet egress inspection)."
    },
    {
      "type": "multiple",
      "question": "An e-commerce company operates a multi-region active-active architecture with us-east-1, eu-west-1, and ap-southeast-1. They use Aurora Global Database for orders and DynamoDB Global Tables for inventory. Customers report that sometimes they see items as 'in stock' but get an error when trying to purchase. The architecture uses: Route 53 latency-based routing, Lambda@Edge for inventory checks at CloudFront, and regional API Gateway + Lambda for order processing. Which THREE issues could cause this inconsistency? (Select THREE)",
      "options": [
        "Lambda@Edge reads inventory from the nearest DynamoDB Global Table replica with eventual consistency, but by the time the order is placed (0.5-2 seconds later), another region has sold the last item",
        "Aurora Global Database replication lag causes order transactions in one region to be visible in other regions after a delay, allowing overselling during high-traffic periods",
        "DynamoDB Global Tables use last-writer-wins conflict resolution, so concurrent writes from multiple regions for the same inventory item can result in incorrect inventory counts",
        "Lambda@Edge functions cache inventory data in CloudFront edge locations for 60 seconds (default TTL), causing stale inventory information to be displayed",
        "API Gateway has a built-in caching mechanism that caches Lambda responses for 300 seconds by default, causing stale order availability checks",
        "Route 53 latency-based routing doesn't guarantee all requests from a single user session go to the same region, causing reads and writes to hit different DynamoDB replicas with replication lag"
      ],
      "correctAnswer": [0, 2, 5],
      "explanation": "Options 0, 2, and 5 are correct. Option 0: This is a classic read-after-write consistency issue in global applications. Lambda@Edge runs at CloudFront edge locations and reads from the nearest DynamoDB region. Due to DynamoDB Global Tables' eventual consistency (typically < 1 second but not guaranteed), an item might show as in-stock at the edge, but by the time the order request reaches the regional API Gateway + Lambda (which might be a different region), the item is sold out. Solution: Implement optimistic locking with conditional writes or check inventory again during the order transaction. Option 2: DynamoDB Global Tables use last-writer-wins (LWW) conflict resolution based on timestamps. If two regions simultaneously decrement inventory for the same item (e.g., from 1 to 0), both writes succeed locally, then replicate. Due to LWW, one write wins and the final state might be incorrect (e.g., inventory = 0 or even negative if not using atomic counters). Solution: Use atomic counters with conditional writes (UpdateItem with ADD operation and ConditionExpression to prevent negative values), or implement a two-phase commit pattern. Option 5: Route 53 latency-based routing makes independent decisions for each DNS query. If a user checks inventory (routed to us-east-1) then immediately places an order (DNS resolves again, routed to eu-west-1), they're reading from one DynamoDB replica and writing to another. With replication lag, this causes inconsistency. Solution: Use session affinity/sticky sessions (Route 53 geolocation routing, cookies to bind user to region, or CloudFront origin groups). Option 1 is partially true (Aurora replication lag exists) but the scenario specifically mentions DynamoDB Global Tables for inventory, not Aurora. Aurora is used for orders, which is less likely to cause the described issue. Option 3 is incorrect because Lambda@Edge doesn't have a default 60-second cache - caching behavior depends on CloudFront cache behaviors and your Lambda function implementation. If you're not explicitly caching, data is fresh. Option 4 is incorrect because API Gateway caching is disabled by default - you must explicitly enable it and configure TTL. The key principle for global active-active architectures with inventory/stock: 1) Inventory reads should be treated as estimates, not guarantees, 2) Always verify inventory during the transaction (write operation), 3) Use conditional writes to prevent overselling, 4) Consider single-region writes for critical resources (inventory updates) with global reads, 5) Implement compensation logic for failed transactions."
    },
    {
      "question": "A financial services company has a compliance requirement that all data in transit between AWS and their on-premises data center must be encrypted with FIPS 140-2 validated cryptographic modules. They're currently using AWS Direct Connect (10 Gbps) with a private VIF to a Virtual Private Gateway. The compliance team has flagged that Direct Connect alone does not meet the encryption requirement. What is the MOST performant solution to meet the compliance requirement?",
      "options": [
        "Replace Direct Connect with multiple Site-to-Site VPN connections (each VPN supports 1.25 Gbps, so use 8 VPN tunnels) to a Transit Gateway with ECMP to achieve 10 Gbps encrypted throughput",
        "Implement a Site-to-Site VPN connection over the existing Direct Connect connection (VPN over Direct Connect) to the Virtual Private Gateway, using the private VIF for transport",
        "Migrate from Virtual Private Gateway to Transit Gateway, create a Connect attachment over the Direct Connect connection, and use GRE tunnels with IPsec encryption provided by Transit Gateway Connect",
        "Deploy third-party VPN appliances (e.g., Cisco CSR 1000v) in a VPC, establish VPN tunnels over Direct Connect to on-premises, and route all traffic through these appliances"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct and provides the best performance. Transit Gateway Connect is specifically designed for this use case: high-performance encrypted connectivity over Direct Connect. Here's how it works: 1) You create a Connect attachment on your Transit Gateway, associated with your Direct Connect VPC attachment or Direct Connect Gateway, 2) Transit Gateway Connect uses GRE (Generic Routing Encapsulation) tunnels over the Direct Connect connection, 3) You can establish multiple GRE tunnels (up to 4 per Connect attachment) to achieve high throughput, 4) IPsec encryption can be layered on top of GRE using native Transit Gateway Connect functionality, 5) Supports up to 50 Gbps of throughput (far exceeding the 10 Gbps Direct Connect in this scenario), 6) Uses BGP over GRE for dynamic routing, 7) FIPS 140-2 compliant encryption. This solution provides encryption without sacrificing performance. Option A is technically possible but operationally complex and doesn't fully utilize Direct Connect. Each Site-to-Site VPN tunnel supports 1.25 Gbps, so you'd need 8 tunnels (4 VPN connections × 2 tunnels each) with ECMP to reach 10 Gbps. You'd still pay for Direct Connect but not use it, which is wasteful. Option B (VPN over Direct Connect to VGW) works and meets compliance, but has limitations: 1) Each Site-to-Site VPN tunnel maxes out at 1.25 Gbps, 2) Virtual Private Gateway doesn't support ECMP, so you can't easily aggregate multiple VPN tunnels, 3) Maximum practical throughput is ~2.5 Gbps (2 tunnels), far below the 10 Gbps Direct Connect capacity. Option D works but is expensive and complex: you pay for EC2 instances running VPN appliances (licensing, compute costs), you're responsible for high availability and scaling, and you need to right-size instances for 10 Gbps throughput (which requires very large instance types like c5n.18xlarge). The key lesson: Transit Gateway Connect is the AWS-native, high-performance solution for encrypted traffic over Direct Connect, providing FIPS-compliant encryption at scale. For < 1.25 Gbps requirements, Site-to-Site VPN over Direct Connect is simpler. For > 1.25 Gbps, use Transit Gateway Connect."
    }
  ]
}
