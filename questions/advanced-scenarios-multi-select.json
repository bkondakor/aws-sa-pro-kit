{
  "domain": "Mixed Domains: Advanced Multi-Select Scenarios",
  "task": "Advanced Scenario-Based Multi-Select Questions",
  "question_count": 15,
  "questions": [
    {
      "question": "A financial services company is implementing a defense-in-depth security strategy for their public-facing web application hosted on AWS. The application serves customers globally and must protect against DDoS attacks, SQL injection, and credential stuffing while maintaining low latency. The security team requires visibility into all blocked requests and automatic threat intelligence updates. Which THREE AWS services should be implemented as part of this multi-layered security architecture? (Select THREE)",
      "options": [
        "AWS Shield Advanced with AWS WAF integration for DDoS protection and layer 7 filtering",
        "Amazon CloudFront with geo-restriction and custom SSL certificates",
        "AWS Network Firewall with IDS/IPS rules for deep packet inspection",
        "AWS WAF with managed rule groups including Bot Control and ATP (Account Takeover Prevention)",
        "AWS Firewall Manager to centrally manage WAF rules across accounts",
        "VPC Flow Logs with Amazon Athena for traffic analysis"
      ],
      "type": "multiple",
      "correctAnswer": [0, 1, 3],
      "explanation": "For a comprehensive defense-in-depth security architecture for a public-facing web application, the correct combination is: (1) AWS Shield Advanced provides DDoS protection at layers 3, 4, and 7, and includes 24/7 access to the DDoS Response Team (DRT). When integrated with WAF, it provides enhanced detection and automatic mitigation. (2) Amazon CloudFront acts as the first layer of defense, distributing content globally with low latency, and enables geographic restrictions. CloudFront also integrates tightly with Shield and WAF, allowing security policies to be enforced at edge locations before traffic reaches origin servers. (3) AWS WAF with managed rule groups is essential for protecting against OWASP Top 10 vulnerabilities including SQL injection. The Bot Control managed rule group protects against automated threats, and ATP (Account Takeover Prevention) specifically addresses credential stuffing attacks. These managed rule groups automatically receive threat intelligence updates from AWS Threat Research team. Option C (Network Firewall) operates at the VPC level and is designed for east-west and north-south traffic inspection within VPCs, not for protecting public web applications at the edge - it would add unnecessary latency and cost for this use case. Option E (Firewall Manager) is useful for multi-account management but isn't a security service itself; it's a management layer. In a single-account scenario or where centralized management isn't the primary concern, it's not essential. Option F (VPC Flow Logs) provides network traffic logging but doesn't actively protect against attacks; it's a visibility tool, not a security control. The key insight is that effective defense-in-depth for public web applications requires edge-based protection (CloudFront), DDoS mitigation (Shield Advanced), and application-layer filtering (WAF with managed rules)."
    },
    {
      "question": "An enterprise is designing a hybrid DNS architecture to support 500 VPCs across 4 AWS regions and 3 on-premises data centers. They need bidirectional DNS resolution where AWS resources can resolve on-premises domain names and on-premises systems can resolve private Route 53 hosted zones. They want centralized management and minimal operational overhead. Which THREE components are required for this architecture? (Select THREE)",
      "options": [
        "Route 53 Resolver inbound endpoints in a central networking VPC in each region",
        "Route 53 Resolver outbound endpoints in every VPC that needs to resolve on-premises names",
        "Route 53 Resolver rules shared via AWS RAM to all accounts and associated with VPCs",
        "AWS Transit Gateway with DNS resolution support enabled on VPC attachments",
        "Route 53 private hosted zones associated with all VPCs",
        "VPN connections from each VPC to each on-premises data center for DNS traffic"
      ],
      "type": "multiple",
      "correctAnswer": [0, 2, 3],
      "explanation": "For a scalable hybrid DNS architecture, the correct components are: (1) Route 53 Resolver inbound endpoints in a central VPC allow on-premises DNS servers to forward queries for AWS private hosted zones to Route 53 Resolver. These endpoints should be deployed in a central networking VPC with redundancy across multiple AZs. On-premises DNS servers configure conditional forwarders pointing to these inbound endpoint IP addresses. (2) Route 53 Resolver rules define which domains should be forwarded to on-premises DNS servers. These rules are created in the central networking account and shared via AWS RAM (Resource Access Manager) to all member accounts. Once shared, each account must associate the rules with their VPCs. This centralized approach eliminates the need to create duplicate rules in every account. The rules point to outbound endpoints for forwarding queries. (3) AWS Transit Gateway with DNS support enabled on VPC attachments is crucial for this architecture. When you enable DNS support on TGW VPC attachments, DNS queries from VPCs can route through the Transit Gateway to reach the central VPC containing the Resolver endpoints. This eliminates the need for VPC peering mesh or redundant endpoint deployments. Option B is incorrect because outbound endpoints should be deployed centrally in the networking VPC, not in every VPC - this is the whole point of the centralized architecture. Resolver rules (shared via RAM) reference the central outbound endpoints. Option E, while private hosted zones are part of the solution, they're not a specific 'component to implement' for the hybrid DNS architecture itself; they're the resources being resolved. Option F (VPN from every VPC) is the opposite of centralized architecture and creates massive operational overhead. The key learning is that Route 53 Resolver with Transit Gateway integration enables hub-and-spoke DNS architecture at scale, and RAM sharing of resolver rules eliminates rule duplication across hundreds of accounts."
    },
    {
      "question": "A global SaaS company is implementing a multi-region disaster recovery strategy for their critical database workload currently running on Amazon Aurora PostgreSQL in us-east-1. The RTO is 15 minutes and RPO is 5 minutes. They require the ability to promote a secondary region to primary with minimal data loss and automatic failover capability. The solution must support read scalability in the DR region during normal operations. Which THREE components should be implemented? (Select THREE)",
      "options": [
        "Aurora Global Database with a secondary region in eu-west-1",
        "Aurora Cross-Region Read Replica with manual promotion process",
        "Amazon RDS Proxy in both regions for connection management and failover",
        "Route 53 health checks with failover routing policy for application endpoints",
        "AWS Backup with cross-region backup copy for point-in-time recovery",
        "Aurora Multi-AZ deployment in the primary region"
      ],
      "type": "multiple",
      "correctAnswer": [0, 2, 3],
      "explanation": "To meet the RTO of 15 minutes and RPO of 5 minutes with multi-region DR, the correct architecture includes: (1) Aurora Global Database provides the best solution for multi-region disaster recovery. It uses dedicated infrastructure for replicating data to secondary regions with typical lag under 1 second (far better than the 5-minute RPO requirement). Global Database supports managed planned failover (RPO of 0) and unplanned failover (RPO typically less than 1 second). The failover process to promote a secondary region can complete in under 1 minute for the database itself, helping meet the 15-minute RTO. Additionally, the secondary region supports up to 16 read replicas that can serve read traffic during normal operations, meeting the read scalability requirement. (2) Amazon RDS Proxy in both regions is crucial for meeting the 15-minute RTO. RDS Proxy pools and shares database connections, and when an Aurora failover occurs, RDS Proxy maintains connections and automatically directs them to the new primary instance, significantly reducing connection storm issues and application recovery time. Deploying RDS Proxy in both regions ensures applications in each region can connect efficiently to whichever region is currently primary. (3) Route 53 health checks with failover routing policy enable automated DNS-level failover for the application tier. Health checks monitor the application endpoints in the primary region, and when failures are detected, Route 53 automatically updates DNS to point to the DR region endpoint. This provides end-to-end automated failover capability at the application level, complementing the database failover. Option B (cross-region read replica) has a manual promotion process and doesn't meet the automated failover requirement, plus the replication lag is typically higher than Global Database. Option E (AWS Backup with cross-region copy) provides point-in-time recovery but cannot meet a 15-minute RTO as restoring a database from backup takes much longer. It's suitable for backup/archival but not for the primary DR strategy with these aggressive RTO/RPO requirements. Option F (Multi-AZ in primary region) protects against AZ failures within a region but doesn't provide multi-region DR capability. The combination of Aurora Global Database, RDS Proxy in both regions, and Route 53 failover routing provides automated, fast failover with minimal data loss while also enabling read scalability in the DR region."
    },
    {
      "question": "A large organization with 800 AWS accounts needs to implement centralized security monitoring and compliance automation. They require automated security assessments, continuous compliance monitoring against CIS AWS Foundations Benchmark, vulnerability scanning of EC2 instances and container images, and a unified dashboard for the security team. Which THREE AWS services should be configured with delegated administrator architecture? (Select THREE)",
      "options": [
        "AWS Security Hub with delegated administrator in a central security account",
        "Amazon GuardDuty with delegated administrator and automatic member enablement",
        "AWS Config with conformance packs deployed via CloudFormation StackSets",
        "Amazon Inspector with delegated administrator for EC2 and ECR scanning",
        "AWS Systems Manager with OpsCenter for centralized operational insights",
        "Amazon Detective with cross-account investigation capabilities"
      ],
      "type": "multiple",
      "correctAnswer": [0, 1, 3],
      "explanation": "For comprehensive centralized security monitoring across 800 accounts, the correct services are: (1) AWS Security Hub with delegated administrator provides a centralized view of security findings and compliance status across all accounts and regions. Security Hub aggregates findings from GuardDuty, Inspector, IAM Access Analyzer, Macie, and third-party security tools into a unified dashboard. It includes built-in compliance standards including CIS AWS Foundations Benchmark, PCI DSS, and AWS Foundational Security Best Practices. The delegated administrator architecture allows a central security account to manage Security Hub across all member accounts without using the management account. Security Hub automatically normalizes findings across services into a standard format (AWS Security Finding Format) for easier analysis. (2) Amazon GuardDuty with delegated administrator provides intelligent threat detection using machine learning to analyze VPC Flow Logs, CloudTrail events, and DNS logs across all accounts. The delegated administrator can automatically enable GuardDuty in new accounts as they join the organization (through auto-enable feature), ensuring comprehensive coverage without manual intervention per account. GuardDuty findings automatically flow into Security Hub for centralized visibility. At 800 accounts, the automated enablement feature is crucial for operational efficiency. (3) Amazon Inspector with delegated administrator provides automated vulnerability management by continuously scanning EC2 instances, container images in ECR, and Lambda functions for software vulnerabilities and network exposure. Inspector automatically discovers resources, scans them, and reports findings. The delegated administrator model allows central management, and Inspector findings also integrate with Security Hub for unified visibility. Inspector is specifically designed for vulnerability scanning, which is a stated requirement. Option C (AWS Config with StackSets) would work for compliance monitoring, but it's more complex to manage at scale compared to Security Hub's built-in compliance standards. Security Hub actually aggregates Config findings, so deploying Config separately via StackSets is an additional operational burden when Security Hub already provides the centralized compliance dashboard. Option E (Systems Manager OpsCenter) is for operational issue management, not security monitoring. Option F (Detective) is for security investigation and forensics, which is valuable but not part of the core requirements for continuous monitoring and compliance assessment. The key architecture insight is that Security Hub acts as the aggregation layer for findings from GuardDuty (threat detection) and Inspector (vulnerability scanning), while also providing compliance monitoring through its security standards feature, making these three services the optimal combination for the stated requirements with minimal operational overhead."
    },
    {
      "question": "A media company is migrating a large-scale video processing application to AWS. The application processes 10 PB of video files stored in an on-premises NFS file system. They need to migrate the data to Amazon S3 with minimal downtime, automatically transition older files to cheaper storage tiers, and implement a lifecycle policy where files older than 90 days move to S3 Glacier, and files older than 2 years are deleted. Which THREE approaches should be combined for this migration and lifecycle management? (Select THREE)",
      "options": [
        "AWS DataSync for initial bulk transfer and ongoing synchronization of the 10 PB dataset",
        "AWS Storage Gateway File Gateway to provide NFS access to S3 during migration",
        "AWS Snowball Edge devices for offline data transfer of the initial 10 PB",
        "S3 Lifecycle policies with transitions to S3 Glacier after 90 days and expiration after 2 years",
        "S3 Intelligent-Tiering for automatic cost optimization based on access patterns",
        "AWS Transfer Family (SFTP) to enable file uploads from on-premises systems"
      ],
      "type": "multiple",
      "correctAnswer": [0, 2, 3],
      "explanation": "For migrating 10 PB of data and implementing lifecycle management, the correct approach combines: (1) AWS Snowball Edge devices for the initial bulk transfer. Transferring 10 PB over the network, even with a 10 Gbps connection, would take months (10 PB = 10,000 TB; at 1 Gbps effective throughput = 10,000+ hours = over 400 days). Snowball Edge devices can hold up to 80 TB each, so you'd need around 125 devices, but AWS allows parallel shipments. Each device can transfer data at local network speeds, and once shipped to AWS, the data is loaded into S3. This approach reduces the initial migration time from months to weeks. (2) AWS DataSync for ongoing synchronization after the bulk transfer. While Snowball handles the initial 10 PB, DataSync is perfect for incremental synchronization of changed and new files. DataSync can be deployed on-premises as a VM or hardware appliance, continuously monitors the source NFS share, and efficiently transfers only changed data to S3. This enables a cutover strategy where: Snowball transfers the bulk data, DataSync synchronizes changes made during the Snowball transfer period, and then final cutover happens with DataSync ensuring all recent changes are captured with minimal downtime. DataSync also automatically handles retries, data verification, and can maintain file metadata. (3) S3 Lifecycle policies provide automated, policy-based data lifecycle management. You configure a single lifecycle rule on the S3 bucket that specifies: transition objects to S3 Glacier Deep Archive (or S3 Glacier Flexible Retrieval) 90 days after creation, and permanently delete objects 2 years (730 days) after creation. This is declarative, requires no custom code, and automatically applies to all current and future objects. S3 lifecycle policies are the native, cost-effective way to implement time-based storage tiering and deletion. Option B (Storage Gateway File Gateway) provides NFS access to S3, which could be useful if the on-premises application needs ongoing NFS access to S3, but the scenario describes a migration where files are moving to S3, not an ongoing hybrid storage architecture. It also doesn't help with the 10 PB initial transfer challenge. Option E (S3 Intelligent-Tiering) automatically moves objects between access tiers based on access patterns (not accessed for 30 days → Infrequent Access tier, etc.), but the requirements specify a time-based policy (90 days → Glacier), not an access-pattern-based policy. Intelligent-Tiering also doesn't support automatic deletion based on age. Option F (Transfer Family) is for enabling SFTP/FTPS/FTP access to S3, which isn't relevant for migrating from an NFS file system - the source is NFS, not FTP-based. The architecture combines offline bulk transfer (Snowball) for the initial dataset, online incremental sync (DataSync) for changes and final cutover, and S3-native lifecycle policies for long-term automated tiering and deletion."
    },
    {
      "question": "A startup is building a serverless microservices application on AWS using Amazon ECS with Fargate. The application consists of 15 microservices that need to communicate securely with each other, with service discovery, mutual TLS authentication, and detailed observability into service-to-service communication. The team wants to avoid managing service mesh infrastructure. Which THREE AWS services/features should be implemented? (Select THREE)",
      "options": [
        "AWS App Mesh for service mesh capabilities with Envoy proxy sidecars",
        "AWS Cloud Map for service discovery and health checking",
        "Amazon ECS Service Connect for simplified service-to-service communication",
        "Application Load Balancer with host-based routing for each microservice",
        "AWS X-Ray for distributed tracing across microservices",
        "Amazon CloudWatch Container Insights for container-level metrics and logs"
      ],
      "type": "multiple",
      "correctAnswer": [2, 4, 5],
      "explanation": "For a serverless microservices architecture on ECS Fargate with the stated requirements, the optimal combination is: (1) Amazon ECS Service Connect provides service discovery, service-to-service communication, and traffic management specifically designed for ECS workloads. Launched in late 2022, Service Connect simplifies microservices networking by automatically handling service discovery, client-side load balancing, and traffic routing without requiring separate service mesh infrastructure. It deploys a lightweight proxy as a sidecar (based on Envoy) but is fully managed by AWS - you don't operate the control plane. Service Connect integrates with AWS Cloud Map for namespace management but abstracts away the complexity. Importantly, it provides traffic metrics and logs for observability. While it doesn't natively provide mutual TLS in all configurations, it significantly simplifies service-to-service communication compared to manual service mesh operation, aligning with the 'avoid managing infrastructure' requirement. (2) AWS X-Ray provides distributed tracing across the microservices architecture. By instrumenting your application code with the X-Ray SDK, you can trace requests as they flow through the 15 microservices, identifying bottlenecks, latency issues, and errors. X-Ray creates a service map showing dependencies and performance characteristics of each service, provides end-to-end request tracing with detailed timing for each service call, and integrates with ECS/Fargate container metadata. For observability into service-to-service communication patterns and troubleshooting distributed transactions, X-Ray is essential. (3) Amazon CloudWatch Container Insights provides comprehensive monitoring for containerized applications. It automatically collects metrics at the cluster, service, and task level including CPU, memory, disk, and network utilization. Container Insights also aggregates logs from all containers and provides a unified dashboard. For ECS on Fargate, it gives visibility into resource utilization, performance metrics, and helps with troubleshooting. Combined with X-Ray (application-level tracing) and Service Connect (network-level metrics), Container Insights provides infrastructure-level observability, completing the observability stack. Option A (AWS App Mesh) is a full-featured service mesh that provides mutual TLS, advanced traffic management, and observability, but requires managing the App Mesh control plane, configuring Envoy proxies, and more operational overhead. The question specifically states 'avoid managing service mesh infrastructure,' making Service Connect a better fit as it provides managed service mesh capabilities without the operational complexity. If strict mutual TLS is absolutely required, App Mesh would be necessary, but Service Connect meets most requirements with less overhead. Option B (AWS Cloud Map) is used by Service Connect under the hood, but you don't configure it directly when using Service Connect - it's abstracted away. Separately deploying Cloud Map would be needed if you're building a custom service discovery solution, but Service Connect handles this. Option D (Application Load Balancer with host-based routing) provides external-facing load balancing but doesn't provide service discovery, mutual TLS, or service mesh capabilities for service-to-service communication. You'd still need ALB for ingress traffic from the internet, but it's not part of the internal microservices communication architecture. The key insight is that ECS Service Connect is purpose-built for simplifying microservices on ECS/Fargate, X-Ray provides application-level distributed tracing, and Container Insights provides infrastructure monitoring - together they provide comprehensive observability without heavy service mesh operational burden."
    },
    {
      "question": "An e-commerce company running a monolithic application on EC2 instances is planning to modernize to a containerized microservices architecture. They need to minimize refactoring, support both containerized and non-containerized components during migration, enable gradual traffic shifting between old and new versions, and maintain session affinity. Which THREE AWS services/features should be part of the migration strategy? (Select THREE)",
      "options": [
        "Amazon ECS with Application Load Balancer target groups for blue/green deployments",
        "AWS App Runner for automatic container deployment with built-in load balancing",
        "AWS CodeDeploy with ECS blue/green deployment support for gradual traffic shifting",
        "Amazon API Gateway with VPC Link for routing traffic to both EC2 and container backends",
        "AWS Lambda with Application Load Balancer integration for serverless microservices",
        "Elastic Beanstalk with Docker platform for simplified container deployment"
      ],
      "type": "multiple",
      "correctAnswer": [0, 2, 3],
      "explanation": "For a migration from monolithic EC2 to containerized microservices with the stated requirements, the optimal combination is: (1) Amazon ECS with Application Load Balancer provides the container orchestration platform and routing capabilities needed. ECS allows you to run containerized microservices on either EC2 or Fargate. ALB target groups can route traffic to ECS tasks (containers) while supporting advanced routing rules (path-based, host-based), and crucially, ALB supports sticky sessions (session affinity) using cookie-based stickiness. You can configure multiple target groups behind a single ALB - some pointing to legacy EC2 instances and others to ECS containers - enabling gradual migration. ALB also integrates with ECS service discovery and health checks. (2) AWS CodeDeploy with ECS blue/green deployment support enables safe, controlled deployments with gradual traffic shifting. CodeDeploy for ECS supports blue/green deployments where a new version of the service (green) is deployed alongside the old version (blue), and traffic is gradually shifted from blue to green using linear or canary strategies (e.g., 10% every 10 minutes). This allows validation and monitoring before full cutover, with automatic rollback if CloudWatch alarms trigger. This is essential for minimizing risk during the microservices migration - each service can be migrated and deployed independently with gradual traffic shifting. (3) Amazon API Gateway with VPC Link provides a unified API front-end that can route requests to both legacy EC2 backends (via VPC Link to NLB/ALB) and new containerized microservices. During the migration, you can use API Gateway to implement the strangler fig pattern: route specific API paths to new containerized microservices while routing other paths to the legacy monolith. API Gateway also provides API management capabilities like throttling, authentication (Cognito, Lambda authorizers), request/response transformation, and caching. VPC Link enables private connectivity from API Gateway to resources in your VPC without internet exposure. This architecture allows gradual decomposition of the monolith where API endpoints are migrated one at a time to new microservices. Option B (AWS App Runner) is a simplified container service but doesn't provide the level of control needed for gradual migration strategies, doesn't support blue/green deployments with traffic shifting, and is designed for simple containerized web apps rather than complex migration scenarios. Option D (Lambda with ALB) could be part of the architecture for some microservices, but the question asks about containerized microservices, not serverless functions. Lambda is valuable for certain use cases but doesn't address the core requirement of container orchestration. Option F (Elastic Beanstalk with Docker) provides simplified container deployment but lacks the advanced deployment strategies (gradual traffic shifting, blue/green) that CodeDeploy offers, and doesn't provide the API management and routing flexibility of API Gateway for managing the migration from monolith to microservices. The architecture uses ECS for container orchestration, CodeDeploy for safe deployments with traffic shifting, and API Gateway as the API management layer that can route to both old and new backends during the migration period, enabling the strangler fig pattern for gradual modernization."
    },
    {
      "question": "A financial institution must implement a multi-account AWS environment with strict compliance requirements including: encryption of all data at rest and in transit, centralized key management with automatic key rotation, audit logging of all key usage, and the ability to disable keys across all accounts immediately in case of a security incident. Which THREE approaches should be implemented? (Select THREE)",
      "options": [
        "AWS KMS with customer managed keys (CMKs) created in each account and automatic key rotation enabled",
        "AWS KMS with a centralized key management account and cross-account key policies for resource access",
        "AWS CloudTrail with CloudWatch Logs integration for monitoring KMS API calls across all accounts",
        "AWS Secrets Manager for storing and rotating encryption keys automatically",
        "AWS Certificate Manager for managing SSL/TLS certificates with automatic renewal",
        "AWS Security Hub with AWS Config rules to ensure encryption at rest compliance"
      ],
      "type": "multiple",
      "correctAnswer": [1, 2, 5],
      "explanation": "For enterprise-grade centralized key management with compliance and security requirements, the correct approach includes: (1) AWS KMS with a centralized key management account provides the best architecture for centralized control. In this model, customer managed keys (CMKs) are created in a dedicated security/key management account, and cross-account access is granted through key policies and grants. This allows the security team to maintain central control over keys while application accounts can use them for encryption/decryption. Key policies can specify which accounts and roles can use keys for which operations. Crucially, this architecture enables the security team to immediately disable or delete keys from the central account in case of a security incident, and the effect is instantaneous across all consuming accounts. Automatic key rotation can be enabled on these centralized keys, rotating the backing key material annually while maintaining the same key ID (ensuring no application changes are needed). (2) AWS CloudTrail with CloudWatch Logs integration provides comprehensive audit logging of all KMS API calls. Every KMS operation (Encrypt, Decrypt, GenerateDataKey, etc.) generates a CloudTrail event that includes details about who made the call, from which account, which key was used, and the timestamp. By configuring an organization trail, you capture KMS API calls across all accounts in a centralized S3 bucket. Integrating CloudTrail with CloudWatch Logs enables real-time monitoring and alerting - you can create metric filters and alarms to detect anomalous KMS usage patterns, unauthorized access attempts, or policy violations. This satisfies the audit logging requirement and provides the security team with visibility into how keys are being used across the organization. (3) AWS Security Hub with AWS Config rules ensures continuous compliance monitoring for encryption at rest. AWS Config rules can be deployed organization-wide (via Config Organizational Rules or StackSets) to check that resources like EBS volumes, S3 buckets, RDS databases, and DynamoDB tables are encrypted. Security Hub aggregates findings from these Config rules into a centralized dashboard and maps them to compliance frameworks (PCI DSS, CIS, etc.). This provides automated, continuous compliance assessment rather than manual periodic audits, and Security Hub can trigger automated remediation through EventBridge when encryption violations are detected. Option A (keys in each account) is the opposite of centralized management and makes incident response difficult - you'd need to disable keys in hundreds of accounts individually. It also makes governance and oversight harder. Option C is close, but the question is about CloudTrail with CloudWatch integration, which is covered in Option B - CloudTrail alone provides audit logs but adding CloudWatch enables real-time monitoring and alerting. Option D (Secrets Manager) is for storing secrets like database passwords and API keys, not for encryption key management - that's KMS's role. Secrets Manager actually uses KMS for encryption. Option E (Certificate Manager) manages SSL/TLS certificates for HTTPS/TLS connections, which addresses encryption in transit for web traffic, but doesn't provide centralized key management for data at rest encryption or the ability to disable keys across all accounts. ACM is a component of a complete solution but isn't one of the core three for centralized key management. The architecture combines centralized KMS key creation and management, comprehensive CloudTrail auditing with real-time monitoring via CloudWatch, and continuous compliance monitoring through Security Hub and Config rules, providing defense-in-depth for key management and encryption compliance."
    },
    {
      "question": "A healthcare organization needs to migrate a 50 TB SQL Server database from on-premises to AWS with minimal downtime. The database is subject to HIPAA compliance, has high transaction volume (10,000 transactions/second), and the migration window allows only 4 hours of downtime. Post-migration, they need automated backups, point-in-time recovery, and the ability to create development database clones quickly. Which THREE components should be part of the migration and operational strategy? (Select THREE)",
      "options": [
        "AWS Database Migration Service (DMS) with ongoing replication for minimal downtime migration",
        "AWS Snowball to transfer the initial 50 TB database dump offline",
        "Amazon RDS for SQL Server with automated backups and Multi-AZ deployment",
        "AWS Backup for centralized backup management across all database instances",
        "Amazon Aurora PostgreSQL with Babelfish for SQL Server compatibility",
        "Amazon RDS read replicas for creating development database clones"
      ],
      "type": "multiple",
      "correctAnswer": [0, 2, 3],
      "explanation": "For migrating a large, high-transaction SQL Server database with minimal downtime and specific operational requirements, the correct approach is: (1) AWS Database Migration Service (DMS) provides the capability for minimal downtime migration through continuous data replication. The migration strategy would be: perform an initial full load of the database to RDS (which can happen while the source database remains operational), enable ongoing replication/CDC (Change Data Capture) to continuously replicate transactions from the source to target, monitor replication lag until caught up, perform a brief cutover during the 4-hour window to switch applications to the RDS database. DMS supports SQL Server as both source and target, handles schema migration, and can replicate ongoing transactions with low latency. For a 10,000 TPS workload, DMS can be configured with multiple tasks and appropriate instance sizing to handle the throughput. The 50 TB initial load can be expedited using DMS with Snowball Edge (though this adds complexity) or by using a dedicated DMS replication instance with sufficient network bandwidth. (2) Amazon RDS for SQL Server is the managed database service that eliminates operational overhead. RDS provides automated backups with configurable retention (up to 35 days), automatic point-in-time recovery to any second within the retention period, automated patching, Multi-AZ deployment for high availability, and performance monitoring. For HIPAA compliance, RDS supports encryption at rest using KMS and encryption in transit using SSL/TLS. RDS can handle 10,000 TPS with appropriate instance sizing (e.g., db.r6i.8xlarge or larger). The managed nature of RDS means the healthcare organization doesn't need to manage SQL Server licensing, OS patching, or backup infrastructure - AWS handles these operational tasks. (3) AWS Backup provides centralized backup management, which is particularly valuable for HIPAA compliance and governance. While RDS has built-in automated backups, AWS Backup adds a centralized backup policy layer across the organization. You can define backup plans with retention policies, copy backups to other regions for DR, apply backup policies across multiple accounts using AWS Organizations, enforce backup compliance with backup vault lock (preventing deletion of backups for compliance), and generate backup compliance reports for audits. For HIPAA compliance, AWS Backup's vault lock feature is particularly valuable as it enforces WORM (Write Once Read Many) for backups, meeting regulatory requirements. AWS Backup also supports cross-account and cross-region backup copies for additional protection. Option B (Snowball for initial transfer) could work but adds complexity to the migration - you'd need to create a database dump, ship it via Snowball, restore to RDS, then set up DMS for the catch-up replication. For a 50 TB database with good network connectivity (1-10 Gbps), DMS can transfer the initial load over the network in a reasonable timeframe (a few days), and since ongoing replication keeps the databases in sync, the total timeline still fits the minimal downtime requirement. Snowball is more suitable when network bandwidth is severely constrained. Option D (Aurora PostgreSQL with Babelfish) is an alternative to RDS SQL Server where Babelfish provides SQL Server wire protocol compatibility, allowing SQL Server applications to connect to Aurora PostgreSQL. However, Babelfish has limitations and may require application testing and potential code changes for compatibility. Given the high transaction volume and 4-hour cutover window, migrating to RDS SQL Server (same database engine) is lower risk than migrating to a different engine (PostgreSQL), even with Babelfish compatibility. Option E is incorrect - RDS for SQL Server does not support read replicas. Read replicas are available for PostgreSQL, MySQL, and MariaDB, but not for SQL Server. For creating development database clones quickly, RDS supports database snapshots that can be restored to new instances (though this takes time for 50 TB), or you could use RDS Blue/Green deployments for cloning. The optimal strategy uses DMS for minimal-downtime migration with continuous replication, RDS for SQL Server as the managed target platform with built-in HA and PITR, and AWS Backup for centralized, compliant backup management meeting HIPAA requirements."
    },
    {
      "question": "A global media company needs to implement a multi-region content delivery solution that serves 4K video content to users worldwide with low latency, provides real-time analytics on content performance and viewer behavior, and protects video content from unauthorized access and hotlinking. The solution must also reduce origin load by maximizing cache hit ratio. Which THREE AWS services/features should be implemented? (Select THREE)",
      "options": [
        "Amazon CloudFront with field-level encryption for protecting sensitive viewer data",
        "CloudFront signed URLs and signed cookies with custom policies for content access control",
        "AWS Elemental MediaPackage for video packaging and origin protection",
        "Amazon CloudWatch with CloudFront standard logs for basic monitoring",
        "Amazon Kinesis Data Streams with CloudFront real-time logs for detailed analytics",
        "AWS WAF with rate-based rules to prevent content scraping and DDoS"
      ],
      "type": "multiple",
      "correctAnswer": [1, 2, 4],
      "explanation": "For a global content delivery solution with access control, analytics, and origin protection, the correct architecture includes: (1) CloudFront signed URLs and signed cookies provide robust content access control, preventing unauthorized access and hotlinking. Signed URLs/cookies use cryptographic signing with a private key (you control the key pair) to authorize access to content. You can create custom policies that specify: who can access content (IP address restrictions), when content can be accessed (time-based expiration), which content can be accessed (URL patterns). This prevents hotlinking because each URL is signed and time-limited, preventing other websites from embedding direct links to your content. The application server generates signed URLs/cookies after authenticating users, and CloudFront validates the signature before serving content. For a media company with premium 4K content, this is essential for access control and monetization. (2) AWS Elemental MediaPackage provides video packaging, origin protection, and improved cache efficiency. MediaPackage acts as a just-in-time video packager and origin for CloudFront, converting video from a single source format into multiple adaptive bitrate (ABR) formats (HLS, DASH, CMAF) dynamically. It includes origin protection features like origin access control, preventing direct access to the origin and ensuring all requests come through CloudFront. MediaPackage also implements time-shifted viewing and DVR-like functionality. Critically, it optimizes cache hit ratio by generating consistent segment URLs and supporting large manifest files, which improves CloudFront caching efficiency. For 4K video delivery at scale, MediaPackage handles the complexity of adaptive bitrate streaming while reducing origin load. (3) Amazon Kinesis Data Streams with CloudFront real-time logs provides detailed, real-time analytics on content performance and viewer behavior. CloudFront real-time logs deliver log data within seconds of a viewer request to Kinesis Data Streams, where you can process them with Kinesis Data Analytics, Lambda, or custom applications. Real-time logs include detailed fields like cache behavior, time to first byte, protocol, country, device type, and more. This enables real-time dashboards showing which content is popular, geographic distribution of viewers, cache hit ratios, error rates, and performance metrics. Compared to standard logs (which have 60-minute delay and are written to S3), real-time logs enable immediate operational insights and can trigger alerts for performance degradation or unusual access patterns. For a media company, real-time analytics on viewer behavior and content performance are critical for content strategy and operations. Option A (field-level encryption) protects specific sensitive fields in POST requests (like credit card numbers) during the request lifecycle, but isn't relevant for protecting video content delivery - signed URLs/cookies are the appropriate mechanism for video access control. Option D (CloudWatch with standard logs) provides basic monitoring but standard CloudFront logs are delivered to S3 with a delay of up to 60 minutes, which doesn't meet the 'real-time analytics' requirement. CloudWatch can provide some metrics, but detailed viewer behavior analytics require the full log data that real-time logs provide to Kinesis. Option F (AWS WAF with rate-based rules) provides security against scraping and DDoS, which is valuable, but the primary requirements are content access control (addressed by signed URLs/cookies) and analytics (addressed by real-time logs). WAF would be a good additional layer but isn't one of the core three components for the stated requirements. The architecture combines signed URLs/cookies for access control and anti-hotlinking, MediaPackage for efficient video packaging and origin protection, and Kinesis with real-time logs for detailed analytics, creating a comprehensive content delivery solution."
    },
    {
      "question": "A financial services company is implementing a cost optimization strategy for their AWS environment with 300 accounts. They need to achieve 30% cost reduction through a combination of right-sizing underutilized resources, implementing automated scheduling for non-production environments, and maximizing commitment-based discounts. They require visibility into cost allocation by business unit and project. Which THREE AWS services/features should be implemented? (Select THREE)",
      "options": [
        "AWS Cost Explorer with rightsizing recommendations and reserved instance purchase recommendations",
        "AWS Compute Optimizer for EC2, Lambda, and EBS volume rightsizing analysis",
        "AWS Cost Anomaly Detection with automated alerts for unusual spending patterns",
        "AWS Budgets with budget actions to automatically stop resources exceeding thresholds",
        "AWS Organizations with consolidated billing and Cost Allocation Tags for business unit tracking",
        "AWS Savings Plans (Compute and EC2 Instance Savings Plans) for commitment-based discounts"
      ],
      "type": "multiple",
      "correctAnswer": [1, 4, 5],
      "explanation": "For a comprehensive cost optimization strategy achieving 30% reduction across 300 accounts, the optimal combination is: (1) AWS Compute Optimizer provides machine learning-powered rightsizing recommendations that go beyond simple metrics. Compute Optimizer analyzes historical utilization data (CloudWatch metrics) for EC2 instances, Auto Scaling groups, EBS volumes, and Lambda functions to recommend optimal resource configurations. Unlike Cost Explorer's basic rightsizing (which looks primarily at CPU), Compute Optimizer analyzes CPU, memory, disk I/O, and network patterns to provide more accurate recommendations. It identifies over-provisioned resources and can recommend downsizing instances, changing instance families, or moving to Graviton processors. For Lambda, it recommends optimal memory configurations. At scale across 300 accounts, Compute Optimizer can identify significant savings opportunities - studies show 20-30% average savings from rightsizing alone. Compute Optimizer can be enabled organization-wide and provides recommendations centrally. (2) AWS Organizations with consolidated billing and Cost Allocation Tags provides the foundation for cost visibility and allocation. Consolidated billing across 300 accounts enables: single bill with aggregated usage for volume discounts, shared Reserved Instance and Savings Plans benefits across accounts, centralized payment method, and cost reporting at organization/OU/account levels. Cost Allocation Tags (both AWS-generated and user-defined tags) enable cost tracking by business unit, project, environment, cost center, and application. You can activate tags organization-wide and then use Cost Explorer and AWS Cost and Usage Reports to filter, group, and analyze costs by these dimensions. For a 300-account enterprise, this visibility is essential for chargeback/showback models and identifying which business units or projects are driving costs. Without proper tagging and consolidated billing, achieving organization-wide cost optimization is nearly impossible. (3) AWS Savings Plans (Compute and EC2 Instance Savings Plans) provide commitment-based discounts that can deliver 20-40% savings compared to On-Demand pricing. Savings Plans require committing to a consistent amount of usage (measured in $/hour) for a 1- or 3-year term. Compute Savings Plans (most flexible) apply to EC2, Fargate, and Lambda across any region, instance family, size, OS, or tenancy. EC2 Instance Savings Plans (less flexible but higher discount) apply to a specific instance family in a specific region. For a large organization with steady-state workloads, Savings Plans should be purchased to cover the baseline usage, with On-Demand for variable workloads. Organizations can share Savings Plans benefits across accounts through consolidated billing. Combined with rightsizing (reducing the baseline) and non-production scheduling (reducing hours), Savings Plans on the optimized baseline can contribute 15-25% of the 30% target reduction. Option A (Cost Explorer) is valuable and includes some rightsizing recommendations, but Compute Optimizer provides more comprehensive and accurate ML-powered recommendations across more service types, making it the better choice for the rightsizing component. Cost Explorer is still useful for visualizing costs and RI recommendations, but Option B is more directly aligned with the rightsizing requirement. Option C (Cost Anomaly Detection) helps identify unusual spending but doesn't directly contribute to the 30% cost reduction target - it's more about governance and preventing budget overruns than optimization. Option D (AWS Budgets with budget actions) can automate stopping resources when budgets are exceeded, which helps with cost control but is reactive rather than proactive optimization. The requirement mentions 'automated scheduling for non-production environments,' which would typically be implemented with Instance Scheduler, Lambda functions with EventBridge rules, or Auto Scaling scheduled actions - Budgets with actions is more about emergency cost controls than planned optimization. The architecture uses Compute Optimizer for identifying rightsizing opportunities (targeting 15-20% savings), Organizations with consolidated billing and cost allocation tags for visibility and sharing of discounts, and Savings Plans for commitment-based discounts on the optimized baseline (targeting 10-15% savings), collectively enabling the 30% cost reduction goal while providing business unit cost visibility."
    },
    {
      "question": "A manufacturing company is implementing an IoT solution for 100,000 factory sensors sending telemetry data every 5 seconds. The solution must ingest high-velocity data, perform real-time anomaly detection to identify equipment failures, store time-series data for 7 years for compliance, and visualize metrics on dashboards. The architecture must be cost-effective at scale. Which THREE AWS services should be implemented? (Select THREE)",
      "options": [
        "AWS IoT Core for device connectivity with MQTT protocol support",
        "Amazon Kinesis Data Streams for ingesting high-velocity sensor data",
        "AWS IoT Analytics for device data processing and analytics",
        "Amazon Timestream for efficient time-series data storage with automated tiering",
        "Amazon Kinesis Data Analytics with Apache Flink for real-time anomaly detection",
        "Amazon QuickSight for creating interactive dashboards and visualizations"
      ],
      "type": "multiple",
      "correctAnswer": [0, 3, 4],
      "explanation": "For a large-scale IoT telemetry solution with real-time analytics and long-term storage, the optimal architecture includes: (1) AWS IoT Core provides managed device connectivity and messaging for IoT devices at scale. IoT Core supports MQTT (the standard IoT protocol), MQTT over WebSocket, and HTTPS for device communication. It handles device authentication using X.509 certificates or IAM, manages device registry, and supports device shadows for tracking device state. Critically, IoT Core includes a rules engine that can route device data to various AWS services (Kinesis, Lambda, DynamoDB, Timestream, etc.) without writing custom code. For 100,000 sensors sending data every 5 seconds, IoT Core can easily handle the message throughput (100,000 devices × 12 messages/min = 1.2M messages/min = 20,000 messages/sec), and it scales automatically. IoT Core provides the managed infrastructure for device connectivity that would otherwise require custom MQTT brokers and scaling logic. (2) Amazon Timestream is a purpose-built time-series database optimized for IoT telemetry, application metrics, and operational data. For this use case, Timestream offers several advantages: columnar storage optimized for time-series queries, automatic data lifecycle management with memory store (recent data) and magnetic store (historical data), built-in time-series analytics functions, and data compression that reduces storage costs by 90% compared to relational databases. Critically, Timestream can store data for 7 years cost-effectively through automatic tiering - recent data stays in the fast memory store, and older data automatically moves to the lower-cost magnetic store. Timestream integrates directly with IoT Core rules engine, QuickSight for visualization, and other analytics tools. For 100,000 devices × 12 messages/min × 7 years, Timestream's compression and tiering make it far more cost-effective than storing raw data in S3 or a relational database. (3) Amazon Kinesis Data Analytics with Apache Flink provides real-time stream processing for anomaly detection. Kinesis Data Analytics can consume data from IoT Core (via Kinesis Data Streams or directly), apply real-time SQL queries or Flink applications to detect anomalies, and send alerts when anomalies are detected (via Lambda, SNS, or directly to operational systems). Flink supports sophisticated anomaly detection algorithms including statistical methods, machine learning models, and pattern matching. The managed service handles scaling automatically based on data throughput. For equipment failure prediction, you can implement algorithms that detect when sensor values deviate from normal ranges, when patterns change suddenly, or when correlations between sensors indicate impending failures. Kinesis Data Analytics can write anomaly detection results back to Timestream, S3, or trigger alerts via SNS/Lambda. Option B (Kinesis Data Streams) could be part of the architecture if IoT Core rules route data to Kinesis Data Streams first, but IoT Core can route data directly to Timestream and Kinesis Data Analytics, making a separate Data Streams component optional unless you need the buffer for processing flexibility. The question asks for the three core components, and IoT Core + Timestream + Kinesis Data Analytics provides a more complete answer. Option C (AWS IoT Analytics) provides pre-built data processing pipeline for IoT data including cleansing, filtering, enrichment, and analysis. However, it overlaps significantly with Timestream (for storage) and Kinesis Data Analytics (for real-time processing). IoT Analytics is valuable when you need its specific features like data set versioning and Jupyter notebooks for data science, but for this use case, Timestream (optimized time-series database with 7-year retention) + Kinesis Data Analytics (real-time anomaly detection) provides better cost and performance characteristics. Option F (QuickSight) is the right choice for visualization and dashboards, but when limited to three core services, the ingestion (IoT Core), storage (Timestream), and real-time analytics (Kinesis Data Analytics) are more fundamental. QuickSight would be the fourth component - it integrates directly with Timestream for building time-series dashboards. The architecture uses IoT Core for managed device connectivity at scale, Timestream for cost-effective long-term time-series storage with automatic tiering, and Kinesis Data Analytics for real-time anomaly detection, providing a complete IoT analytics solution."
    },
    {
      "question": "A SaaS company serving enterprise customers needs to implement a tenant isolation strategy for their multi-tenant application. Each tenant's data must be logically isolated, tenant-specific encryption keys must be used, and they need detailed audit logs showing which tenant accessed what data. The solution must scale to 10,000 tenants without significant operational overhead. Which THREE approaches should be implemented? (Select THREE)",
      "options": [
        "Amazon DynamoDB with tenant ID as partition key and fine-grained access control using IAM policies",
        "Separate AWS accounts for each tenant using AWS Control Tower for account provisioning",
        "Amazon S3 with bucket per tenant and S3 Bucket Keys for tenant-specific encryption",
        "AWS KMS with tenant-specific customer managed keys (CMKs) and key policies for tenant isolation",
        "Amazon Cognito with user pools per tenant for authentication and tenant context",
        "AWS CloudTrail with S3 object-level logging for detailed tenant data access auditing"
      ],
      "type": "multiple",
      "correctAnswer": [0, 3, 5],
      "explanation": "For a scalable multi-tenant SaaS architecture with data isolation, tenant-specific encryption, and audit logging for 10,000 tenants, the correct approach is: (1) Amazon DynamoDB with tenant ID as partition key and fine-grained access control provides efficient data isolation at scale. In this pattern, the tenant ID is part of the partition key (e.g., 'TENANT#12345#USERID#67890'), ensuring tenant data is logically separated. DynamoDB's fine-grained access control using IAM policies with conditions enables you to restrict users to only access data for their tenant. For example, IAM policies can use DynamoDB condition keys to ensure users can only query items where the partition key contains their tenant ID. This is called the 'pool model' where all tenants share the same tables but data is logically partitioned. This approach scales to thousands of tenants without operational overhead of managing separate tables/databases per tenant. DynamoDB's scalability and performance characteristics make it ideal for multi-tenant SaaS applications. (2) AWS KMS with tenant-specific customer managed keys (CMKs) provides the tenant-specific encryption required for isolation. In this architecture, each tenant has a dedicated KMS CMK. Application code uses the tenant's specific key for encrypting/decrypting their data using envelope encryption. KMS key policies can restrict which IAM roles can use which keys, and key usage is logged in CloudTrail, providing an audit trail of encryption operations per tenant. While managing 10,000 KMS keys requires careful architecture (you can programmatically create and manage keys via API/CloudFormation), it's feasible and provides strong tenant isolation - even if there's a data breach or misconfiguration, data encrypted with Tenant A's key cannot be decrypted by Tenant B. This is critical for enterprise SaaS where tenants demand cryptographic isolation. Some SaaS providers use a key-per-tenant model stored in a centralized key table, but the question specifically asks for tenant-specific encryption keys, making KMS CMKs per tenant the appropriate choice. (3) AWS CloudTrail with S3 object-level logging provides detailed audit logs for data access. CloudTrail logs all API calls including DynamoDB and KMS operations. For DynamoDB, enabling CloudTrail data events captures read/write operations with details about which user/role accessed which items. For S3 (if used for tenant file storage), object-level logging captures GetObject, PutObject, and DeleteObject calls. For KMS, CloudTrail automatically logs all key usage including Encrypt, Decrypt, and GenerateDataKey operations, showing which tenant's key was used for which operation. These logs can be analyzed to create tenant-specific audit reports showing exactly what data was accessed, when, by whom, and for which tenant. For enterprise SaaS customers (especially in regulated industries), this audit capability is a requirement. CloudTrail logs can be centralized, analyzed with Athena, and retained for compliance periods. Option B (separate accounts per tenant) provides the strongest isolation (known as the 'silo model') but creates massive operational overhead at 10,000 tenants. Managing 10,000 AWS accounts, even with Control Tower automation, involves significant complexity in billing, cross-account access, resource deployment, updates, and monitoring. This approach is typically used for high-tier enterprise customers (tens to hundreds), not for scaling to thousands of tenants. Option C (S3 bucket per tenant) faces similar challenges - while S3 Bucket Keys improve encryption performance and reduce KMS costs, managing 10,000 S3 buckets creates operational overhead (bucket policies, lifecycle policies, versioning configuration, etc.). A more scalable approach is using a shared bucket with prefixes per tenant (TENANT#12345/files/) combined with IAM policies restricting access to specific prefixes and using tenant-specific KMS keys for encryption. Option D (Cognito user pools per tenant) provides tenant-specific authentication but creates operational overhead at 10,000 tenants. A more scalable approach is a single Cognito user pool with custom attributes indicating tenant ID, or using a centralized IdP (like Auth0, Okta) with tenant context. While Cognito per tenant provides isolation, it's not necessary for the stated requirements and complicates user management at scale. The architecture uses DynamoDB with partition key-based data isolation (pool model), tenant-specific KMS CMKs for cryptographic separation, and CloudTrail for comprehensive audit logging, providing tenant isolation that scales to thousands of tenants with manageable operational overhead."
    },
    {
      "question": "A global e-commerce company needs to migrate 5,000 on-premises Windows and Linux servers to AWS over a 12-month period. They require automated discovery of server dependencies, application grouping for wave-based migration planning, and continuous tracking of migration progress. The migration will use a combination of rehosting and replatforming strategies. Which THREE AWS services should be used for discovery, planning, and migration execution? (Select THREE)",
      "options": [
        "AWS Migration Hub for centralized migration tracking and progress monitoring",
        "AWS Application Discovery Service for automated discovery of server inventory and dependencies",
        "AWS Migration Evaluator (formerly TSO Logic) for total cost of ownership analysis",
        "AWS Application Migration Service (MGN) for lift-and-shift rehosting of servers",
        "AWS Server Migration Service (SMS) for incremental replication-based migration",
        "AWS Database Migration Service (DMS) for database replatforming"
      ],
      "type": "multiple",
      "correctAnswer": [0, 1, 3],
      "explanation": "For a large-scale server migration with automated discovery, planning, and execution, the correct combination is: (1) AWS Migration Hub provides centralized tracking and visibility across the entire migration. Migration Hub acts as the command center where you can: discover existing servers using Application Discovery Service, group servers into applications for wave planning, track the status of migrations across multiple tools (MGN, DMS, etc.), view migration progress in a unified dashboard, and integrate with AWS migration partners. For a 5,000-server migration over 12 months, Migration Hub is essential for program management, stakeholder reporting, and ensuring nothing falls through the cracks. It provides the single pane of glass across the migration program. (2) AWS Application Discovery Service automates the discovery process for on-premises infrastructure. Discovery Service has two modes: Agentless Discovery (via VMware vCenter) collects configuration and utilization data, and Agent-based Discovery (via lightweight agents installed on servers) collects detailed data including server specs, performance metrics, network connections, and running processes. Critically, agent-based discovery maps server dependencies by observing network connections, identifying which servers communicate with each other. This dependency mapping is essential for application grouping and wave planning - you can't migrate a web server without also migrating its database and cache servers. Discovery Service exports data to Migration Hub and can generate server dependency maps showing application architectures, enabling intelligent wave planning that respects dependencies. For 5,000 servers, automated discovery is the only scalable approach; manual discovery is error-prone and time-consuming. (3) AWS Application Migration Service (MGN) is AWS's primary tool for rehosting (lift-and-shift) migrations. MGN has replaced SMS as the recommended solution. MGN performs continuous, block-level replication of source servers to AWS, creates low-cost staging instances for testing, enables non-disruptive testing of migrated servers, and performs final cutover with minimal downtime (typically minutes). MGN supports both Windows and Linux, works with physical, virtual, and cloud servers, and is agentless from the AWS perspective (you install a lightweight replication agent on source servers). For rehosting strategy at scale, MGN is the current best practice - it's simpler than SMS, supports more source types, and provides better testing capabilities. MGN integrates with Migration Hub for tracking. Option C (Migration Evaluator) provides business case development and TCO analysis, which is valuable for justifying the migration and rightsizing targets, but isn't part of the core 'discovery, planning, and execution' workflow described in the question. Migration Evaluator would typically be used before the migration program starts to build the business case. Option E (SMS) has been superseded by MGN (Application Migration Service) as AWS's recommended rehosting solution. While SMS still works, AWS is directing new customers to MGN. If this were an older exam question, SMS would be correct, but for current best practices, MGN is the answer. Option F (DMS) is for database migration and replatforming (e.g., Oracle to Aurora, SQL Server to RDS), which may be part of the overall program, but the question specifically asks about discovery, planning, and migration of servers (compute workloads). While DMS might be used for some databases, it's not one of the core three for the server migration program - MGN handles the server rehosting including application databases on those servers, and DMS would be a supplementary tool for database-specific replatforming scenarios. The architecture uses Application Discovery Service for automated server inventory and dependency mapping, Migration Hub for centralized tracking and wave planning across the program, and Application Migration Service (MGN) for executing the rehosting migrations, providing end-to-end capabilities for a large-scale server migration program."
    },
    {
      "question": "A healthcare provider is migrating a legacy on-premises application to AWS that processes Protected Health Information (PHI). The application must comply with HIPAA regulations, including encryption of data at rest and in transit, audit logging of all access to PHI, network isolation, and automatic security patching. They need to minimize operational overhead while maintaining compliance. Which THREE AWS services/features should be implemented as part of the compliance architecture? (Select THREE)",
      "options": [
        "AWS Systems Manager Patch Manager for automated patch management of EC2 instances",
        "Amazon Macie for automated discovery and protection of PHI stored in S3",
        "AWS Config with HIPAA-specific conformance packs for continuous compliance monitoring",
        "AWS Artifact for accessing HIPAA compliance documentation and BAA agreements",
        "AWS Shield Standard for DDoS protection of healthcare applications",
        "Amazon GuardDuty for threat detection and continuous security monitoring"
      ],
      "type": "multiple",
      "correctAnswer": [0, 1, 2],
      "explanation": "For HIPAA-compliant migration with automated security and compliance, the correct services are: (1) AWS Systems Manager Patch Manager automates the patching process for EC2 instances running Windows and Linux. Patch Manager enables you to define patch baselines (which patches to apply), create maintenance windows (when patching occurs), scan instances for missing patches, automatically apply patches, and generate compliance reports showing patch status across all instances. For HIPAA compliance, automatic security patching is critical to prevent vulnerabilities that could lead to PHI breaches. Patch Manager integrates with AWS Config to report compliance status and can be configured to automatically patch instances on a schedule (e.g., weekly), meeting the 'minimize operational overhead' requirement. For a healthcare provider migrating from on-premises, automating patch management removes significant operational burden while improving security posture. (2) Amazon Macie provides automated discovery and protection of sensitive data including PHI. Macie uses machine learning and pattern matching to automatically discover PHI stored in S3 buckets (e.g., patient names, medical record numbers, health insurance information). Macie continuously monitors S3 for sensitive data, evaluates bucket policies and access controls for security risks, detects overly permissive access to sensitive data, generates findings when sensitive data is discovered or security risks are identified, and integrates with Security Hub and EventBridge for alerting. For HIPAA compliance, knowing where PHI is stored and ensuring it's properly protected is fundamental. Macie automates this discovery process, which would otherwise require manual data classification - with potentially thousands of S3 buckets, automated discovery is the only scalable approach. (3) AWS Config with HIPAA-specific conformance packs provides continuous compliance monitoring against HIPAA requirements. AWS Config conformance packs are pre-built collections of Config rules that map to compliance frameworks. The Operational Best Practices for HIPAA Security conformance pack includes rules that check: encryption at rest for EBS, S3, RDS; encryption in transit with HTTPS/TLS; CloudTrail logging enabled; VPC flow logs enabled; MFA enabled for root account; security groups don't allow unrestricted access; and many other HIPAA-related security controls. Config continuously evaluates resources against these rules and reports non-compliant resources, enabling you to remediate issues before they become audit findings. For the stated requirements (encryption, audit logging, network isolation), Config conformance packs provide automated, continuous verification that these controls are in place, reducing compliance risk and audit preparation effort. Option D (AWS Artifact) provides access to AWS compliance reports, HIPAA attestations, and the Business Associate Addendum (BAA) required for HIPAA compliance. While signing a BAA with AWS is a prerequisite for hosting PHI, Artifact is a documentation portal, not an active compliance service that enforces security controls or provides automated compliance capabilities. It's a necessary step during setup but not part of the ongoing compliance architecture. Option E (AWS Shield Standard) provides DDoS protection and is automatically enabled for all AWS customers at no additional cost. While DDoS protection contributes to availability (which is important for healthcare applications), Shield Standard doesn't specifically address the HIPAA requirements listed (encryption, audit logging, patching, network isolation). Shield Advanced (not Standard) would provide enhanced DDoS protection and 24/7 support, but neither Shield Standard nor Advanced are among the core services for this HIPAA compliance architecture focused on data protection and compliance monitoring. Option F (GuardDuty) provides threat detection by analyzing VPC Flow Logs, CloudTrail logs, and DNS logs to identify malicious or unauthorized activity. GuardDuty is valuable for security monitoring and can detect threats like cryptocurrency mining, communication with known malicious IPs, or compromised instances. However, when limited to three core services for the stated requirements, Patch Manager (addresses 'automatic security patching'), Macie (addresses 'PHI discovery and protection'), and Config with HIPAA conformance pack (addresses 'encryption, audit logging, network isolation' through continuous compliance checking) provide more direct alignment with the specific HIPAA requirements listed. GuardDuty would be an excellent fourth service for defense-in-depth threat detection. The architecture uses Systems Manager Patch Manager for automated security patching, Macie for PHI discovery and data protection monitoring, and Config with HIPAA conformance packs for continuous compliance validation, collectively addressing HIPAA requirements with minimal operational overhead."
    }
  ]
}
