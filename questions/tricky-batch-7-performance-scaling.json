{
  "domain": "Mixed Domains: Advanced Scenarios",
  "task": "Tricky Batch 7: Performance & Auto-Scaling Optimization",
  "question_count": 15,
  "questions": [
    {
      "question": "A financial trading application uses RDS PostgreSQL (db.r5.4xlarge) with 16 vCPUs and 128 GB RAM. Performance Insights shows that 'Client:ClientRead' wait events account for 60% of database load, and 'CPU' accounts for 30%. The application executes complex analytical queries that scan large tables. The DBA has already added appropriate indexes. Which optimization would provide the MOST performance improvement?",
      "options": [
        "Enable RDS Proxy to pool database connections and reduce connection overhead, which will reduce Client:ClientRead wait events by reusing connections",
        "Upgrade to a memory-optimized instance with more vCPUs (db.r5.8xlarge) to reduce CPU bottleneck and improve query parallelization",
        "The 'Client:ClientRead' wait event indicates the database is waiting for the application to fetch results. Optimize the application to fetch results faster, increase network bandwidth, or implement result streaming instead of fetching all rows at once",
        "Enable RDS read replicas and configure the application to offload analytical queries to read replicas, reducing load on the primary instance"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct. 'Client:ClientRead' is a PostgreSQL wait event that occurs when the database has query results ready but is waiting for the client application to read/fetch them. This is NOT a database performance issue - it's an application issue. Common causes: 1) Application is processing results slowly (slow business logic between fetches), 2) Network latency between application and database, 3) Application fetches all rows into memory instead of streaming results, 4) Small fetch sizes causing many round trips. Solutions: 1) Use cursor-based pagination for large result sets, 2) Implement streaming/chunked result processing, 3) Increase fetch size in database drivers, 4) Place application servers in the same VPC/AZ as RDS to reduce network latency, 5) Profile application code to identify slow result processing. This is a classic example of a wait event that looks like a database problem but is actually an application or network problem. Option A is incorrect - RDS Proxy helps with connection pooling and is beneficial for applications with many short-lived connections (like Lambda), but it doesn't address Client:ClientRead wait events, which are about result fetching, not connection management. Option B is incorrect - while CPU is 30% of the load, the dominant issue is Client:ClientRead (60%). Upgrading the instance won't fix the application's slow result consumption. You'd be paying for larger instances without addressing the root cause. Option D is incorrect - Read replicas help with read scalability, but they don't solve the Client:ClientRead issue. Even if you offload queries to replicas, the application still needs to fetch results, and the wait event would just move to the replica. The key lesson: Performance Insights wait events must be correctly interpreted. Client:ClientRead, Client:ClientWrite, and network-related wait events often indicate application or network issues, not database issues. Always correlate database metrics with application metrics (latency, throughput) and network metrics (bandwidth, packet loss)."
    },
    {
      "type": "multiple",
      "question": "An e-commerce application experiences traffic spikes during flash sales (10x normal load for 30 minutes). The architecture uses Application Load Balancer, ECS Fargate with Auto Scaling, and ElastiCache Redis (cluster mode enabled) with 3 shards. During the last flash sale, they observed: ALB 5XX errors increased, ECS tasks scaled from 10 to 100 within 5 minutes, ElastiCache CPU reached 90%, and Redis READONLY errors in application logs. Which THREE issues contributed to the application failures? (Select THREE)",
      "options": [
        "ElastiCache Redis cluster mode enabled uses read replicas for each shard, but write operations can only go to the primary node of each shard. At 90% CPU, the primary nodes are bottlenecked, and some clients are incorrectly attempting to write to read replicas, causing READONLY errors",
        "ALB requires pre-warming for traffic spikes over 50% of current capacity. Without pre-warming, ALB cannot scale fast enough to handle 10x traffic, causing connection timeouts and 5XX errors",
        "ECS Fargate task startup time (typically 1-2 minutes for pulling images and starting containers) creates a gap where new tasks are counted in the desired count but not yet serving traffic, causing existing tasks to be overloaded",
        "ElastiCache Redis cluster mode with 3 shards means write throughput is limited by the 3 primary nodes. With 90% CPU, adding more shards or upgrading node types is required to handle write-heavy workloads",
        "ALB target health checks have default thresholds (2 consecutive successful checks to mark healthy). New ECS tasks may receive traffic before they're fully initialized, causing 5XX errors during the scaling event",
        "ElastiCache Redis does not support automatic scaling. The cluster was provisioned for normal load, and during the 10x spike, cache eviction rate increased, causing cache misses and increased database load"
      ],
      "correctAnswer": [0, 2, 4],
      "explanation": "Options 0, 2, and 4 are correct. Option 0: ElastiCache Redis cluster mode uses sharding for horizontal scaling. Each shard has a primary node (read/write) and optional read replicas (read-only). If the application misconfigures the Redis client or uses a non-cluster-aware client, write operations might be directed to read replicas, causing 'READONLY You can't write against a read only replica' errors. At 90% CPU on primary nodes, write capacity is exhausted. Solution: 1) Use cluster-aware Redis clients, 2) Scale horizontally by adding more shards (resharding), 3) Scale vertically by upgrading node types, 4) Optimize write operations (batching, pipelining). Option 2: ECS Fargate tasks take time to start - pulling container images from ECR (if not cached), container initialization, application startup. During this time, Auto Scaling counts these tasks in the desired count, but they're not serving traffic. Existing tasks bear the full load, potentially causing overload and failures. Solutions: 1) Use smaller container images and layer caching, 2) Implement proper health checks so tasks only receive traffic when ready, 3) Use Target Tracking scaling with lower target values to scale proactively, 4) Consider provisioning minimum capacity for flash sales, 5) Implement connection draining and gradual traffic shifting. Option 4: ALB target health checks determine when targets are healthy and should receive traffic. Default: unhealthy threshold = 2, healthy threshold = 2, interval = 30 seconds, timeout = 5 seconds. This means a new task must pass 2 consecutive health checks (minimum 60 seconds with default settings) before receiving traffic. However, if Auto Scaling is aggressive and health check thresholds are not tuned, ALB might route traffic to tasks that are registered but not yet healthy, causing 5XX errors (specifically 502/503). Solutions: 1) Tune health check paths to verify application readiness (not just TCP port open), 2) Adjust healthy threshold and interval based on application startup time, 3) Implement application-level readiness checks. Option 1 is incorrect - ALB automatically scales to handle traffic, and AWS does not require manual pre-warming for most use cases (though you can request it for extreme spikes). ALB can handle normal traffic spikes without pre-warming. Option 3 is partially correct - write throughput is limited by primary nodes, and 90% CPU is a bottleneck. However, this alone doesn't cause READONLY errors. The READONLY errors come from clients writing to read replicas (Option 0). Adding shards or upgrading nodes would help with CPU, but it's not one of the three MOST contributing issues given the specific errors described. Option 5 is incorrect - ElastiCache Redis does support some scaling (adding replicas for read scaling, adding shards for write scaling, changing node types), though not fully automatic like DynamoDB. Also, cache eviction increasing is a symptom, not a root cause of the described 5XX errors and READONLY errors. Key principle: For handling traffic spikes: 1) Architect for scale (appropriate scaling policies, node capacity), 2) Minimize startup time (smaller images, caching), 3) Tune health checks for fast but safe traffic ramping, 4) Use proper Redis clients for cluster mode, 5) Monitor leading indicators (queue depth, CPU trends) and scale proactively."
    },
    {
      "question": "A media streaming company uses CloudFront with an S3 origin to deliver video content. They notice that cache hit ratio is only 40% despite most users watching the same popular videos. Investigation shows that viewer requests include query strings with tracking parameters (e.g., ?user_id=123&session=abc). Videos are stored in S3 as video123.mp4 without query strings. What is the MOST effective solution to improve cache hit ratio while maintaining tracking capabilities?",
      "options": [
        "Configure CloudFront to ignore all query strings by setting 'Query String Forwarding' to 'None', which will cache objects regardless of query string variations",
        "Configure CloudFront to forward only specific query strings needed by the origin using 'Query String Forwarding' set to 'Whitelist', and exclude tracking parameters like user_id and session from the whitelist",
        "Enable CloudFront cache key normalization by creating a cache policy that excludes query strings from the cache key, while still forwarding them to the origin for logging purposes",
        "Implement Lambda@Edge Origin Request function to strip query strings before forwarding to S3, and use Lambda@Edge Viewer Request to log tracking parameters separately to CloudWatch"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct and represents the modern best practice using CloudFront cache policies (introduced in 2020). Cache policies allow you to control what is included in the cache key (which determines cache hits) separately from what is forwarded to the origin. For this scenario: 1) Create a cache policy that excludes query strings from the cache key (or includes only specific ones if needed), 2) Create an origin request policy that forwards all query strings to the origin (for logging in CloudFront access logs), 3) Attach both policies to the CloudFront behavior. Result: All requests to video123.mp4?user_id=123, video123.mp4?user_id=456, video123.mp4?session=abc are treated as the same object for caching (high cache hit ratio), but the full query strings are still forwarded to S3 and logged in CloudFront access logs for tracking analysis. Cache policies also allow you to control: headers in cache key, cookies in cache key, compression (Gzip/Brotli), TTL settings. This provides fine-grained control over caching behavior. Option A is partially correct but less flexible - setting 'Query String Forwarding' to 'None' improves cache hit ratio, but the query strings are completely discarded and won't appear in CloudFront access logs, breaking tracking capabilities. This is the legacy approach. Option B is incorrect for this scenario - whitelisting specific query strings is useful when the origin (S3 or custom origin) needs certain parameters to serve different content versions (e.g., ?quality=hd vs ?quality=sd). In this case, the origin (S3) doesn't use query strings at all, so whitelisting doesn't help. Also, non-whitelisted query strings are discarded, losing tracking data. Option D is technically feasible but operationally complex and costly - Lambda@Edge functions run on every request, adding latency and cost. For simple query string handling, cache policies are more efficient. Use Lambda@Edge for complex transformations, authentication, or A/B testing. Key principle: CloudFront cache optimization requires separating 'what affects caching' from 'what is forwarded to origin' from 'what is needed by the application'. Cache policies and origin request policies provide this separation. Common patterns: 1) Ignore query strings for cache key but log them (this scenario), 2) Include specific query strings for cache key (e.g., ?version=1), 3) Include headers for cache key (e.g., Accept-Encoding for compression, CloudFront-Is-Mobile-Viewer for device-based content), 4) Use Lambda@Edge for complex logic that can't be handled by policies. Monitor CloudFront metrics: CacheHitRate, OriginLatency, 4xxErrorRate, BytesDownloaded to optimize performance and cost."
    },
    {
      "question": "A real-time bidding platform uses Auto Scaling Groups (ASG) with EC2 instances (c5.2xlarge) behind an Application Load Balancer. They've configured target tracking scaling based on ALBRequestCountPerTarget with a target value of 1000 requests per target. During peak load, the application becomes slow and unresponsive, but Auto Scaling is not triggering. CloudWatch shows ALBRequestCountPerTarget at 800 (below the 1000 target), but CPU utilization is at 95%. What is the root cause of this scaling issue?",
      "options": [
        "Target tracking scaling policies have a cooldown period (default 300 seconds) that prevents scaling during rapid load changes. The load increased too quickly for Auto Scaling to respond",
        "The application is CPU-bound, not request-bound. The target value of 1000 requests per instance is not being reached because high CPU is slowing request processing, preventing the request count from reaching the threshold",
        "ALBRequestCountPerTarget metric is calculated as a 1-minute average, which lags behind real-time load. By the time the metric reflects high load, the application is already overwhelmed",
        "Auto Scaling target tracking policies only scale out when the metric exceeds the target. At 800 requests per target, the scaling threshold of 1000 has not been breached, so no scaling occurs regardless of CPU utilization"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct and illustrates a critical understanding of choosing the right scaling metric. The application is CPU-bound (95% CPU), meaning each request takes longer to process because CPU is the bottleneck. As CPU reaches saturation, request throughput actually decreases (fewer requests completed per second), so ALBRequestCountPerTarget may decrease or plateau rather than increase. The scaling policy is based on requests per target, but the application's bottleneck is CPU, not request volume. This creates a situation where: 1) Application is slow (high CPU, long response times), 2) Request count doesn't reach the threshold because the instance can't process requests fast enough, 3) Auto Scaling doesn't trigger because the metric (800) is below the target (1000), 4) Application remains in a degraded state. Solution: Use a composite scaling strategy: 1) Primary: Target tracking on CPU utilization (e.g., 70%) - scales when the actual resource bottleneck is reached, 2) Secondary: Target tracking on ALBTargetResponseTime (e.g., 1 second) - scales when user experience degrades, 3) Tertiary: Optional simple scaling or step scaling for extreme scenarios. For most applications, CPU utilization or custom application metrics (queue depth, active connections) are better scaling triggers than request count. Option A is incorrect - Target tracking policies do NOT have cooldown periods (simple scaling and step scaling do). Target tracking continuously evaluates metrics and scales as needed. However, there is a minimum time between scaling activities (typically 60 seconds for scale-out) to allow metrics to stabilize. Option C is incorrect - while CloudWatch metrics have some delay (1-minute intervals for standard metrics), this doesn't explain why scaling isn't happening when the metric is consistently at 800. The metric is being evaluated correctly; the problem is that the wrong metric is being used for the application's actual bottleneck. Option D is partially correct in describing how target tracking works but misses the deeper issue. Yes, 800 < 1000 so scaling doesn't occur, but the question is WHY is the metric at 800 when the application is overwhelmed? The answer is Option B - CPU bottleneck prevents request count from increasing. Key principle: Choose scaling metrics that reflect your application's actual bottlenecks and user experience: 1) CPU-bound applications: CPU utilization, 2) I/O-bound applications: Disk or network throughput, 3) Queue-based applications: Queue depth (SQS ApproximateNumberOfMessagesVisible), 4) Latency-sensitive applications: ALBTargetResponseTime, 5) Throughput-based applications: ALBRequestCountPerTarget or custom metrics. Combine multiple scaling policies (AWS evaluates all policies and chooses the one that provides the most capacity). Monitor both scaling metrics and application performance metrics to validate scaling behavior."
    },
    {
      "type": "multiple",
      "question": "A gaming company runs a multiplayer game with real-time leaderboards using ElastiCache Redis (cluster mode disabled) with 1 primary node and 2 read replicas. During peak gaming hours (50,000 concurrent players), they experience: Redis primary CPU at 85%, increased write latency (avg 10ms increased to 50ms), and occasional connection timeouts. The application performs: 10,000 writes/sec (player score updates) and 50,000 reads/sec (leaderboard queries). Which THREE optimizations would improve performance and scalability? (Select THREE)",
      "options": [
        "Migrate to ElastiCache Redis cluster mode enabled with 5 shards to distribute write load across multiple primary nodes, increasing write throughput and reducing CPU on individual nodes",
        "Implement Redis pipelining in the application to batch multiple commands into single network round trips, reducing network overhead and improving throughput",
        "Add more read replicas (up to 5 total) to handle the 50,000 reads/sec load, and configure the application to distribute read queries across all replicas using DNS-based load balancing",
        "Upgrade the node type from cache.r5.large to cache.r5.4xlarge to provide more CPU and network capacity for handling both read and write operations",
        "Implement client-side caching in the application for frequently accessed leaderboard data with a short TTL (5-10 seconds), reducing read load on ElastiCache",
        "Enable Redis persistence (RDB snapshots or AOF) to prevent data loss during failovers, which may be causing connection timeouts when the primary node is under high load"
      ],
      "correctAnswer": [0, 1, 4],
      "explanation": "Options 0, 1, and 4 are correct. Option 0: The primary bottleneck is write throughput - 10,000 writes/sec on a single primary node causing 85% CPU and increased latency. Cluster mode enabled (called 'sharding' in Redis) distributes data across multiple shards, each with its own primary node. With 5 shards, write load is distributed (~2,000 writes/sec per shard), dramatically reducing CPU and latency. Each shard can also have its own read replicas. Trade-offs: 1) Cluster mode requires cluster-aware clients, 2) Multi-key operations limited to same hash slot, 3) Resharding causes brief disruption. This is the most impactful optimization for write-heavy workloads. Option 1: Redis pipelining allows sending multiple commands without waiting for each response, reducing network round trips. Instead of: send command 1 → wait → response 1 → send command 2 → wait → response 2, pipelining does: send commands 1,2,3...N → wait → responses 1,2,3...N. This is especially beneficial for: 1) High-latency networks, 2) Many small operations, 3) Batch updates (e.g., updating 100 player scores). Can improve throughput by 5-10x for certain workloads with minimal code changes. Also consider Redis transactions (MULTI/EXEC) for atomic operations. Option 4: Client-side caching for leaderboards (which don't need to be real-time accurate to the millisecond) reduces load on ElastiCache. For a leaderboard that updates every 10 seconds, each application instance can cache the data locally. With 100 application servers, this reduces 50,000 reads/sec to potentially 100 reads/10 seconds = 10 reads/sec to ElastiCache. This is a massive reduction. Implement cache invalidation based on TTL or pub/sub notifications from Redis when leaderboards update. Option 2 is helpful for read scaling but doesn't address the primary bottleneck (writes at 85% CPU). Read replicas are already in place (2 replicas), and adding more helps with read distribution, but the write load on the primary remains the same. Without addressing writes, the system will still be bottlenecked. Option 3 (vertical scaling) helps but is less effective than horizontal scaling (Option 0). A larger instance provides more CPU, but you still have a single point of bottleneck for writes. Also, you're limited by the largest instance size. Horizontal scaling with cluster mode provides better scalability. Vertical scaling is easier to implement (no application changes) but hits limits. Option 5 is incorrect - Redis persistence (RDB or AOF) actually increases latency and CPU usage, especially for write-heavy workloads. AOF (Append-Only File) writes every operation to disk, adding I/O overhead. RDB snapshots cause periodic fork() operations that can briefly freeze the instance. For performance-critical applications, persistence is often disabled, and data durability is achieved through replication and application-level backup strategies. Connection timeouts are more likely caused by high CPU and slow command processing, not lack of persistence. Key principle: For Redis performance optimization: 1) Write-heavy: Use cluster mode for horizontal scaling, 2) Read-heavy: Use read replicas, 3) Network overhead: Use pipelining/transactions, 4) Application-level: Implement client-side caching, 5) Data structure optimization: Use appropriate Redis data types (sorted sets for leaderboards, HyperLogLog for unique counts), 6) Monitor: Use CloudWatch metrics (CPU, connections, evictions, replication lag) and Redis INFO command. For gaming leaderboards specifically, consider: 1) Redis sorted sets (ZADD, ZRANGE, ZREVRANGE), 2) Periodic snapshots to S3 for backup, 3) Lazy loading pattern (cache aside), 4) Read replicas with eventually consistent reads for non-critical queries."
    },
    {
      "question": "A SaaS application uses an Application Load Balancer with Lambda targets (ALB-to-Lambda integration). During load testing, they observe that some requests fail with 502 Bad Gateway errors when Lambda concurrency reaches 500 concurrent executions. Lambda CloudWatch metrics show no errors, and all invocations complete successfully in under 2 seconds. The Lambda function has a reserved concurrency limit of 1000. What is the MOST likely cause of the 502 errors?",
      "options": [
        "ALB-to-Lambda integration has a maximum payload size of 1 MB for both requests and responses. The Lambda function is returning responses larger than 1 MB, causing ALB to return 502 errors",
        "The Lambda function response is not properly formatted for ALB integration. ALB expects a specific JSON response format with statusCode, headers, and body fields. Malformed responses cause 502 errors",
        "ALB health checks are failing because Lambda functions are stateless and don't support health check endpoints. This causes ALB to mark Lambda targets as unhealthy and return 502 errors",
        "Lambda is being throttled due to account-level concurrent execution limits (default 1000 for the account). Even with reserved concurrency of 1000 for this function, if other functions in the account are using concurrency, this function may be throttled"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct. When using ALB with Lambda targets, the Lambda function must return a response in a specific format that ALB expects. The response must be a JSON object with: 'statusCode' (integer, required), 'statusDescription' (string, optional), 'headers' (object, optional), 'body' (string, required), 'isBase64Encoded' (boolean, optional). Example: {statusCode: 200, headers: {'Content-Type': 'application/json'}, body: JSON.stringify({message: 'success'})}. If the Lambda function returns: 1) A plain string instead of the JSON structure, 2) Missing required fields (statusCode or body), 3) Incorrect data types (e.g., statusCode as string '200' instead of integer 200), 4) Unhandled exceptions that cause Lambda to return error responses not in ALB format, then ALB cannot process the response and returns 502 Bad Gateway to the client. Important: The Lambda CloudWatch metrics show successful invocations (200 status from Lambda's perspective), but the response format is invalid for ALB, causing integration failure. Solution: 1) Ensure Lambda returns proper ALB-formatted response, 2) Implement error handling to catch exceptions and return ALB-formatted error responses, 3) Test with ALB test events in Lambda console, 4) Monitor ALB target group metrics (TargetResponseTime, HTTPCode_Target_5XX_Count), 5) Enable ALB access logs to see the actual responses. Option A is incorrect - while ALB-to-Lambda does have a 1 MB payload limit (for both request and response), exceeding this limit would cause consistent failures for those specific large responses, not intermittent failures at 500 concurrent executions. Also, the scenario states invocations complete successfully, implying responses are returned. Option C is incorrect - ALB doesn't perform traditional health checks on Lambda targets. When you configure Lambda as an ALB target, health checks are not applicable because Lambda is a managed service and AWS ensures Lambda availability. The target group configuration for Lambda doesn't require or support health checks. Option D is incorrect - the function has reserved concurrency of 1000, which guarantees up to 1000 concurrent executions for this specific function regardless of other functions in the account. Reserved concurrency is a hard guarantee. Also, if Lambda was throttled, CloudWatch metrics would show throttling errors (Throttles metric), which the scenario states is not happening. Key principle: ALB-to-Lambda integration requires: 1) Proper response format from Lambda, 2) Payload size limits (1 MB request, 1 MB response), 3) Timeout alignment (ALB idle timeout default 60 seconds, Lambda max timeout 15 minutes - ensure ALB timeout >= Lambda timeout + buffer), 4) IAM permissions (ALB needs lambda:InvokeFunction on the target Lambda), 5) Multi-value headers handling if needed. For debugging 502 errors with ALB-to-Lambda: 1) Check CloudWatch Logs for Lambda errors, 2) Verify response format, 3) Check ALB access logs for details, 4) Test Lambda function with ALB sample events, 5) Monitor Lambda duration and ensure it completes within ALB timeout."
    },
    {
      "question": "A data analytics platform uses Amazon Redshift with 10 dc2.8xlarge nodes in a cluster. Queries have become slow, and users report wait times of 5-10 minutes for reports. The admin runs 'SELECT * FROM stv_wlm_query_state' and observes many queries in 'Waiting for Lock' state. Further investigation shows that COPY commands loading data from S3 are running during business hours and holding locks. What is the MOST effective solution to improve query performance without impacting data ingestion?",
      "options": [
        "Enable Concurrency Scaling on the Redshift cluster, which automatically adds transient cluster capacity to handle spikes in concurrent read queries while COPY operations run",
        "Configure Workload Management (WLM) with separate queues: a high-priority queue for user queries with more memory and concurrency, and a low-priority queue for COPY operations with lower memory and concurrency",
        "Schedule COPY operations during off-peak hours using AWS Lambda with EventBridge (CloudWatch Events) to trigger data loads only during maintenance windows when user query load is low",
        "Implement streaming ingestion using Kinesis Data Firehose instead of batch COPY commands, which provides micro-batch loading and reduces locking contention"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct. Workload Management (WLM) in Redshift allows you to define query queues with different priorities, concurrency levels, and memory allocations. For this scenario: 1) Create a 'user_queries' queue with higher memory allocation (e.g., 50% of cluster memory), higher concurrency (e.g., 10 concurrent queries), and higher priority, 2) Create a 'data_loading' queue with lower memory (e.g., 20% of cluster memory), lower concurrency (e.g., 2 concurrent COPY operations), and lower priority, 3) Use query groups or user groups to route queries to appropriate queues, 4) Configure query timeouts to prevent long-running queries from monopolizing resources. WLM ensures user queries get priority and resources while COPY operations still proceed but with limited impact. Additionally: 1) Use 'commit' less frequently in COPY operations to reduce lock frequency, 2) Enable 'COMPUPDATE OFF' and 'STATUPDATE OFF' in COPY commands for performance (update statistics manually during off-peak), 3) Use 'COPY from multiple files' to parallelize data loading across nodes. Option A is incorrect for this specific scenario - Concurrency Scaling adds temporary cluster capacity for read queries (SELECT), but the problem is lock contention from COPY operations (write operations). Concurrency Scaling doesn't help with write contention or locks. It's useful for read-heavy spikes, not write/read contention scenarios. Option C is partially valid - scheduling COPY during off-peak hours would eliminate the contention, but the requirement states 'without impacting data ingestion', implying real-time or near-real-time data loading is needed. Delaying data loads to off-peak hours may not meet business requirements for data freshness. However, if business requirements allow, this is the simplest solution. Option D is incorrect - Kinesis Data Firehose to Redshift uses COPY commands under the hood (Firehose buffers data in S3 and then runs COPY), so it doesn't fundamentally solve the locking issue. Firehose provides micro-batching (buffers for 60 seconds or 1 MB by default), which might reduce lock duration, but it's not as effective as WLM. Also, migrating from batch COPY to Firehose is a significant architectural change. Key principle: Redshift performance optimization for mixed workloads: 1) WLM for query prioritization and resource allocation, 2) Sort keys and distribution keys for query optimization, 3) VACUUM and ANALYZE for table maintenance, 4) Concurrency Scaling for read query spikes, 5) Materialized views for frequently accessed aggregations, 6) Redshift Spectrum for querying S3 data without loading, 7) Short Query Acceleration (SQA) to automatically prioritize short queries. For lock contention specifically: 1) Check SVL_STATEMENTTEXT and STL_QUERY to identify locking queries, 2) Use 'LOCK table_name' explicitly in transactions to control lock behavior, 3) Minimize transaction duration (commit frequently for loads, but balance with lock overhead), 4) Use WLM to isolate workloads."
    },
    {
      "type": "multiple",
      "question": "A video processing platform uses Auto Scaling Groups with Spot Instances (c5.4xlarge) to process uploaded videos. Each video takes 10-30 minutes to process. They're experiencing frequent job failures due to Spot Instance interruptions (2-minute warning before termination). The application architecture includes: S3 for video storage, SQS queue for job tracking, and EC2 instances polling SQS and processing videos. Which THREE improvements would minimize job failures from Spot interruptions? (Select THREE)",
      "options": [
        "Use Spot Instance diversification by configuring the Auto Scaling Group with multiple instance types (c5.4xlarge, c5.9xlarge, c5a.4xlarge, c5n.4xlarge) and multiple Availability Zones to reduce interruption frequency",
        "Implement Spot Instance interruption handling by monitoring EC2 instance metadata (http://169.254.169.254/latest/meta-data/spot/instance-action) every 5 seconds, and when interruption is detected, send job back to SQS queue for reprocessing",
        "Configure SQS queue visibility timeout to 60 minutes (longer than job duration) to prevent jobs from being reprocessed if an instance is interrupted before completing processing",
        "Use AWS Batch with Spot Instances instead of direct EC2 Auto Scaling, as Batch automatically handles Spot interruptions by checkpointing job state and resuming on new instances",
        "Implement application-level checkpointing where the processing job saves progress to S3 every 2 minutes, and on restart, checks for existing checkpoint to resume from last saved state",
        "Replace Spot Instances with a mix of On-Demand Instances (20% of fleet) and Spot Instances (80%) using Auto Scaling Groups with mixed instances policy to ensure baseline capacity during Spot interruptions"
      ],
      "correctAnswer": [0, 1, 4],
      "explanation": "Options 0, 1, and 4 are correct. Option 0: Spot Instance diversification is a critical best practice. By using multiple instance types and multiple AZs, you reduce the likelihood of interruptions because: 1) Different instance types have different Spot pricing and availability, 2) Interruptions are more likely when Spot prices spike for a specific instance type in a specific AZ, 3) AWS Auto Scaling with mixed instances policy automatically launches instances from the pools (instance type + AZ combinations) with the lowest interruption rate. Configure attribute-based instance type selection or manually specify types. Also consider Spot Instance allocation strategy: 'price-capacity-optimized' (default, recommended) balances price and interruption risk. Option 1: Handling Spot interruptions gracefully is essential. EC2 provides a 2-minute warning via instance metadata before termination. Best practice: 1) Poll the instance metadata endpoint every 5 seconds for spot/instance-action, 2) When interruption notice is received, immediately stop processing new jobs, 3) Send the current job back to SQS queue (by deleting the message or letting visibility timeout expire), 4) Optionally save checkpoint to S3, 5) Gracefully shut down. This ensures jobs are not lost. Also consider subscribing to EC2 Spot Instance interruption notices via EventBridge for centralized handling. Option 4: Application-level checkpointing is the most robust solution for long-running jobs. Every 2 minutes (or based on processing milestones), save: 1) Job ID, 2) Current processing state (e.g., 'processed 50% of video'), 3) Any intermediate results. Store in S3 or DynamoDB. On job start, check for existing checkpoint and resume. This minimizes rework - instead of reprocessing 30 minutes of work, you only redo the last 2 minutes. Trade-off: added complexity in application code, but significant resilience improvement. Option 2 is incorrect - while monitoring instance metadata is correct, the visibility timeout strategy is wrong. The visibility timeout should be set to slightly longer than the expected job duration (e.g., 35 minutes for 30-minute jobs) so that if an instance crashes or is interrupted without returning the job, the job becomes visible again for reprocessing. Setting it to 60 minutes would delay retries unnecessarily. Option 3 is incorrect - AWS Batch does support Spot Instances and provides some retry logic, but it does NOT automatically checkpoint job state. Batch will retry failed jobs (respecting retry strategy), but each retry starts from the beginning. Application-level checkpointing must still be implemented. Batch is beneficial for simplified job scheduling, resource management, and retry logic, but doesn't eliminate the need for interruption handling. Option 5 is a valid strategy for ensuring availability but doesn't minimize job failures - it just ensures some capacity is always available. Jobs running on Spot Instances still fail during interruptions unless you implement Options 0, 1, and 4. On-Demand + Spot mix is good for critical workloads where you need guaranteed capacity, but for cost-optimized batch processing, maximizing Spot usage with proper interruption handling is more cost-effective. Key principle: Spot Instance best practices: 1) Diversify across instance types and AZs, 2) Handle interruption notices gracefully, 3) Implement checkpointing for long-running jobs, 4) Use appropriate allocation strategies (price-capacity-optimized), 5) Monitor Spot Instance interruption rates and adjust strategy, 6) For critical workloads, mix On-Demand for baseline capacity. For video processing specifically: 1) Use SQS for job queuing (decouples upload from processing), 2) Implement idempotent processing (safe to retry), 3) Save progress frequently, 4) Consider AWS Elemental MediaConvert as a managed alternative for video transcoding."
    },
    {
      "question": "A financial services company uses Aurora PostgreSQL for their trading application. During market open (9:30 AM ET), query latency increases from 10ms to 500ms despite CPU utilization remaining under 40%. Performance Insights shows the top wait event is 'IO:DataFileRead' accounting for 70% of database load. The database has appropriate indexes, and queries are optimized. The current instance is db.r5.4xlarge with 128 GB RAM. What is the MOST effective solution to reduce the IO:DataFileRead wait events?",
      "options": [
        "Upgrade to a larger instance class (db.r5.8xlarge) with more RAM (256 GB) to increase the buffer cache size, allowing more data to be cached in memory and reducing disk I/O",
        "Enable Aurora's Query Plan Management (QPM) to pin optimal query plans and prevent the query optimizer from choosing plans with excessive I/O during high load",
        "The IO:DataFileRead wait event indicates the working set size exceeds available RAM for buffer cache. Implement read replicas and distribute read queries to offload the primary instance",
        "Enable Performance Insights Enhanced Monitoring to identify specific tables with high I/O, then implement table partitioning to reduce data scan volume per query"
      ],
      "correctAnswer": 0,
      "explanation": "Option A is correct. 'IO:DataFileRead' wait events in Aurora PostgreSQL (and RDS PostgreSQL) indicate that the database is waiting for data to be read from storage into memory (buffer cache). This happens when the working set (actively accessed data) is larger than the available buffer cache in RAM. During market open, the trading application likely queries a large volume of recent trades, orders, and market data that doesn't fit in the 128 GB instance's buffer cache. Solution: Upgrade to a larger instance with more RAM (e.g., db.r5.8xlarge with 256 GB or db.r5.12xlarge with 384 GB). More RAM means: 1) Larger buffer cache (PostgreSQL shared_buffers parameter, which Aurora manages automatically), 2) More data cached in memory, 3) Fewer disk reads required, 4) Lower IO:DataFileRead wait events. Aurora storage is fast (SSD-based, distributed), but memory access is still orders of magnitude faster. Important: Monitor 'BufferCacheHitRatio' metric in CloudWatch - values below 90% indicate insufficient buffer cache. For Aurora, you can also check Performance Insights 'Database Load' and correlate with RAM usage. Option B is incorrect - Query Plan Management (QPM) is useful for stabilizing query plans and preventing plan regression when statistics change, but it doesn't address the root cause of I/O wait events. Even with optimal query plans, if the data isn't in memory, I/O waits will occur. QPM is more relevant for CPU-bound queries with plan variability. Option C is incorrect - Read replicas help with read scalability (distributing read queries across multiple instances), but they don't solve the buffer cache issue on the primary instance. Each replica has its own buffer cache, so if you route read queries to replicas, those replicas need sufficient RAM too. Also, the primary instance still handles all writes and synchronous replication to replicas. Read replicas are beneficial for read-heavy workloads that can tolerate eventual consistency (replica lag is typically < 100ms for Aurora). Option D is incorrect - while Enhanced Monitoring and table partitioning are good practices, they don't directly address the immediate IO:DataFileRead issue caused by insufficient buffer cache. Partitioning helps with query performance by reducing data scan volume (partition pruning), but if the accessed partitions still don't fit in memory, I/O waits persist. Partitioning is more effective for time-series data or large fact tables in OLAP workloads. Key principle: PostgreSQL/Aurora performance tuning for I/O wait events: 1) IO:DataFileRead - increase RAM for buffer cache (vertical scaling), 2) IO:DataFileWrite - optimize checkpoint settings, increase IOPS (though Aurora abstracts this), 3) Monitor buffer cache hit ratio - aim for > 95% for OLTP workloads, 4) Use Performance Insights to correlate wait events with SQL statements and identify hot tables, 5) Consider Aurora read replicas for read scalability, 6) Optimize queries to reduce data access (indexes, query rewrites, materialized views). For trading applications specifically: 1) High RAM for caching active market data, 2) Read replicas for historical queries and reporting, 3) Time-series partitioning for trade/order tables, 4) Consider Aurora Global Database for multi-region disaster recovery, 5) Monitor replication lag and failover time for high availability."
    },
    {
      "question": "A mobile gaming application uses API Gateway with Lambda backend and DynamoDB for game state storage. During a promotional event, API Gateway throttled requests with '429 Too Many Requests' errors despite Lambda concurrency being well below limits. The API receives 10,000 requests per second during the event. The default API Gateway throttling limit is 10,000 requests per second per account per region. What is the MOST likely cause of the throttling?",
      "options": [
        "API Gateway's default throttling limit of 10,000 RPS is a burst limit, not a sustained limit. For sustained high throughput, the steady-state limit is 5,000 RPS unless increased via service quota request",
        "API Gateway throttling limits are applied per API stage (dev, staging, prod). If multiple stages are active and receiving traffic, the 10,000 RPS limit is shared across all stages",
        "The 10,000 RPS limit is account-level across all APIs. If the account has multiple APIs receiving traffic, they compete for the shared quota, and this API is being throttled to stay within the account limit",
        "API Gateway applies per-client throttling based on IP address. If a large number of requests come from a small number of client IPs (e.g., NAT gateway), per-client rate limits cause throttling even if account-level limits are not reached"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct. API Gateway throttling limits are account-level and region-level, meaning all APIs in the same account and region share the quota. Default limits: 1) 10,000 requests per second (steady-state), 2) 5,000 burst capacity (token bucket algorithm). If the account has multiple APIs (e.g., production API, staging API, partner API), they all draw from the same 10,000 RPS quota. During the promotional event, if other APIs in the account are also receiving traffic, the combined traffic may exceed 10,000 RPS, causing throttling. Solution: 1) Request a service quota increase for API Gateway (can be increased to hundreds of thousands of RPS), 2) Implement usage plans and API keys for different client tiers to allocate quota, 3) Use multiple AWS accounts for environment isolation (prod in one account, staging in another), 4) Implement client-side retry with exponential backoff and jitter, 5) Monitor CloudWatch metrics: Count, 4XXError, 5XXError, Latency, CacheHitCount to understand traffic patterns. Request quota increases through AWS Service Quotas console or support case. Increases are typically approved quickly for legitimate use cases. Option A is incorrect - API Gateway's 10,000 RPS is the steady-state limit, not the burst limit. The burst limit is 5,000 requests (token bucket burst capacity). Burst capacity allows handling short spikes above the steady-state rate. The throttling algorithm is: tokens accumulate at the rate limit (10,000/sec), up to burst capacity (5,000 tokens). Each request consumes a token. If tokens are exhausted, requests are throttled. Option B is incorrect - Throttling limits are NOT shared across API stages. Each stage can handle up to the account limit independently (though all stages in the account compete for the account-level quota). The issue isn't stage-specific; it's account-level. Option D is incorrect - API Gateway does not apply automatic per-client IP throttling by default. You can configure per-client throttling using usage plans and API keys, but this is explicit configuration, not automatic. The default throttling is account-level and method-level (you can set method-level throttling in stage settings). Key principle: API Gateway throttling hierarchy: 1) Account-level (10,000 RPS default, shared across all APIs in account/region), 2) Usage plan throttle limits (if configured, applies to specific API keys/clients), 3) Method throttle limits (if configured, overrides account-level for specific methods), 4) Burst capacity (5,000 requests default). For high-traffic APIs: 1) Request quota increases proactively before events, 2) Use CloudFront with caching in front of API Gateway to reduce backend requests, 3) Implement API response caching in API Gateway (reduces Lambda invocations and DynamoDB calls), 4) Monitor and alert on 4XXError metrics, 5) Design APIs for idempotency to support retries. For gaming applications specifically: 1) Cache static game data (levels, configs) in CloudFront, 2) Use API Gateway caching for leaderboards, 3) Use DynamoDB with on-demand billing or provisioned capacity with auto-scaling, 4) Consider AppSync (GraphQL) for real-time game state updates with subscriptions."
    },
    {
      "type": "multiple",
      "question": "A data analytics company uses Amazon Athena to query 10 TB of log data stored in S3 (partitioned by date). Users complain that queries are slow (5-10 minutes) and expensive ($5-10 per query). Example query: 'SELECT user_id, COUNT(*) FROM logs WHERE event_date BETWEEN '2024-01-01' AND '2024-01-31' GROUP BY user_id'. The data is stored as uncompressed JSON files (100-500 MB each). Which THREE optimizations would reduce query time and cost? (Select THREE)",
      "options": [
        "Convert data from JSON to Apache Parquet format with Snappy compression, which provides columnar storage and reduces data scanned by 90%+ for analytical queries",
        "Enable Athena query result caching, which automatically reuses results from previous identical queries for 24 hours, eliminating re-scanning data",
        "Use AWS Glue ETL to coalesce small files into larger files (1 GB each), reducing S3 API overhead and improving query parallelization",
        "Partition data by additional dimensions (event_date, event_type, region) to enable more granular partition pruning and reduce data scanned per query",
        "Enable S3 Select and configure Athena to use S3 Select for filtering, which pushes predicate evaluation down to S3 and reduces data transfer",
        "Create materialized views in Athena for common aggregations (daily user counts), which are automatically refreshed and queried instead of raw data"
      ],
      "correctAnswer": [0, 1, 2],
      "explanation": "Options 0, 1, and 2 are correct. Option 0: Converting from JSON to Parquet with compression provides massive performance and cost improvements. Parquet is a columnar format optimized for analytics: 1) Columnar storage - Athena only reads columns referenced in the query (user_id), not entire rows, 2) Compression - Snappy compression reduces file sizes by 80-90%, 3) Encoding - Parquet uses efficient encoding (dictionary, run-length), 4) Predicate pushdown - filtering happens at the file/column level before loading data. Real-world impact: 10 TB JSON → ~1 TB Parquet compressed. Query scans 1 TB instead of 10 TB, reducing cost from $5/query (10 TB × $5/TB scanned) to $0.50/query. Query time improves 5-10x. Use AWS Glue or Apache Spark to convert data. Option 1: Athena query result caching stores results of previous queries for 24 hours. If users run the same query multiple times (common for dashboards), subsequent executions return cached results instantly with no data scanned and no cost. Requirements: 1) Exact same SQL query (case-sensitive), 2) Underlying data has not changed, 3) Within 24-hour TTL. For dashboards refreshing every 15 minutes with the same queries, this eliminates 95% of redundant scans. Note: DDL changes (table schema changes) invalidate cache. Option 2: Small files cause performance issues in Athena (and Spark-based engines) because: 1) S3 overhead - each file requires API calls and metadata lookups, 2) Parallelization - Athena creates tasks per file/split, many small files create overhead, 3) Optimal file size for Athena: 100 MB - 1 GB. Coalescing 10 TB of 100 MB files (100,000 files) into 1 GB files (10,000 files) reduces S3 API calls and improves parallelization. Use AWS Glue ETL with groupFiles option or Apache Spark with repartition/coalesce. Option 3 is partially beneficial but can cause issues - over-partitioning creates too many small files and partitions, causing metadata overhead. Athena has a limit of 20,000 partitions per table. Partitioning by event_date (365 partitions/year) is good. Adding event_type (10 types) and region (5 regions) creates 365 × 10 × 5 = 18,250 partitions. This can cause: 1) Slow metadata operations (SHOW PARTITIONS), 2) Glue Data Catalog limits, 3) Many small files per partition. Better approach: partition by date (coarse), use Parquet with predicate pushdown for filtering on event_type and region. Only add partitions for high-cardinality, frequently filtered columns. Option 4 is incorrect - S3 Select is designed for simple filtering on individual objects (e.g., SELECT * FROM S3Object WHERE field = 'value'), not for Athena's distributed query engine. Athena already implements predicate pushdown and columnar scanning with Parquet. S3 Select doesn't integrate with Athena for improving query performance. Option 5 is incorrect - Athena does NOT support materialized views in the traditional database sense (pre-computed, automatically refreshed). You can manually create summary tables using CREATE TABLE AS SELECT (CTAS) and schedule refreshes with AWS Glue workflows or Step Functions, but this is manual management, not automatic materialized views. Amazon Redshift supports materialized views with auto-refresh; Athena does not. Key principle: Athena cost and performance optimization: 1) Use columnar formats (Parquet, ORC) with compression, 2) Optimize file sizes (100 MB - 1 GB), 3) Partition strategically (date is common), 4) Enable query result caching, 5) Use CTAS for frequently accessed aggregations, 6) Monitor data scanned and query patterns, 7) Consider Athena workgroups for cost allocation and query limits. Athena pricing: $5 per TB scanned (with 10 MB minimum per query). Partitioning, compression, and columnar formats directly reduce TB scanned."
    },
    {
      "question": "A microservices application uses ECS Fargate with Application Load Balancer. Each microservice has a target group with health checks configured (path: /health, interval: 30 seconds, healthy threshold: 2, unhealthy threshold: 2). During deployments using ECS rolling updates, new tasks fail health checks and rollback occurs, even though the /health endpoint returns 200 OK when tested manually. CloudWatch Logs show the application starts successfully and listens on port 8080. What is the MOST likely cause of the health check failures during deployment?",
      "options": [
        "ECS Fargate tasks take 60-90 seconds to fully initialize (pull image, start container, application startup), but ALB starts health checks immediately. New tasks fail initial health checks before they're ready",
        "ALB health checks originate from ALB nodes in the VPC, and the ECS task security group does not allow inbound traffic from the ALB security group on port 8080, causing health checks to fail",
        "The /health endpoint requires authentication or specific headers, which ALB health checks don't provide, causing the endpoint to return 401 or 403 instead of 200 OK",
        "ECS task definition has 'healthCheck' configured, which conflicts with ALB health checks. ECS container-level health checks fail before ALB health checks begin, causing task termination"
      ],
      "correctAnswer": 0,
      "explanation": "Option A is correct and represents a common deployment issue with containerized applications. The sequence of events: 1) ECS rolling update starts a new Fargate task, 2) ECS pulls the container image from ECR (10-30 seconds depending on image size and caching), 3) Container starts and application initializes (database connection pools, caching, configuration loading - potentially 30-60 seconds), 4) Application begins listening on port 8080, 5) ALB begins health checks as soon as the task is registered with the target group (happens early in the process). If health checks start before the application is ready, the health check fails (connection refused or timeout). With healthy threshold = 2, the task needs 2 consecutive successful checks (30 sec × 2 = 60 seconds), but if checks start too early, it might fail 2 consecutive checks first (unhealthy threshold = 2), causing the task to be marked unhealthy and replaced. Solution: 1) Increase ALB health check interval and healthy threshold to allow more time: interval 15 seconds, healthy threshold 3 = 45 seconds to mark healthy, 2) Implement ECS container health check with a startPeriod (grace period before first health check), e.g., startPeriod: 60 seconds, giving the container time to initialize, 3) Optimize application startup time (smaller images, lazy initialization, faster dependencies), 4) Use ECS deployment configuration with minimumHealthyPercent and maximumPercent to control rollout speed, 5) Implement better health check endpoints that verify actual readiness (database connectivity, cache availability) not just HTTP 200. Example ECS task definition health check: {command: ['CMD-SHELL', 'curl -f http://localhost:8080/health || exit 1'], interval: 30, timeout: 5, retries: 3, startPeriod: 60}. Option B is incorrect - if the security group blocked ALB health checks, manual testing from within the VPC would also fail (unless testing from a different security group or from inside the container). The scenario states manual testing succeeds, implying network connectivity is fine. However, this is a common misconfiguration to check: ensure ECS task security group allows inbound from ALB security group on the container port. Option C is incorrect - if the health check endpoint required authentication, manual testing would also fail unless credentials were provided. The scenario states manual testing returns 200 OK, so authentication is not the issue. Best practice: health check endpoints should be unauthenticated and lightweight. Option D is incorrect - ECS container-level health checks and ALB health checks are independent and complementary. ECS health checks determine if the container is healthy (if it fails, ECS stops the task). ALB health checks determine if the task should receive traffic (if it fails, ALB stops routing traffic but doesn't stop the task). They don't conflict. It's actually a best practice to have both: ECS health checks for container health, ALB health checks for traffic routing. Key principle: Container deployment health check strategy: 1) ECS container health check with startPeriod for initialization grace period, 2) ALB health check with appropriate thresholds and intervals, 3) Health check endpoints should verify actual readiness, not just process running, 4) Monitor deployment rollbacks and adjust health check settings iteratively, 5) Use blue/green deployments (ECS with CodeDeploy) for safer production deployments with automatic rollback. For ECS Fargate specifically: 1) Optimize image sizes (multi-stage builds, minimal base images), 2) Use image caching (Fargate caches recently used images), 3) Implement fast startup (lazy loading, parallel initialization), 4) Test deployment configurations in non-production environments."
    },
    {
      "question": "A real-time bidding application uses Amazon Kinesis Data Streams with 50 shards to ingest bid events (100,000 records/second). A Lambda function consumes the stream and processes bids. During peak load, Lambda is throttled with 'IteratorAge' metric showing values over 10 minutes, indicating processing lag. The Lambda function has concurrency limit of 1000, and current concurrent executions during peak is 50. The Lambda function execution duration is 200ms average. What is the root cause of the processing lag?",
      "options": [
        "Kinesis Data Streams has a shard limit on read throughput (2 MB/sec or 5 transactions/sec per shard). Lambda is polling too aggressively, exceeding the read limit and being throttled by Kinesis, causing lag",
        "Lambda concurrency for Kinesis stream processing is calculated as the number of shards (50 shards = 50 concurrent Lambda executions). With 100,000 records/sec and 200ms processing time per batch, Lambda cannot process records fast enough to keep up with ingestion",
        "The Lambda function is configured with 'Batch Size' too large (10,000 records), causing each function invocation to take too long to process, resulting in IteratorAge increase",
        "Kinesis Data Streams 'Enhanced Fan-Out' is not enabled, causing Lambda to use polling (GetRecords API) instead of push-based delivery, introducing latency and reducing throughput"
      ],
      "correctAnswer": 1,
      "explanation": "Option B is correct. For Kinesis Data Streams and Lambda integration, Lambda scales concurrency based on the number of shards - one concurrent Lambda execution per shard (can increase to 2 per shard temporarily under load, but baseline is 1:1). With 50 shards and 100,000 records/second: 1) Each shard receives ~2,000 records/second, 2) Lambda polls each shard and processes records in batches (default batch size: 100 records, max 10,000), 3) With 200ms processing time per invocation and 100 records per batch: each Lambda can process 5 batches/second × 100 records = 500 records/second, 4) But each shard is receiving 2,000 records/second, so Lambda falls behind by 1,500 records/second per shard. Result: IteratorAge increases as records queue up waiting for processing. Solutions: 1) Increase Lambda concurrency per shard by configuring 'ParallelizationFactor' (2-10) in the event source mapping, which processes multiple batches from a shard concurrently, 2) Optimize Lambda function to process faster (code optimization, increase memory for more CPU), 3) Increase batch size to process more records per invocation (reduce per-invocation overhead), but balance with processing time limits, 4) Add more shards to Kinesis stream to distribute load (though this doesn't change per-shard Lambda concurrency without ParallelizationFactor), 5) Use 'TumblingWindowInSeconds' for micro-batching and aggregation if applicable. Example: Setting ParallelizationFactor = 4 allows up to 4 concurrent Lambda executions per shard, increasing throughput 4x. Option A is incorrect - while Kinesis shards have read limits (2 MB/sec per shard, or 5 GetRecords transactions/sec per shard when using polling), Lambda's integration with Kinesis is optimized to stay within these limits. The 5 transactions/sec limit would allow reading up to 10 MB/sec if each GetRecords returns 2 MB, which is more than sufficient for most use cases. The lag is not due to Kinesis read throttling but Lambda processing throughput. Option C is incorrect - a large batch size would increase per-invocation processing time, but the root cause is that Lambda concurrency (50 concurrent executions for 50 shards) is insufficient for the ingestion rate. Adjusting batch size alone doesn't solve the concurrency limitation. Option D is incorrect - Enhanced Fan-Out provides dedicated read throughput (2 MB/sec per consumer per shard) and push-based delivery (SubscribeToShard API), which reduces latency (typically 70ms vs 200ms for polling). While Enhanced Fan-Out improves latency and allows multiple consumers to read at full speed, it doesn't change the fundamental Lambda concurrency model (still 1 concurrent execution per shard unless ParallelizationFactor is configured). For this scenario, the bottleneck is Lambda processing throughput, not read latency. Key principle: Lambda + Kinesis scaling: 1) Concurrency = shards × ParallelizationFactor (default ParallelizationFactor = 1), 2) Monitor IteratorAge (time between when record arrives in Kinesis and when Lambda processes it), IteratorAge > 0 consistently = processing lag, 3) Tune batch size, ParallelizationFactor, and Lambda performance, 4) Enhanced Fan-Out for multiple consumers or latency-sensitive workloads, 5) Use Kinesis Data Firehose for simple ETL to S3/Redshift/OpenSearch without custom Lambda code. For high-throughput scenarios: 1) Calculate required throughput: (records/sec / records per batch) × processing time = required concurrency, 2) Add shards or increase ParallelizationFactor to meet requirements, 3) Optimize Lambda (efficient code, appropriate memory allocation), 4) Consider alternatives (Kinesis Data Analytics, Flink, Apache Spark) for complex stream processing."
    },
    {
      "type": "multiple",
      "question": "A financial analytics platform uses Amazon OpenSearch Service (formerly Elasticsearch) with 10 m5.large.search data nodes across 3 Availability Zones. Users report slow search queries (5-10 seconds) for date range queries on log data (1 TB indexed). The cluster status is green, CPU is at 30%, JVM memory pressure is at 85%, and heap usage spikes during queries. Which THREE actions would improve query performance? (Select THREE)",
      "options": [
        "Upgrade to memory-optimized instances (r5.large.search) to provide more JVM heap memory and reduce JVM memory pressure and garbage collection pauses",
        "Implement index lifecycle management (ILM) to automatically roll over to new indices daily, delete old indices after 90 days, and search only recent indices for time-bound queries",
        "Enable UltraWarm nodes to move older, infrequently accessed data (> 30 days old) to cost-effective warm storage, reducing the working set size on hot data nodes",
        "Increase the number of shards from the default 5 to 50 to parallelize queries across more shards and improve query performance",
        "Implement search result caching by enabling the 'query cache' and 'request cache' in OpenSearch, which cache frequently executed queries and aggregations",
        "Disable replica shards to reduce cluster size and resource usage, as replicas are only needed for high availability, not query performance"
      ],
      "correctAnswer": [0, 1, 4],
      "explanation": "Options 0, 1, and 4 are correct. Option 0: JVM memory pressure at 85% is a critical issue. OpenSearch uses JVM heap for in-memory data structures (caches, buffers, query processing). High heap usage causes frequent garbage collection, which pauses query processing and causes slow response times. Memory-optimized instances (r5.large.search: 16 GB RAM vs m5.large.search: 8 GB RAM) provide more heap memory. OpenSearch allocates 50% of instance RAM to JVM heap (capped at 32 GB to stay below compressed OOPs threshold). With more heap, queries can process larger datasets without triggering GC pauses. Also tune: 1) JVM heap size (via cluster settings), 2) Circuit breakers to prevent OOM, 3) Field data cache size for aggregations. Option 1: Index lifecycle management (ILM) automates index management and significantly improves performance. For log data: 1) Roll over to a new index daily (or based on size: 50 GB), creating time-based indices (logs-2024-01-01, logs-2024-01-02), 2) Date range queries only search relevant indices (logs from last 7 days = 7 indices instead of 1 massive index), 3) Delete old indices after retention period (90 days), 4) Shrink indices to reduce shard count for old data. Smaller, time-bound indices improve: query performance (fewer documents to scan), index management (easier to delete old data), shard optimization (right-size shard count per index). Option 4: OpenSearch has two caches: 1) Query cache - caches filter clause results (e.g., date range filters), shared across shards, 2) Request cache - caches aggregation results for entire queries. Enabling these caches (they may be disabled or under-utilized) dramatically improves performance for repeated queries (dashboards, monitoring). For date range queries with aggregations on log data, request cache can return results instantly if the same query was run recently and data hasn't changed. Monitor cache hit rates and eviction rates to tune cache sizes. Option 2 is beneficial for cost optimization but doesn't improve query performance on hot data. UltraWarm moves old indices to S3-backed storage (1/10th the cost of hot storage), but queries on UltraWarm indices are slower than hot indices because data is stored remotely. Use UltraWarm for: archival data queried infrequently, compliance retention, cost optimization. For query performance on recent data, keep it on hot nodes. Option 3 is incorrect and counterproductive - increasing shard count from 5 to 50 would likely hurt performance. OpenSearch sharding best practices: 1) Shard size: 10-50 GB per shard (1 TB / 50 shards = 20 GB/shard is reasonable), 2) Shard count: too many shards create overhead (cluster state management, query coordination), 3) Default 5 shards is often too few for large indices, but 50 might be too many. Recommended: 10-20 shards for 1 TB index, or use ILM with multiple smaller indices. Over-sharding causes: excessive cluster state size, coordination overhead, reduced cache efficiency. Option 5 is incorrect - replica shards DO improve query performance. OpenSearch distributes search queries across both primary and replica shards, effectively doubling query throughput (if you have 1 replica, you have 2 copies of each shard, both can serve queries). Disabling replicas reduces query capacity by 50% and eliminates high availability (if a data node fails, you lose data). Keep at least 1 replica for production. Key principle: OpenSearch performance optimization: 1) Right-size instances (memory for heap, CPU for query processing), 2) Implement ILM for time-series data, 3) Optimize shard count and size, 4) Enable caching, 5) Use UltraWarm/Cold for old data, 6) Monitor JVM memory pressure, GC frequency, query latency, 7) Use replicas for performance and availability. For log analytics specifically: 1) Time-based indices with ILM, 2) Use date range filters (index pruning), 3) Limit returned fields (use 'fields' parameter), 4) Use index templates for consistent mapping, 5) Consider Amazon Kinesis Data Firehose for ingestion, 6) Use CloudWatch for monitoring OpenSearch cluster health."
    },
    {
      "question": "A data lake application uses AWS Glue to run ETL jobs that process data from S3, transform it, and load it into another S3 bucket in Parquet format. Glue jobs are scheduled to run every hour and process ~100 GB of data. Recently, jobs have been failing with 'OutOfMemoryError: Java heap space' errors. The Glue job uses 10 DPUs (Data Processing Units) with the standard worker type (G.1X = 1 DPU = 4 vCPU, 16 GB RAM, 64 GB disk). What is the MOST effective solution to resolve the memory errors?",
      "options": [
        "Increase the number of DPUs from 10 to 20 to provide more total memory across the Glue cluster, distributing data processing across more workers",
        "Change the worker type from G.1X to G.2X (2 DPU = 8 vCPU, 32 GB RAM), which provides more memory per worker for processing larger datasets",
        "Implement partitioning in the ETL job to process data in smaller chunks (e.g., partition by date and process one partition at a time) instead of loading all 100 GB into memory at once",
        "Enable Glue job bookmarks to track processed data and prevent reprocessing, reducing the amount of data loaded into memory during each run"
      ],
      "correctAnswer": 2,
      "explanation": "Option C is correct. The 'OutOfMemoryError: Java heap space' error in Glue jobs indicates that the Spark job is trying to load or process more data in memory than available heap space allows. Glue uses Apache Spark, which performs in-memory processing for performance. However, certain operations (wide transformations, joins, aggregations) require shuffling data across partitions and can cause memory pressure. The most effective solution is to process data in smaller chunks. For example: 1) If data is partitioned by date in S3 (s3://bucket/data/year=2024/month=01/day=01/), process one day at a time in a loop, 2) Use Spark's partitioning (repartition() or coalesce()) to control parallelism, 3) Avoid operations that pull entire datasets into driver memory (collect(), toPandas() on large datasets), 4) Use Spark's built-in optimizations (broadcast joins for small tables, predicate pushdown). This approach solves the root cause (trying to process too much data at once) without simply throwing more resources at the problem. Option A is partially helpful but inefficient - increasing DPUs adds more workers, which distributes processing, but if the Spark job itself has inefficient operations (e.g., loading all 100 GB into a single partition), more DPUs won't solve the problem. Also, more DPUs = higher cost. Option B is also partially helpful - G.2X workers have more memory per worker, which helps if individual tasks are memory-intensive. However, if the job is trying to load 100 GB into a single task/partition, even 32 GB RAM per worker is insufficient. This is a band-aid, not a root cause fix. Option D is incorrect for this scenario - Glue job bookmarks track which data has been processed to enable incremental processing (only process new data since last run). While this reduces data volume over time, it doesn't help if the job is failing on the initial 100 GB. Bookmarks are useful for ongoing incremental ETL, not for solving memory errors on existing jobs. Key principle: Glue/Spark memory optimization: 1) Process data in chunks (partitioning, incremental processing), 2) Optimize Spark operations (avoid collect(), use efficient joins, filter early), 3) Right-size workers (G.1X for general, G.2X for memory-intensive, G.4X/G.8X for very large data), 4) Monitor Spark UI and CloudWatch metrics (executor memory, shuffle spill, GC time), 5) Use Spark configuration tuning (spark.executor.memory, spark.driver.memory, spark.sql.shuffle.partitions), 6) Implement dynamic partitioning (use repartition() based on data size). For ETL jobs specifically: 1) Use columnar formats (Parquet, ORC) for efficient reading, 2) Implement partition pruning (filter on partition columns), 3) Use Glue DynamicFrame optimizations (pushdown predicates), 4) Consider Glue Streaming for continuous processing instead of batch, 5) Test with subsets of data before running full jobs."
    }
  ]
}
