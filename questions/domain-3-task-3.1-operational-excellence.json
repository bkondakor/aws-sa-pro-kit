{
  "domain": "Domain 3: Continuous Improvement for Existing Solutions",
  "task": "Task 3.1: Operational Excellence",
  "question_count": 12,
  "questions": [
    {
      "id": "D3-T3.1-Q1",
      "question": "A company needs to query application logs across 100 AWS accounts to troubleshoot a distributed transaction failure. Logs are stored in CloudWatch Logs in each account. Which approach provides the FASTEST query capability across all accounts?",
      "options": [
        "Use CloudWatch Logs Insights with cross-account cross-region functionality",
        "Export all logs to S3 and query with Athena",
        "Use CloudWatch Logs subscription filters sending to centralized Kinesis Data Streams",
        "Manually query each account's CloudWatch Logs individually"
      ],
      "correctAnswer": 0,
      "explanation": "CloudWatch Logs Insights supports cross-account, cross-region queries directly. You can select multiple log groups across accounts and regions in a single query, making it the fastest approach for ad-hoc troubleshooting. Setup requires: (1) Create a monitoring account, (2) Set up resource links in each source account, (3) Query from the monitoring account selecting all relevant log groups. Logs Insights uses a SQL-like query language with automatic field discovery. Option B (S3 + Athena) has export delay and requires data to be in S3 first - slower for real-time troubleshooting. Option C (Kinesis) is for streaming processing, not interactive queries. Option D is impractical for 100 accounts. CloudWatch Logs Insights pricing is per GB scanned, making it cost-effective for targeted queries. Use saved queries and dashboards for recurring analysis. The cross-account feature simplifies centralized monitoring without complex ETL pipelines."
    },
    {
      "id": "D3-T3.1-Q2",
      "question": "An application uses X-Ray for distributed tracing. The development team reports that trace data for failed requests is incomplete, missing segments from downstream Lambda functions. What is the MOST likely cause and solution?",
      "options": [
        "X-Ray sampling rate is too low; increase to 100% for all requests",
        "Lambda functions don't have X-Ray tracing enabled; enable active tracing on Lambda functions",
        "X-Ray SDK is not initialized in Lambda code; add X-Ray SDK initialization",
        "IAM role for Lambda lacks xray:PutTraceSegments permission"
      ],
      "correctAnswer": 1,
      "explanation": "Lambda requires explicit enablement of X-Ray active tracing either: (1) In Lambda console/CLI with TracingConfig mode: Active, or (2) Via infrastructure as code (CloudFormation, SAM, CDK). Without active tracing enabled, Lambda doesn't send trace segments to X-Ray even if the X-Ray SDK is in the code. When active tracing is enabled, Lambda automatically: patches HTTP requests, sends trace data, provides environment variables (AWS_XRAY_DAEMON_ADDRESS). Option A (sampling) wouldn't cause missing segments; sampling decisions are made at request entry - if a request is traced, all segments should appear. Option C is incorrect because Lambda's active tracing mode automatically instruments common libraries without SDK initialization (though SDK provides more features). Option D would cause permission errors logged in CloudWatch, not silently missing segments. Best practice: Enable active tracing on all Lambda functions in distributed traces, use X-Ray SDK for custom subsegments and annotations. Check X-Ray service map to visualize request flow and identify missing components."
    },
    {
      "id": "D3-T3.1-Q3",
      "question": "A company wants to automate patching of 500 EC2 instances across multiple accounts, with different maintenance windows for production (Sundays 2AM) and development (daily 2AM). Failed patches should trigger alerts. Which solution provides the MOST operationally efficient approach?",
      "options": [
        "Use Systems Manager Patch Manager with patch baselines, maintenance windows, and SNS notifications for compliance",
        "Create Lambda functions with CloudWatch Events (EventBridge) to trigger yum/apt update commands",
        "Manually apply patches during maintenance windows using SSH",
        "Use third-party patch management tools integrated with AWS"
      ],
      "correctAnswer": 0,
      "explanation": "AWS Systems Manager Patch Manager provides comprehensive, native patch management: (1) Patch baselines define which patches to install (security, critical, all), (2) Maintenance windows specify when patching occurs with different schedules per environment, (3) Run commands execute patching across instance groups (tags), (4) Patch compliance reporting shows which instances are compliant, (5) SNS integration for alerts on patch failures, (6) AWS-managed patch baselines updated by AWS as new patches release, (7) Support for cross-account patching with Organizations integration. For this scenario: Create maintenance window for production (Sunday 2AM, tagged Env=prod) and development (daily 2AM, tagged Env=dev). Associate patch baseline (e.g., AWS-DefaultPatchBaseline). Patch Manager handles orchestration, reboot, and compliance reporting. Option B (Lambda + EventBridge) requires custom code for patching logic, error handling, and reporting. Option C is not scalable or automatable. Option D adds cost and complexity. Patch Manager also supports custom patch baselines, testing patches before production (install-override list), and integration with Change Manager for change approval workflows."
    },
    {
      "id": "D3-T3.1-Q4",
      "question": "An operations team receives hundreds of CloudWatch Alarms daily, many from transient issues that self-resolve. They want alarms only when multiple related metrics indicate a real problem (e.g., high CPU AND high error rate AND high latency). Which CloudWatch feature addresses this?",
      "options": [
        "Create composite alarms combining multiple alarms with AND/OR logic",
        "Increase alarm evaluation periods to reduce false positives",
        "Use CloudWatch anomaly detection on each metric",
        "Configure SNS filter policies to suppress duplicate notifications"
      ],
      "correctAnswer": 0,
      "explanation": "CloudWatch composite alarms allow combining multiple alarms using boolean logic (AND, OR, NOT). For this scenario: Create individual alarms for CPU (>80%), error rate (>5%), and latency (>2s). Then create a composite alarm: ALARM when (CPUAlarm AND ErrorRateAlarm AND LatencyAlarm). The composite alarm triggers only when all three conditions are true simultaneously, reducing false positives from isolated metric spikes. Composite alarms support: nested composition (composite alarms referencing other composite alarms), up to 100 alarm rules, suppression of underlying alarm notifications (preventing alert fatigue). Option B (longer evaluation periods) may miss short-duration but severe issues. Option C (anomaly detection) helps identify unusual patterns but doesn't correlate multiple metrics. Option D (SNS filtering) suppresses notifications but doesn't change alarm logic. Use composite alarms for: complex failure scenarios requiring multi-metric correlation, reducing alert fatigue, implementing service-level indicators (SLIs) requiring multiple metrics. Configure different thresholds for warning (2 of 3) vs critical (all 3) composite alarms."
    },
    {
      "id": "D3-T3.1-Q5",
      "question": "A DevOps team manages infrastructure changes via CloudFormation but notices stacks showing DRIFT even though no manual changes were made. Investigation shows that some resources (security groups, IAM roles) were modified outside CloudFormation by automation scripts. How should they prevent and detect this?",
      "options": [
        "Run CloudFormation drift detection daily and automatically update stacks to fix drift",
        "Implement AWS Config rules to prevent changes to CloudFormation-managed resources and alert on violations",
        "Use CloudFormation Stack Policy to prevent updates to critical resources",
        "Delete and recreate stacks monthly to eliminate drift"
      ],
      "correctAnswer": 1,
      "explanation": "AWS Config rules provide preventive and detective controls for drift: (1) Create custom Config rules checking if resources are managed by CloudFormation (using cloudformation:stack-id tag or GetStackResources API), (2) Config rule evaluates on configuration changes, (3) Non-compliant resources (modified outside CloudFormation) trigger alerts via SNS, (4) Optional: Use Config remediation actions with Systems Manager Automation to revert unauthorized changes. This prevents drift by alerting immediately when out-of-band changes occur, allowing quick correction. Option A (auto-update stacks) is risky - drift might be intentional for valid reasons; automatic updates could revert legitimate emergency changes. Option C (Stack Policy) prevents CloudFormation updates but doesn't prevent external changes. Option D (recreation) causes unnecessary downtime. Best practice: (1) Tag CloudFormation-managed resources consistently, (2) Use Config to monitor for external changes, (3) Educate teams to make changes through CloudFormation only, (4) Run drift detection before stack updates to avoid conflicts. For preventing changes entirely: use SCPs denying API calls without CloudFormation role, or IAM policies restricting manual changes."
    },
    {
      "id": "D3-T3.1-Q6",
      "question": "A company uses AWS Service Catalog to provision pre-approved infrastructure for development teams. They want to ensure launched products (VPCs, databases) remain compliant with organizational standards over their lifecycle, detecting drift from the original portfolio configuration. What should they implement?",
      "options": [
        "Use AWS Config rules to monitor Service Catalog launched resources for compliance",
        "Enable Service Catalog TagOptions to track provisioned products",
        "Use CloudFormation drift detection on underlying stacks",
        "Implement AWS CloudTrail logging for Service Catalog actions"
      ],
      "correctAnswer": 0,
      "explanation": "AWS Config provides ongoing compliance monitoring for Service Catalog-launched resources: (1) Service Catalog products are deployed via CloudFormation stacks, (2) Config rules evaluate these stacks' resources against compliance requirements (encryption, tagging, network configs), (3) Config detects configuration changes over time, reporting non-compliance, (4) Service Catalog integrates with Config for governance at scale. Example rules: ensuring S3 buckets have encryption, RDS has backups enabled, EC2 in approved VPCs. Option B (TagOptions) helps with organization and cost allocation but doesn't monitor compliance. Option C (drift detection) shows changes from original template but requires manual triggering and doesn't evaluate against compliance policies. Option D (CloudTrail) logs actions but doesn't evaluate compliance. Service Catalog best practices: (1) Define constraints in portfolios (launch constraints, tag update constraints), (2) Use Config for continuous monitoring, (3) Integrate with AWS Budgets for cost control, (4) Use CloudFormation StackSets to update products across accounts. Service Catalog constraints can enforce things like specific IAM roles, required tags, and resource limits at launch time."
    },
    {
      "id": "D3-T3.1-Q7",
      "question": "An application generates high-cardinality custom metrics (unique customer IDs as dimensions) in CloudWatch, resulting in thousands of metric streams and high costs. Which approach optimizes cost while maintaining observability?",
      "options": [
        "Use CloudWatch embedded metric format (EMF) in application logs, extracting metrics only when needed for queries",
        "Continue publishing all metrics but increase aggregation period from 1 minute to 5 minutes",
        "Store custom metrics in DynamoDB instead of CloudWatch",
        "Publish metrics to S3 and use Athena for analysis"
      ],
      "correctAnswer": 0,
      "explanation": "CloudWatch Embedded Metric Format (EMF) provides cost-effective high-cardinality metrics: (1) Application logs structured JSON to CloudWatch Logs with metric metadata, (2) Metrics are extracted automatically from logs, appearing in CloudWatch Metrics, (3) You only pay for log storage ($0.50/GB ingested) instead of custom metrics ($0.30 per metric), (4) High-cardinality dimensions (customer IDs) can be included in logs without creating thousands of metric streams, (5) CloudWatch Logs Insights can query logs with full dimensionality. EMF is ideal for: high-cardinality metrics, metrics from serverless functions (Lambda automatically uses EMF), scenarios where not all dimensions are queried regularly. Option B (longer aggregation) saves some storage but loses granularity and doesn't address cardinality. Option C (DynamoDB) requires custom code for metric collection, querying, and visualization - reinventing CloudWatch. Option D (S3/Athena) has query latency unsuitable for real-time dashboards. EMF format example: {\"_aws\": {\"CloudWatchMetrics\": [{\"Namespace\": \"App\", \"Metrics\": [{\"Name\": \"Latency\"}], \"Dimensions\": [[\"CustomerId\"]]}]}, \"CustomerId\": \"12345\", \"Latency\": 145}. This appears as both a log entry and a CloudWatch metric."
    },
    {
      "id": "D3-T3.1-Q8",
      "question": "A company runs a multi-tier application with ALB, ECS, and RDS. They want automated remediation when: ALB returns 5XX errors, ECS tasks restart frequently, or RDS CPU exceeds 90%. Actions should include: restart tasks, scale out, or page on-call engineer. Which architecture provides this automation?",
      "options": [
        "CloudWatch Alarms triggering Lambda functions with remediation logic, using SNS for paging",
        "EventBridge rules detecting CloudWatch alarm state changes, triggering Systems Manager Automation documents for remediation and SNS for paging",
        "CloudWatch Alarms directly triggering Auto Scaling policies and SNS topics",
        "AWS Config remediation actions for non-compliant resources"
      ],
      "correctAnswer": 1,
      "explanation": "EventBridge + Systems Manager Automation provides comprehensive remediation automation: (1) CloudWatch Alarms detect issues (5XX, task restarts, high CPU), (2) Alarms change state (OK â†’ ALARM), (3) EventBridge rule matches alarm state change events, (4) EventBridge triggers Systems Manager Automation documents with remediation logic, (5) Automation documents can: restart ECS tasks (ECS:UpdateService), modify RDS (RDS:ModifyDBInstance), invoke Lambda, execute AWS APIs, (6) SNS notifies on-call for severe issues. This architecture separates concerns: CloudWatch for detection, EventBridge for routing, Automation for remediation, SNS for human notification. Option A works but Lambda requires custom code for each remediation; Automation documents are reusable and AWS-managed. Option C (direct alarm actions) limited to Auto Scaling and SNS - can't restart tasks or modify RDS. Option D (Config remediation) for configuration compliance, not performance issues. Systems Manager Automation benefits: visual workflow editor, AWS-managed documents for common tasks (e.g., AWS-StopEC2Instance), approval steps for human validation, runbooks as code (version control). Use Automation for: self-healing (restart failed components), auto-remediation (security group fixes), operational runbooks (deployment procedures)."
    },
    {
      "id": "D3-T3.1-Q9",
      "question": "A global application uses CloudWatch dashboards for monitoring. Operations teams in different regions want customized views (US team sees US resources, EU team sees EU resources) without maintaining separate dashboards. How can this be achieved?",
      "options": [
        "Create dashboard variables allowing users to select region dynamically",
        "Duplicate dashboards per region with different resource filters",
        "Use CloudWatch cross-region functionality but manually switch regions",
        "Create a custom dashboard application querying CloudWatch APIs"
      ],
      "correctAnswer": 0,
      "explanation": "CloudWatch dashboard variables (also called dynamic dashboards) allow runtime customization: (1) Define variables for dimensions like Region, InstanceType, Environment, (2) Dashboard widgets reference variables: {region}, {instance}, (3) Users select variable values from dropdowns, (4) Dashboard updates to show selected resources. This enables a single dashboard serving multiple teams/regions. Variables support: property values (regions, AZs), dimension values from metrics, label values, custom values. For this scenario: Create variable 'region' with values [us-east-1, eu-west-1, ap-southeast-1], reference in widgets: \"AWS/EC2\" metrics for region=variable.region. Users switch regions via dropdown. Option B (duplicate dashboards) creates maintenance burden - changes must be applied to all copies. Option C still requires manual switching. Option D is unnecessary complexity. Dashboard variables are also useful for: environment selection (dev/staging/prod), application filtering (AppA/AppB), auto-scaling group selection. Combine with CloudWatch dashboard sharing and IAM permissions to provide role-based dashboard access. Variables can populate from CloudWatch Metric streams dynamically (e.g., all Auto Scaling groups in the account)."
    },
    {
      "id": "D3-T3.1-Q10",
      "question": "A company needs to inventory all EC2 instances, RDS databases, and S3 buckets across 50 AWS accounts, including configuration details (encryption, public access, tags). They need this data queryable for compliance reports. Which AWS service provides this with minimal operational overhead?",
      "options": [
        "AWS Config with aggregator for multi-account configuration tracking",
        "Custom Lambda functions querying AWS APIs and storing results in DynamoDB",
        "AWS Systems Manager Inventory for resource data collection",
        "CloudTrail logs analysis with Athena"
      ],
      "correctAnswer": 0,
      "explanation": "AWS Config with multi-account multi-region aggregator provides comprehensive resource inventory: (1) Enable Config in all accounts to record resource configurations, (2) Create aggregator in central account collecting data from all source accounts, (3) Config tracks: resource type, creation time, relationships, configuration changes over time, (4) Config Advanced Queries use SQL to query across all accounts/regions, (5) Compliance dashboard shows aggregate compliance across organization. Example query: SELECT resourceId, resourceType, configuration.encrypted WHERE resourceType = 'AWS::RDS::DBInstance' AND configuration.encrypted = false. This identifies unencrypted RDS instances across all accounts. Option B (custom Lambda) requires significant development for API pagination, handling limits, incremental updates, and maintaining schema. Option C (Systems Manager Inventory) focuses on EC2 instance software inventory (applications, OS details), not broad AWS resource inventory. Option D (CloudTrail) logs API calls but doesn't maintain current state inventory. Config aggregator supports: up to 10,000 source accounts, retention of configuration history (up to 7 years), snapshot delivery to S3 for compliance archives. Use Config for: compliance reporting, resource tracking, change management, security audits, cross-account asset inventory."
    },
    {
      "id": "D3-T3.1-Q11",
      "question": "An application team wants to receive notifications when CloudFormation stack operations fail, succeed, or require manual intervention. They use stacks across multiple accounts and want centralized notification handling. Which solution provides this?",
      "options": [
        "Configure SNS topics per stack with subscriptions for operations team",
        "Use EventBridge rules matching CloudFormation events, routing to central SNS topic across accounts",
        "Enable CloudFormation stack notifications in each stack configuration",
        "Use CloudTrail to log CloudFormation API calls and trigger Lambda on failures"
      ],
      "correctAnswer": 1,
      "explanation": "EventBridge (CloudWatch Events) provides centralized, event-driven notifications for CloudFormation: (1) CloudFormation emits events for stack operations (create complete/failed, update complete/failed, drift detected), (2) EventBridge rules match these events using event patterns, (3) Rules can route to cross-account/cross-region targets including SNS, (4) Single SNS topic receives all CloudFormation notifications from all stacks/accounts, (5) SNS filters allow subscribers to filter by account, stack name, or operation type. Event pattern example: {\"source\": [\"aws.cloudformation\"], \"detail-type\": [\"CloudFormation Stack Status Change\"], \"detail\": {\"stack-status\": [\"CREATE_FAILED\", \"UPDATE_FAILED\"]}}. This matches only failure events. Option A (SNS per stack) requires configuring each stack individually - not scalable. Option C (stack notifications) requires manual config per stack and doesn't aggregate across accounts. Option D (CloudTrail + Lambda) adds unnecessary complexity when EventBridge provides native event matching. EventBridge benefits: event filtering (only failures, only specific stacks), transformation (customize notification format), multiple targets (SNS, Lambda, Step Functions simultaneously), cross-account event bus for central event collection. Use for: centralized operations monitoring, compliance tracking (drift detection events), integration with incident management systems."
    },
    {
      "id": "D3-T3.1-Q12",
      "question": "A company enforces tagging standards (Project, Environment, Owner) on all resources. They want automated detection and remediation: new untagged resources should be tagged automatically if possible, or notifications sent to resource owners for manual tagging. Which combination achieves this? (Select TWO)",
      "options": [
        "AWS Config rule detecting untagged resources with automatic remediation via Systems Manager Automation",
        "Service Control Policy (SCP) denying resource creation without required tags",
        "EventBridge rule detecting resource creation events, triggering Lambda to tag resources",
        "CloudFormation drift detection to identify tagging drift",
        "AWS Organizations tag policies enforcing required tags",
        "CloudWatch alarm on untagged resource count"
      ],
      "type": "multiple",
      "correctAnswer": [0, 2],
      "explanation": "Comprehensive tagging automation requires detection and remediation: (1) AWS Config rule for tag compliance - evaluates resources against tagging requirements, marks non-compliant resources. Config remediation actions trigger Systems Manager Automation to apply tags automatically (if permissions allow). (2) EventBridge rule on resource creation - detects AWS API calls creating resources (via CloudTrail), triggers Lambda to immediately tag new resources before they violate compliance. This provides real-time tagging vs Config's periodic evaluation. Option B (SCP) is preventive but blocks resource creation entirely if tags missing - too strict for scenarios where tags should be applied post-creation. Option D (drift detection) is for CloudFormation managed resources only. Option E (tag policies) validates tag keys/values but doesn't automatically remediate. Option F (CloudWatch alarm) doesn't provide remediation. Best practice: Layered approach: (1) Tag policies enforce valid values, (2) EventBridge + Lambda for immediate tagging, (3) Config for compliance detection, (4) Systems Manager Automation for remediation, (5) SCPs as last resort to prevent untagged resources in critical environments. Tag automation Lambda should: attempt to infer tags from creator identity (Owner), environment from VPC (Environment), query CMDB for Project tag, notify if tags can't be determined."
    }
  ]
}
