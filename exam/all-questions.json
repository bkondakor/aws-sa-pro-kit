{
  "metadata": {
    "totalFiles": 15,
    "totalQuestions": 132,
    "domains": [
      "Domain 1: Organizational Complexity",
      "Domain 2: Design for New Solutions",
      "Domain 3: Continuous Improvement",
      "Domain 3: Continuous Improvement for Existing Solutions",
      "Domain 4: Accelerate Workload Migration and Modernization"
    ],
    "lastUpdated": "2025-11-18T21:49:04.846Z"
  },
  "questionSets": [
    {
      "filename": "domain-1-task-1.1-network-connectivity.json",
      "domain": "Domain 1: Organizational Complexity",
      "task": "Task 1.1: Network Connectivity",
      "question_count": 12,
      "questions": [
        {
          "question": "A global financial services company has four 100 Gbps AWS Direct Connect connections in a Link Aggregation Group (LAG) at their primary location. They need to enable MACsec encryption for security compliance. After reviewing the requirements, what is the PRIMARY limitation they will face?",
          "options": [
            "MACsec is not supported on 100 Gbps connections, only on 10 Gbps and 400 Gbps",
            "Each connection in the LAG can use a different MACsec key for enhanced security",
            "LAGs with 100 Gbps connections can only have a maximum of two connections, not four",
            "MACsec requires dynamic CAK mode which is not supported on Direct Connect"
          ],
          "correctAnswer": 2,
          "explanation": "According to AWS Direct Connect limits, LAGs can have a maximum of 4 connections when the port speed is 1 Gbps or 10 Gbps, but only 2 connections when the port speed is 100 Gbps or 400 Gbps. This means they would need to reduce from four to two connections. Option A is incorrect because MACsec IS supported on 100 Gbps (also 10 Gbps and 400 Gbps). Option B is incorrect because only a single MACsec key can be used across all LAG links at any time (multiple keys are only for rotation). Option D is incorrect because Direct Connect supports static CAK mode, which is what's required (dynamic CAK is not supported, but static is)."
        },
        {
          "question": "An enterprise with a hub-and-spoke network architecture using AWS Transit Gateway across multiple regions is experiencing routing issues. They have 15,000 routes that need to be advertised from on-premises through a transit virtual interface. What is the MOST likely cause of their connectivity problems?",
          "options": [
            "Transit Gateway route tables have a hard limit of 10,000 routes, exceeding which causes route drops",
            "Transit virtual interfaces support only 100 prefixes from AWS to on-premises by default",
            "Transit Gateway VPN connections have a hard limit of 100 BGP routes, causing random BGP session resets",
            "The MTU size of 8500 bytes on Transit Gateway is causing packet fragmentation issues"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Transit Gateway route tables can hold up to 10,000 routes (static or propagated combined). With 15,000 routes being advertised, this exceeds the limit and will cause routing issues. Option B is partially correct about limits but outdated - the prefix limit per Transit Gateway from AWS to on-premises on a transit virtual interface was increased to 200 in early 2023, and this relates to routes FROM AWS, not TO AWS. Option C is true about VPN connections but the scenario mentions a transit virtual interface (Direct Connect), not VPN. Option D is incorrect because 8500 byte MTU is supported for Direct Connect attachments and wouldn't cause connectivity problems; rather it's a feature for jumbo frames."
        },
        {
          "question": "A company is designing a multi-region network architecture and is evaluating between AWS Transit Gateway with cross-region peering and AWS Cloud WAN. They have workloads in 8 AWS regions with plans to expand to 15 regions, require automated VPC attachments, and need centralized network policy management. Which solution is MOST appropriate and why?",
          "options": [
            "AWS Transit Gateway because it provides better cross-region performance and lower latency than Cloud WAN",
            "AWS Cloud WAN because it provides centralized management, automated VPC attachments, and global network automation that scales across multiple regions",
            "AWS Transit Gateway because Cloud WAN doesn't support integration with existing Transit Gateway infrastructure",
            "AWS Cloud WAN because Transit Gateway has a hard limit of 5 regions per deployment"
          ],
          "correctAnswer": 1,
          "explanation": "AWS Cloud WAN is specifically designed for multi-region, global network deployments with centralized management and automation. It provides automated VPC attachments, centralized policy management, and automatic inter-region connectivity without manual peering configuration. With 8+ regions and plans for 15, Cloud WAN's automated management significantly reduces operational overhead compared to manually managing Transit Gateway peering connections across 15 regions. Option A is incorrect - both services provide similar performance characteristics. Option C is false - Cloud WAN can federate with Transit Gateways and replace manual peering. Option D is false - Transit Gateway has no such 5-region limit; you can use it in all AWS regions, but managing many regions becomes operationally complex."
        },
        {
          "question": "A Solutions Architect is designing a hybrid network with redundant connectivity. The company has two AWS Direct Connect connections at different locations and wants to implement backup VPN connectivity. For both Direct Connect connections, they're using BGP with the same AS number, and they want the VPN to only be used when both Direct Connect connections fail. Which configuration achieves this MOST effectively?",
          "options": [
            "Configure the VPN with a longer AS path prepend to make it less preferred than Direct Connect routes",
            "Set the Direct Connect BGP routes with a local preference of 200 and VPN routes with local preference of 100",
            "Use the same BGP weight on all connections and rely on the inherent Direct Connect route preference",
            "Configure Direct Connect to advertise routes with MED value of 50 and VPN with MED value of 100"
          ],
          "correctAnswer": 0,
          "explanation": "AS path prepending is the correct approach for making VPN backup routes less preferred. When you prepend additional AS numbers to the VPN route advertisements, the BGP path becomes longer, making it less preferred in BGP path selection. Direct Connect routes will naturally be preferred due to shorter AS path. This ensures VPN is only used when Direct Connect fails. Option B discusses local preference, which is configured on the AWS side and controls outbound traffic preference from AWS, but the question implies controlling both directions and ensuring proper failover. Option C is incorrect because while Direct Connect routes are generally preferred over VPN, relying solely on implicit preferences without explicit configuration isn't a reliable design pattern. Option D (MED) works for influencing incoming traffic from a single AS, but AS path prepending is more universally effective across different routing scenarios."
        },
        {
          "question": "An organization has overlapping CIDR ranges (10.0.0.0/16) in two VPCs that cannot be re-addressed due to legacy application constraints. Both VPCs need to communicate with a central shared services VPC. The shared services VPC should be able to initiate connections to specific subnets in both VPCs. What is the MOST operationally efficient solution?",
          "options": [
            "Use Transit Gateway with separate route tables and route propagation to handle overlapping CIDRs automatically",
            "Deploy AWS PrivateLink endpoints in both VPCs, allowing the shared services VPC to access services without direct network routing",
            "Implement NAT Gateways in each VPC with Elastic IPs and use IP-based routing in the shared services VPC",
            "Use VPC Peering with longest prefix match routing to route to the correct VPC based on more specific subnet ranges"
          ],
          "correctAnswer": 1,
          "explanation": "AWS PrivateLink is specifically designed to solve the overlapping IP problem. Services in the overlapping VPCs can be exposed through PrivateLink endpoints, which are accessed via unique DNS names and endpoint-specific IP addresses that don't overlap. The shared services VPC can connect to these endpoint services without requiring direct network routing between overlapping CIDR ranges. Option A is incorrect because Transit Gateway does NOT support overlapping CIDRs between attached VPCs - this is a fundamental limitation. Option C is overly complex and doesn't truly solve the bidirectional communication problem efficiently. Option D is incorrect because VPC Peering also does not support overlapping CIDR blocks - this is explicitly documented as a limitation."
        },
        {
          "question": "A company using AWS Site-to-Site VPN with BGP routing to their corporate data center is experiencing issues where their Transit Gateway VPN connection randomly drops and re-establishes BGP sessions every few hours. They have 150 BGP prefixes being advertised from on-premises. What is the MOST likely root cause?",
          "options": [
            "The VPN tunnel encryption overhead is causing packet loss at high throughput",
            "Transit Gateway VPN has a hard limit of 100 BGP routes, and exceeding this causes random BGP session resets",
            "BGP keepalive timers are misconfigured, causing the sessions to time out prematurely",
            "The VPN connection is experiencing MTU issues because Transit Gateway doesn't support PMTUD on VPN"
          ],
          "correctAnswer": 1,
          "explanation": "This is a critical and tricky limitation: Transit Gateway VPN connections have the same hard limit of 100 BGP routes as classic VGW VPN. When BGP prefixes exceed 100, the TGW VPN randomly resets the BGP session, leading to unpredictable network outages. With 150 prefixes being advertised, this exceeds the limit and would cause exactly the behavior described. The solution would be to summarize routes or use static routing. Option A is unlikely to cause random BGP resets. Option C could cause issues but wouldn't specifically correlate with the number of routes. Option D is true (Transit Gateway doesn't support PMTUD on VPN) but this would cause fragmentation issues, not random BGP session resets."
        },
        {
          "question": "A global corporation needs to connect 6,000 VPCs across multiple AWS accounts and regions. They require network segmentation, where only certain groups of VPCs can communicate with each other. They want to minimize operational overhead. Which architecture should they implement?",
          "options": [
            "Deploy multiple Transit Gateways, one per segment, with VPC attachments and cross-region peering",
            "Use a single AWS Cloud WAN with segment-based policies and automated VPC attachment rules",
            "Implement VPC Peering with a hub-and-spoke topology using a central inspection VPC",
            "Cannot be achieved as Transit Gateway has a hard limit of 5,000 VPC attachments"
          ],
          "correctAnswer": 1,
          "explanation": "AWS Cloud WAN is the optimal solution for this scale and requirement. Cloud WAN supports network segmentation through segments (isolated routing domains) and can handle thousands of VPC attachments with automated attachment policies. It provides centralized configuration and policy management across regions, significantly reducing operational overhead compared to managing multiple Transit Gateways. Option A would work but creates significant operational overhead managing multiple Transit Gateways and their peering connections at this scale. Option C (VPC Peering) is not scalable - you'd need thousands of individual peering connections. Option D contains a true fact (Transit Gateway limit is 5,000 VPCs) but that's why this scenario exceeds a single TGW capacity and Cloud WAN is needed, which can scale beyond a single TGW's limits through its distributed architecture."
        },
        {
          "question": "A company has a Direct Connect connection with a private virtual interface attached to a Virtual Private Gateway (VGW) for a single VPC. They want to expand connectivity to 50 VPCs in the same region without creating 50 separate virtual interfaces. What is the MOST scalable solution?",
          "options": [
            "Create a Transit Gateway, migrate the VGW attachment to a Transit Virtual Interface, and attach all VPCs to the Transit Gateway",
            "Use VPC Peering to connect all 50 VPCs to the original VPC that has the Direct Connect connection",
            "Create 50 private virtual interfaces on the same Direct Connect connection, one for each VPC",
            "Use AWS PrivateLink to share the Direct Connect connectivity across all VPCs"
          ],
          "correctAnswer": 0,
          "explanation": "Migrating to Transit Gateway with a Transit Virtual Interface is the correct and most scalable approach. A single Direct Connect connection with a Transit Virtual Interface can connect to a Transit Gateway, which can then attach up to 5,000 VPCs. This provides a hub-and-spoke model with centralized routing. Option B (VPC Peering) would require 49 peering connections from the primary VPC and doesn't provide a scalable routing model for on-premises connectivity. Option C technically works but is operationally nightmare - managing 50 virtual interfaces and their BGP sessions is not scalable and you're limited by the number of virtual interfaces per connection (50 private VIFs limit). Option D is incorrect - PrivateLink is for service connectivity, not for providing network-layer Direct Connect connectivity."
        },
        {
          "question": "A company is implementing AWS Direct Connect with MACsec encryption for compliance. They are using 10 Gbps dedicated connections. During the security review, the CISO asks which encryption cipher will be used. What should the Solutions Architect respond?",
          "options": [
            "AES-128-GCM only, as it's the standard for 10 Gbps connections",
            "Either GCM-AES-256 or GCM-AES-XPN-256, both are supported for 10 Gbps connections",
            "GCM-AES-XPN-256 only, as it's required for all MACsec implementations",
            "TLS 1.3 with AES-256, as Direct Connect uses TLS for encryption"
          ],
          "correctAnswer": 1,
          "explanation": "For 10 Gbps Direct Connect connections, AWS MACsec supports both GCM-AES-256 and GCM-AES-XPN-256 cipher suites. The XPN (Extended Packet Numbering) variant provides extended packet numbering for very high-volume traffic scenarios. Only 256-bit keys are supported (not 128-bit). For 100 Gbps and 400 Gbps connections, only GCM-AES-XPN-256 is supported. Option A is incorrect because 128-bit is not supported; only 256-bit keys are supported. Option C is incorrect because for 10 Gbps, you have a choice between standard and XPN variants. Option D is fundamentally wrong - MACsec operates at Layer 2 (data link layer) and uses GCM-AES encryption, not TLS which is a Layer 4/7 protocol."
        },
        {
          "question": "An enterprise is designing a hybrid DNS architecture using Amazon Route 53 Resolver. They need on-premises servers to resolve AWS private hosted zone queries and AWS resources to resolve on-premises DNS queries. The on-premises network is connected via Direct Connect. Which components are required? (Select THREE)",
          "options": [
            "Route 53 Resolver inbound endpoints in AWS VPCs",
            "Route 53 Resolver outbound endpoints in AWS VPCs",
            "Resolver rules forwarding on-premises domains to on-premises DNS servers",
            "VPC DNS resolution enabled (enableDnsSupport)",
            "A separate NAT Gateway for DNS traffic",
            "Route 53 public hosted zones configured as private"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            1,
            2
          ],
          "explanation": "This requires three main components: (1) Inbound endpoints - allow on-premises DNS servers to forward queries to Route 53 Resolver for private hosted zones, (2) Outbound endpoints - allow AWS resources to forward queries for on-premises domains to on-premises DNS servers, and (3) Resolver rules - define which domains should be forwarded to on-premises DNS servers. While VPC DNS resolution (Option D) must be enabled as a prerequisite, the question asks for the three main components you actively configure. Option E (NAT Gateway) is not needed - DNS traffic flows through the Direct Connect connection using the Resolver endpoints. Option F is incorrect - you don't convert public hosted zones to private; private hosted zones are created separately for internal DNS resolution."
        },
        {
          "question": "A company has a Transit Gateway with three route tables: Production, Development, and Shared-Services. The Production VPCs should be able to access Shared-Services but NOT Development. Development VPCs should access both Shared-Services and Production for read-only database access. Shared-Services should reach both. What is the CORRECT route table association and propagation strategy?",
          "options": [
            "Associate each VPC with its own route table, propagate Shared-Services routes to all tables, propagate Production to Development and Shared-Services tables only",
            "Associate each VPC with its own route table, propagate routes bidirectionally between all tables to ensure full connectivity",
            "Use a single route table with security groups to control access between environments",
            "Associate Production and Development with separate tables, use blackhole routes in Production table for Development CIDRs, propagate all routes to Shared-Services table"
          ],
          "correctAnswer": 0,
          "explanation": "Transit Gateway route table association and propagation works as follows: Each VPC attachment is associated with ONE route table (which determines where traffic FROM that VPC can go). Route propagation determines which VPC routes appear in which route tables. For this scenario: Production VPCs associate with Production route table (which has Shared-Services routes propagated, but NOT Development routes). Development VPCs associate with Development route table (which has both Shared-Services AND Production routes propagated). Shared-Services VPCs associate with Shared-Services route table (which has both Production and Development routes propagated). Option B creates full mesh which violates the requirement. Option C is incorrect - security groups don't work across VPCs through Transit Gateway; routing controls the connectivity. Option D's blackhole approach is more complex than necessary and doesn't address the Development-to-Production read access requirement properly."
        },
        {
          "question": "A media streaming company requires their AWS resources to communicate with their on-premises data center using private IP addresses for services like S3 and DynamoDB, without traversing the public internet. They have Direct Connect established. Which architecture components are required? (Select THREE)",
          "options": [
            "VPC Gateway Endpoints for S3 and DynamoDB in each VPC",
            "AWS Transit Gateway with Direct Connect Gateway attachment",
            "VPC Interface Endpoints (AWS PrivateLink) for S3 and DynamoDB",
            "Direct Connect public virtual interface for AWS service access",
            "Route tables in each VPC routing S3 and DynamoDB prefixes to the Transit Gateway",
            "NAT Gateway for outbound connectivity to AWS services"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            1,
            4
          ],
          "explanation": "This scenario requires: (1) VPC Gateway Endpoints for S3 and DynamoDB - these create route table entries that direct traffic to these services through AWS's private network, (2) Transit Gateway with Direct Connect Gateway - this connects on-premises to the VPCs through private connectivity, and (3) Route table entries routing the VPC endpoint prefixes to the Transit Gateway so on-premises traffic can reach the gateway endpoints. The gateway endpoints use AWS's private IP space and don't traverse the internet. Option C (Interface Endpoints) could technically work but are more expensive than gateway endpoints for S3/DynamoDB and the question implies using gateway endpoints (the standard solution). Option D (public VIF) would traverse public AWS network space, not meeting the requirement. Option F (NAT Gateway) is for internet access, not AWS service access via private IPs."
        }
      ]
    },
    {
      "filename": "domain-1-task-1.2-security-controls.json",
      "domain": "Domain 1: Organizational Complexity",
      "task": "Task 1.2: Security Controls",
      "question_count": 12,
      "questions": [
        {
          "question": "A company has implemented AWS Organizations with multiple OUs. The security team has created an IAM policy allowing EC2:* actions and attached it to developers' roles. However, there's an SCP at the OU level denying ec2:TerminateInstances. A developer with the IAM policy tries to terminate an instance but receives an access denied error. After investigation, the security team updates the IAM policy to explicitly allow ec2:TerminateInstances. What will happen?",
          "options": [
            "The developer will now be able to terminate instances because the explicit IAM allow overrides the SCP deny",
            "The developer still cannot terminate instances because SCPs are at the top of the permission hierarchy and IAM policies cannot override SCP denies",
            "The developer can terminate instances only in the specific region where the IAM policy was updated",
            "The developer can terminate instances because IAM policies are evaluated before SCPs in the permission evaluation logic"
          ],
          "correctAnswer": 1,
          "explanation": "SCPs sit at the top of the AWS permission hierarchy. Even if an IAM policy explicitly grants a permission, an SCP can override this by denying it. If an SCP denies an action on an account, no entity in that account can perform that action, regardless of their IAM permissions. SCPs act as a permission filter - they set the maximum permissions available. The intersection of allowed permissions (IAM policy allows ec2:TerminateInstances) and SCP permissions (SCP denies ec2:TerminateInstances) results in a deny. The only way to fix this is to modify the SCP, not the IAM policy."
        },
        {
          "question": "An organization wants to implement ABAC (Attribute-Based Access Control) to reduce the number of IAM policies they manage. They have 50 development teams, each working on different projects. Currently, they have separate IAM roles for each team-project combination (200+ roles). Which ABAC implementation would be MOST effective for reducing policy management complexity?",
          "options": [
            "Create one IAM role per team with policies granting access only to resources where the resource tag 'Team' matches the principal's tag 'Team', and tag resources with project names",
            "Create one IAM role per project with policies checking both team and project tags, reducing roles from 200 to 50",
            "Create a single developer role with a policy that grants access when both principal tags (Team and Project) match the corresponding resource tags",
            "Keep separate roles but use ABAC to validate that resource tags match principal tags at runtime"
          ],
          "correctAnswer": 2,
          "explanation": "The most effective ABAC implementation is option C: a single IAM role with a policy using conditions like 'StringEquals: {\"aws:PrincipalTag/Team\": \"${aws:ResourceTag/Team}\", \"aws:PrincipalTag/Project\": \"${aws:ResourceTag/Project}\"}'. When developers federate into AWS, their Team and Project attributes from the IdP become session tags. The single policy grants access when tags match, eliminating the need for 200+ roles. This scales automatically - when new teams or projects are created, no policy updates are needed, just proper tagging. Option A still requires multiple roles (50). Option B only partially reduces roles. Option D doesn't actually reduce the number of roles. The key ABAC benefit is: you can allow actions on all resources if the resource's tag matches the principal's tag, dramatically reducing policy count."
        },
        {
          "question": "A security team needs to implement cross-account access for an application in Account A to read objects from an S3 bucket in Account B. The bucket contains highly sensitive data. What is the MOST secure implementation that follows AWS best practices?",
          "options": [
            "Create an IAM user in Account B with access keys, grant S3 read permissions, and store the credentials in AWS Secrets Manager in Account A",
            "Enable S3 bucket public access with an S3 bucket policy restricting access to the source IP addresses of Account A's resources",
            "Create an IAM role in Account B with S3 read permissions, configure a trust policy allowing Account A to assume it, and have the application in Account A assume the role using STS",
            "Use S3 bucket ACLs to grant the Account A root account read permissions on all objects"
          ],
          "correctAnswer": 2,
          "explanation": "Cross-account access using IAM roles with AssumeRole is the AWS best practice and most secure approach. The IAM role in Account B has a trust policy specifying Account A as a trusted entity. The application in Account A assumes this role using STS AssumeRole, receiving temporary credentials. This approach: (1) uses temporary credentials (not long-lived access keys), (2) can include ExternalId for additional security, (3) can include conditions like source IP or MFA, (4) provides clear audit trails in CloudTrail. Option A uses long-lived credentials which are less secure and harder to rotate. Option B making the bucket public is a severe security risk even with IP restrictions. Option D using ACLs is deprecated and AWS recommends using bucket policies and IAM policies instead. Additionally, bucket ACLs can't enforce the same granular controls as IAM roles."
        },
        {
          "question": "A company implements AWS SSO (IAM Identity Center) integrated with their corporate Active Directory. They need some users to access AWS with elevated privileges only after MFA verification, while others can access without MFA for read-only operations. How should this be implemented in IAM Identity Center?",
          "options": [
            "Create two permission sets: one requiring MFA at the permission set level for elevated access, and one without MFA for read-only access",
            "Configure MFA at the IAM Identity Center identity source level, making it required for all users, then use attribute-based access control to bypass MFA for read-only users",
            "Create one permission set with conditions using 'aws:MultiFactorAuthPresent' to grant elevated permissions when MFA is used and read-only permissions otherwise",
            "IAM Identity Center enforces MFA at the login level only, so use separate AWS accounts for elevated and read-only access"
          ],
          "correctAnswer": 2,
          "explanation": "The correct approach is to create a single permission set with conditional policies using 'aws:MultiFactorAuthPresent'. The policy grants elevated permissions when 'aws:MultiFactorAuthPresent': 'true' and grants only read-only permissions when this condition is false or when MFA wasn't used. Users can choose to sign in with or without MFA, and their permissions adjust accordingly. This provides flexibility and follows the principle of progressive access. Option A would work but requires users to be assigned to different permission sets based on their intended action, which is less flexible. Option B is incorrect - you can't selectively bypass MFA requirements for certain users if it's enforced at the identity source level. Option D is overly complex and not necessary. The key is understanding that IAM conditions can differentiate permissions based on MFA presence in the same session."
        },
        {
          "question": "An enterprise must ensure that no IAM role in their organization can be created or modified to allow iam:PassRole to 'AdminRole' without security team approval. They want to prevent this across all accounts in their AWS Organization proactively. What is the MOST effective implementation? (Select TWO)",
          "options": [
            "Create an SCP that denies iam:CreateRole and iam:PutRolePolicy if the policy being created contains iam:PassRole for AdminRole",
            "Use AWS Config with a custom rule that detects iam:PassRole permissions for AdminRole and automatically remediates",
            "Implement IAM Access Analyzer custom policy checks in a CI/CD pipeline to validate policies before deployment",
            "Use AWS Control Tower guardrails to prevent creation of policies with iam:PassRole for AdminRole",
            "Enable AWS CloudTrail and create EventBridge rules to detect and alert on iam:PassRole usage",
            "Configure AWS Organizations to require approval workflows for all IAM changes"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            2
          ],
          "explanation": "The most effective preventive controls are: (1) An SCP that denies creating/modifying roles containing iam:PassRole for AdminRole - this provides organization-wide preventive control at the permission boundary level. In September 2025, AWS Organizations added full IAM policy language support for SCPs, including conditions and resource ARNs, making it possible to deny policies based on their content. (2) IAM Access Analyzer custom policy checks in CI/CD pipelines provide proactive validation before deployment, detecting violations early. Options B and E are detective controls (detect after the fact) rather than preventive. Option D (Control Tower guardrails) could work but is less granular than SCPs for this specific use case. Option F doesn't exist - Organizations doesn't have built-in approval workflows. The key is preventing the issue before it happens (preventive) vs detecting it after (detective)."
        },
        {
          "question": "A company uses AWS Secrets Manager for database credentials rotation. Their application runs on ECS Fargate across multiple environments (dev, staging, prod). They want to ensure that the dev environment can only access dev secrets, staging can only access staging secrets, etc. What is the MOST secure and maintainable approach?",
          "options": [
            "Create separate AWS accounts for each environment and store secrets in each account",
            "Use a single Secrets Manager with resource-based policies on each secret restricting access to specific ECS task roles",
            "Tag secrets with environment tags and use IAM policies with conditions checking that the principal tag matches the secret's resource tag",
            "Store secrets in Parameter Store instead and use different AWS KMS keys for each environment with key policies restricting access"
          ],
          "correctAnswer": 2,
          "explanation": "ABAC with tag-based access control (Option C) is the most scalable and maintainable approach. Tag secrets with 'Environment:dev', 'Environment:staging', etc., and tag ECS task roles with the same. The IAM policy uses: 'Condition': {'StringEquals': {'aws:PrincipalTag/Environment': '${aws:ResourceTag/Environment}'}}. This scales automatically - new environments require no policy updates, just consistent tagging. Option A (separate accounts) works but is heavy-weight for environment separation within the same workload. Option B (resource policies on each secret) doesn't scale well and requires updating policies for each new secret or task role. Option D (Parameter Store with different KMS keys) could work but is more complex than necessary and doesn't leverage ABAC's scalability benefits. The tag-based approach also makes it easy to audit and visualize access patterns."
        },
        {
          "question": "A security audit reveals that several IAM policies in the organization grant iam:PassRole with a wildcard (*) in the Resource element. IAM Access Analyzer flags this as a security warning. Why is this a security concern, and what is the recommended remediation?",
          "options": [
            "It's not actually a security risk; IAM Access Analyzer is overly cautious. The Action element restrictions are sufficient",
            "It allows privilege escalation - users could pass highly privileged roles to services like Lambda or EC2, gaining those privileges indirectly. Restrict the Resource to specific role ARNs",
            "It only affects CloudFormation deployments. Remediation is to use service-specific PassRole permissions",
            "The warning is about performance. Wildcards in PassRole slow down IAM evaluation. Use explicit role ARNs for better performance"
          ],
          "correctAnswer": 1,
          "explanation": "iam:PassRole with wildcard resource is a critical security finding because it enables privilege escalation. A user with this permission can pass ANY role (including highly privileged admin roles) to AWS services. For example, they could: (1) Create a Lambda function and pass it an AdminRole, then invoke the function to execute admin actions, or (2) Launch an EC2 instance with an AdminRole, then access the instance to gain admin privileges. This bypasses direct permission grants. The remediation is to explicitly specify which roles can be passed: 'Resource': ['arn:aws:iam::account:role/SpecificAppRole']. This is why IAM Access Analyzer flags it as a SECURITY WARNING (not just a suggestion). Options C and D are incorrect - this is not about CloudFormation or performance. Option A is dangerously wrong - this IS a genuine security risk. The principle: iam:PassRole should be as restrictive as direct permission grants, as it's effectively an indirect way to gain those permissions."
        },
        {
          "question": "An organization with 500+ AWS accounts wants to implement centralized security findings aggregation from AWS Security Hub. They want a security team in a central account to view and manage findings from all accounts. What configuration is required? (Select THREE)",
          "options": [
            "Enable AWS Security Hub in all 500+ accounts",
            "Designate one account as the Security Hub administrator account in AWS Organizations",
            "Create cross-account IAM roles for Security Hub to assume in each member account",
            "Enable auto-enabling for Security Hub to automatically enable it in new accounts",
            "Configure Security Hub finding aggregation across all AWS regions to the administrator account",
            "Use AWS Config aggregators to collect Security Hub findings"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            1,
            3
          ],
          "explanation": "The required configuration includes: (1) Enable Security Hub in all accounts - required for findings generation, (2) Designate a Security Hub administrator account - this central account can view and manage findings from all member accounts in the organization, (3) Enable auto-enabling - ensures new accounts automatically have Security Hub enabled and associated with the administrator account. Option C is not needed - when using AWS Organizations integration, Security Hub uses service-linked roles automatically, not cross-account IAM roles. Option E relates to cross-region aggregation, which is a separate feature and not strictly required for multi-account setup (though useful). Option F is incorrect - AWS Config aggregators are for Config data, not Security Hub. Security Hub has its own built-in aggregation mechanism when you designate an administrator account."
        },
        {
          "question": "A company needs to enforce that all EBS volumes and RDS databases created in their AWS Organization must be encrypted with customer-managed KMS keys (CMKs), not AWS-managed keys. What is the MOST effective enforcement mechanism?",
          "options": [
            "Create an SCP that denies ec2:CreateVolume and rds:CreateDBInstance unless the KMS key ARN matches a specific customer-managed key pattern",
            "Use AWS Config rules that detect unencrypted or AWS-managed-key-encrypted resources and automatically remediate by re-encrypting",
            "Enable default encryption for EBS and RDS in each account, specifying the customer-managed KMS key",
            "Implement AWS Control Tower detective guardrails that alert when resources aren't encrypted with CMKs"
          ],
          "correctAnswer": 0,
          "explanation": "An SCP with a deny statement is the most effective preventive control. The SCP can deny CreateVolume and CreateDBInstance unless the request includes encryption with a customer-managed KMS key. Example condition: 'StringNotEquals': {'ec2:KmsKeyId': 'arn:aws:kms:*:*:key/*'} combined with denying unencrypted volumes. Since September 2025, SCPs support full IAM policy language including conditions and individual resource ARNs, making this highly granular control possible. Option B is detective (detects after creation) not preventive, and re-encrypting RDS requires recreation. Option C sets defaults but doesn't ENFORCE - users can still override. Option D is purely detective/alerting, not enforcement. The key principle: preventive controls (SCPs) are superior to detective controls (Config, Control Tower detective guardrails) for security requirements that must never be violated."
        },
        {
          "question": "A development team uses AWS Certificate Manager (ACM) to manage SSL/TLS certificates. They need some certificates to be exportable for use on on-premises servers, while others remain non-exportable for AWS services. What approach should they use?",
          "options": [
            "Use ACM for all certificates; certificates generated by ACM are exportable by default",
            "Use ACM Private CA for exportable certificates and ACM for non-exportable certificates",
            "Import externally generated certificates into ACM for exportable certificates, use ACM-generated certificates for non-exportable ones",
            "ACM certificates cannot be exported; use AWS Systems Manager Parameter Store for exportable certificate storage"
          ],
          "correctAnswer": 1,
          "explanation": "ACM-generated certificates are not exportable and can only be used with integrated AWS services (ELB, CloudFront, API Gateway). For exportable certificates, you must use ACM Private CA, which issues certificates that can be exported and used anywhere, including on-premises. Option A is incorrect - ACM certificates are NOT exportable. Option C is partially correct (imported certificates can be exported) but creates operational complexity managing external CAs. Option D misunderstands the requirement - Parameter Store is for storing secrets, not for certificate issuance. The distinction is critical: ACM (free) provides certificates locked to AWS services for security; ACM Private CA (paid) provides a private CA that can issue exportable certificates. For a hybrid environment, you'd use both: ACM Private CA for on-premises needs and regular ACM for AWS service integrations."
        },
        {
          "question": "A company implements permission boundaries for all IAM roles created by developers. The permission boundary allows only S3 and DynamoDB actions. A developer creates a role with a policy allowing S3, DynamoDB, and EC2 actions, then tries to launch an EC2 instance using this role. What happens?",
          "options": [
            "The EC2 instance launches successfully because the IAM policy explicitly allows EC2 actions",
            "The EC2 instance launch fails because permission boundaries set the maximum permissions, and EC2 actions are not included in the boundary",
            "The EC2 instance launches but the role cannot perform any actions because permission boundaries override IAM policies completely",
            "The permission boundary is ignored because IAM policies take precedence over boundaries"
          ],
          "correctAnswer": 1,
          "explanation": "Permission boundaries set the maximum permissions that an IAM entity can have. The effective permissions are the intersection of the identity-based policy and the permission boundary. In this case: IAM policy allows S3, DynamoDB, and EC2. Permission boundary allows only S3 and DynamoDB. Effective permissions = intersection = S3 and DynamoDB only. The role can be used to launch an EC2 instance (if the developer has permission to launch instances), but when the instance tries to perform EC2 API actions using this role, those actions will be denied because EC2 actions are not in the intersection. Option A is incorrect - the IAM policy alone doesn't determine permissions. Option C is incorrect - boundaries don't override policies; they intersect with them. Option D is wrong - boundaries are always enforced when present. Permission boundaries are powerful for delegating user/role creation while maintaining security - developers can create roles but cannot grant permissions beyond the boundary."
        },
        {
          "question": "An organization wants to automatically detect when an IAM policy change grants more permissive access than the previous version, preventing accidental privilege escalation. Which AWS service and feature should they use?",
          "options": [
            "AWS Config with managed rule 'iam-policy-no-statements-with-admin-access'",
            "IAM Access Analyzer custom policy checks using the 'Check for new access' feature",
            "AWS CloudTrail Insights to detect unusual IAM API activity patterns",
            "AWS Security Hub with CIS AWS Foundations Benchmark controls"
          ],
          "correctAnswer": 1,
          "explanation": "IAM Access Analyzer custom policy checks with the 'Check for new access' feature is specifically designed for this use case. It uses automated reasoning (provable security based on mathematical logic) to determine whether an updated policy grants new access compared to the existing version. This provides comprehensive findings about what new permissions the updated policy grants. You can integrate this into CI/CD pipelines to prevent deploying more permissive policies. Option A (Config rule) only checks if policies grant admin access, not whether they're more permissive than before. Option C (CloudTrail Insights) detects unusual API activity, not policy permission changes. Option D (Security Hub/CIS) provides security best practice compliance checks but doesn't compare policy versions for new access. The key differentiator: Access Analyzer uses provable security to mathematically prove what access a policy grants, making the comparison definitive, not heuristic-based."
        }
      ]
    },
    {
      "filename": "domain-1-task-1.3-reliable-resilient.json",
      "domain": "Domain 1: Organizational Complexity",
      "task": "Task 1.3: Reliable and Resilient Architectures",
      "question_count": 10,
      "questions": [
        {
          "question": "A global gaming company needs to route UDP traffic for their multiplayer game servers to the optimal AWS region with sub-second failover capabilities. They require static IP addresses for allowlisting by enterprise customers. Players are distributed worldwide. Which solution meets these requirements?",
          "options": [
            "Amazon CloudFront with custom origins pointing to game servers in multiple regions",
            "AWS Global Accelerator with endpoints in multiple regions and health checks configured",
            "Application Load Balancer with cross-zone load balancing in multiple regions",
            "Route 53 latency-based routing with health checks to game server endpoints"
          ],
          "correctAnswer": 1,
          "explanation": "AWS Global Accelerator is the correct choice for this scenario. It provides: (1) Support for non-HTTP protocols including UDP (required for gaming), (2) Two static anycast IP addresses that don't change, making them ideal for allowlisting, (3) Sub-30-second failover to healthy endpoints when health checks fail, (4) Routing through AWS's private global network for lower latency and better performance. CloudFront (Option A) is for HTTP/HTTPS only and caches content - not suitable for real-time game traffic. ALB (Option C) is regional and only supports HTTP/HTTPS/gRPC. Route 53 (Option D) can route UDP but doesn't provide static IPs (DNS returns different IPs) and failover is slower (depends on DNS TTL). Global Accelerator continually monitors endpoint health and redirects traffic to healthy endpoints in less than 30 seconds, making it ideal for high-availability gaming workloads."
        },
        {
          "question": "An e-commerce company operates in US, EU, and Asia regions using Aurora PostgreSQL. They need an RPO of 1 second and RTO of under 5 minutes for regional failures. The application must automatically failover to the nearest healthy region. However, during planned maintenance, they need zero data loss. Which Aurora deployment strategy meets ALL requirements?",
          "options": [
            "Aurora Multi-AZ deployment in each region with Route 53 health checks for cross-region failover",
            "Aurora Global Database with managed planned switchover for maintenance and manual failover for disasters",
            "Aurora Multi-AZ with read replicas promoted manually during regional failures",
            "Aurora Global Database with automated cross-region failover using Route 53 Application Recovery Controller"
          ],
          "correctAnswer": 1,
          "explanation": "Aurora Global Database with both managed planned switchover and manual failover capabilities is the correct answer. Aurora Global Database provides: (1) RPO of ~1 second for unplanned outages (replication lag typically < 1 second), (2) RTO of approximately 1-5 minutes for manual cross-region failover, (3) Most importantly, managed planned switchover provides RPO of 0 (zero data loss) for planned maintenance by synchronizing secondary DB clusters with the primary before failover. Option A (Multi-AZ) only protects against AZ failures, not regional failures. Option C doesn't meet the RPO requirement and requires significant manual effort. Option D doesn't exist - Aurora Global Database doesn't have automated cross-region failover; failover must be manually initiated (though Route 53 ARC can help orchestrate it). The key insight: the question requires BOTH low RPO/RTO for disasters AND zero data loss for planned maintenance - only Aurora Global Database's managed switchover provides RPO=0 for planned events."
        },
        {
          "question": "A financial services company requires cross-region replication for their DynamoDB application with an RPO of 0 (zero data loss) and the ability to read the most recent data from any region after a write. As of 2025, which DynamoDB configuration supports these requirements?",
          "options": [
            "DynamoDB Global Tables with Multi-Region Eventual Consistency (MREC) and strongly consistent reads",
            "DynamoDB Global Tables with Multi-Region Strong Consistency (MRSC) deployed across exactly 3 regions",
            "DynamoDB with DynamoDB Streams and Lambda for custom cross-region replication",
            "DynamoDB Global Tables cannot achieve RPO of 0; the minimum RPO is 1 second with eventual consistency"
          ],
          "correctAnswer": 1,
          "explanation": "As of June 2025, DynamoDB Global Tables supports Multi-Region Strong Consistency (MRSC), which provides: (1) RPO of zero - writes are synchronously replicated to at least one other region before returning success, (2) Strongly consistent reads always return the latest version from any replica. However, MRSC has specific constraints: must be deployed in exactly 3 regions, does not support transaction APIs, and has higher latencies for writes and strongly consistent reads. Option A is incorrect - MREC (default mode) provides asynchronous replication with sub-second RPO but not zero RPO. Option C (custom replication) is overly complex and difficult to achieve true zero RPO. Option D was true before June 2025 but is now incorrect with the introduction of MRSC. Important: You cannot change a global table's consistency mode after creation, so this must be chosen at table creation time."
        },
        {
          "question": "A media streaming company uses S3 Cross-Region Replication (CRR) for disaster recovery. They need to ensure that metadata changes and deletions are replicated, and they want to replicate existing objects that were present before CRR was enabled. What configuration is required? (Select TWO)",
          "options": [
            "Enable Delete Marker Replication in the CRR configuration",
            "Enable S3 Versioning on both source and destination buckets",
            "Use S3 Batch Replication to replicate existing objects",
            "Enable S3 Lifecycle policies to move objects to the destination bucket",
            "Configure S3 Event Notifications to trigger Lambda for object replication",
            "Enable S3 Inventory for tracking replicated objects"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            2
          ],
          "explanation": "The required configurations are: (1) S3 Versioning must be enabled on both source and destination buckets - this is a prerequisite for CRR, and (2) S3 Batch Replication is needed to replicate existing objects because CRR only replicates new objects uploaded after CRR is enabled. For delete markers, you'd also enable Delete Marker Replication (Option A), but the question specifically asks about metadata and existing objects. Option D (Lifecycle policies) doesn't replicate objects; it transitions or expires them. Option E (custom Lambda replication) is unnecessary complexity when CRR is available. Option F (S3 Inventory) is for reporting, not replication. Key points: CRR requires versioning, only replicates NEW objects by default (objects uploaded after enabling CRR), and S3 Batch Replication is the AWS-native solution for replicating existing objects. Note: S3 Replication Time Control (RTC) can provide SLA of 99.99% for replication within 15 minutes."
        },
        {
          "question": "An enterprise is implementing chaos engineering using AWS Fault Injection Simulator (FIS) to test their multi-AZ RDS deployment resilience. They want to simulate an AZ failure but prevent any real impact to production databases. What is the SAFEST approach?",
          "options": [
            "Run FIS experiments directly in production during low-traffic hours with rollback actions configured",
            "Create a production-like staging environment with identical architecture and run FIS experiments there first",
            "Use FIS stop conditions with CloudWatch Alarms to automatically stop the experiment if RTO exceeds 5 minutes",
            "Enable RDS automated backups before running FIS experiments in production"
          ],
          "correctAnswer": 2,
          "explanation": "Using FIS stop conditions with CloudWatch Alarms is the safest approach for production chaos engineering. Stop conditions continuously monitor specified CloudWatch Alarms during the experiment and automatically stop the experiment if the alarm breaches, preventing cascading failures or extended outages. You might configure alarms for: database connection errors exceeding threshold, query latency exceeding SLA, or CPU reaching critical levels. This allows you to safely test in production while having guardrails. Option A (running without stop conditions) risks real production impact. Option B (staging only) is safer but doesn't validate real production behavior, including actual traffic patterns, data volumes, and dependencies. Option D (backups) provides recovery but doesn't prevent the outage. The key principle: chaos engineering in production requires automated stop conditions to limit blast radius. FIS integrates with CloudWatch Alarms, AWS CloudWatch Evidently, and other monitoring tools for this purpose."
        },
        {
          "question": "A SaaS company needs to implement disaster recovery for their application spanning multiple AWS services: EC2 instances, RDS databases, DynamoDB tables, and S3 buckets. They need point-in-time recovery within the last 35 days with automated backup policies. Which AWS service should they use for centralized backup management?",
          "options": [
            "AWS Backup with backup plans defining retention policies and schedules for all supported resources",
            "AWS CloudFormation with backup and restore scripts in Lambda functions",
            "Native service-specific backups (RDS snapshots, DynamoDB backups, S3 versioning) managed separately",
            "AWS Systems Manager Automation Documents to orchestrate backups across services"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Backup is the purpose-built, centralized backup service that supports EC2, EBS, RDS, DynamoDB, EFS, S3, and many other services. Key benefits: (1) Centralized backup policies and scheduling, (2) Cross-region backup copy for disaster recovery, (3) Point-in-time recovery support for supported services, (4) Compliance reporting and backup lifecycle management, (5) Tag-based backup policies allowing automatic backup of resources based on tags. For 35-day retention, you'd create a backup plan with appropriate retention rules. Option B (CloudFormation with Lambda) is overly complex and you'd be building what AWS Backup already provides. Option C (native backups) works but lacks centralized management, cross-service consistency, and unified compliance reporting. Option D (Systems Manager) can orchestrate tasks but isn't designed for comprehensive backup management. AWS Backup also supports AWS Organizations integration, allowing centralized backup policies across accounts."
        },
        {
          "question": "A video processing company uses Route 53 for DNS with health checks on their application endpoints in three regions: us-east-1, eu-west-1, and ap-southeast-1. They want traffic to go to the geographically nearest healthy region, but if all regions fail, they want to serve a static maintenance page from S3. How should they configure this?",
          "options": [
            "Use Route 53 geolocation routing with health checks, and configure evaluate target health on each record set",
            "Use Route 53 latency-based routing with health checks on application endpoints, and create a failover record pointing to S3 as secondary",
            "Configure Route 53 geoproximity routing with health checks and bias settings for each region",
            "Use Route 53 multivalue answer routing returning all healthy endpoints and letting the client choose"
          ],
          "correctAnswer": 1,
          "explanation": "The correct configuration is latency-based routing with a failover record as backup. Here's how: Create latency-based routing records for each region's endpoint with health checks. Then create a Route 53 failover routing policy as the parent, with the latency-based records as PRIMARY and an S3 static website as SECONDARY. When all health checks fail, Route 53 automatically fails over to the S3 maintenance page. Option A (geolocation) routes based on user geographic location but not network latency, which can be suboptimal. Option C (geoproximity) requires manual bias configuration and doesn't inherently route to the lowest latency endpoint. Option D (multivalue) returns multiple IP addresses but leaves the client to choose, not providing true failover to S3. The key: combining routing policies (latency for performance + failover for disaster scenario) provides both optimal performance and graceful degradation."
        },
        {
          "question": "A global financial application requires active-active deployment across two regions with automatic synchronization of user sessions. The application uses Application Load Balancer, ECS Fargate containers, and needs session persistence. Which architecture provides BOTH active-active capability AND session persistence?",
          "options": [
            "Use ALB sticky sessions with DynamoDB Global Tables (MREC) for session storage, replicated across both regions",
            "Store sessions in ElastiCache Redis with Redis Global Datastore for cross-region replication",
            "Use ALB sticky sessions at the cookie level; sessions remain in the region where they originated",
            "Store sessions in Aurora Global Database with write forwarding enabled from both regions"
          ],
          "correctAnswer": 1,
          "explanation": "ElastiCache for Redis with Redis Global Datastore is the optimal solution for active-active session management. Redis Global Datastore provides: (1) Cross-region replication with sub-second replication latency, (2) Active-active topology where both regions can serve reads and writes, (3) Automatic failover and promotion, (4) Low-latency session access from containers in both regions. Users can be served from either region and their sessions remain available. Option A (DynamoDB Global Tables) works but has higher latency than ElastiCache for session access. Option C (ALB sticky sessions alone) doesn't sync sessions across regions - if a region fails, users lose sessions. Option D (Aurora with write forwarding) works but is overengineered for session storage and has higher latency than in-memory Redis. For active-active deployments, sessions must be replicated in near-real-time, making Redis Global Datastore ideal with its sub-second replication and in-memory performance."
        },
        {
          "question": "An IoT company processes sensor data through Amazon Kinesis Data Streams, which feeds into multiple Lambda functions for processing. They need to ensure that if Lambda processing fails, data is not lost and can be reprocessed. What configuration ensures maximum reliability?",
          "options": [
            "Increase Kinesis stream retention period to 365 days and configure Lambda retry attempts to 0",
            "Configure Lambda with an on-failure destination pointing to an SQS Dead Letter Queue (DLQ), and set maximum retry attempts to 2",
            "Enable Kinesis Data Streams Enhanced Fan-Out and configure Lambda event source mapping with bisect on function error and maximum record age",
            "Use Kinesis Data Firehose instead of Lambda for processing to ensure reliable delivery"
          ],
          "correctAnswer": 2,
          "explanation": "The correct configuration uses Lambda event source mapping advanced features: (1) Bisect on function error - when Lambda fails to process a batch, Kinesis splits the batch in half and retries each half separately, isolating the problematic record(s), (2) Maximum record age - prevents repeatedly processing very old records that might be causing issues, (3) Enhanced Fan-Out - provides dedicated throughput for each consumer, preventing slow processing from affecting others. Additionally, you should configure on-failure destination to capture records that exceed retry attempts. Option A with 0 retries means failures immediately lose data. Option B's configuration is incomplete without bisect batch on error for stream-based sources. Option D (Kinesis Firehose) is for delivery to destinations like S3/Redshift, not custom Lambda processing. The key: stream-based event sources (Kinesis, DynamoDB Streams) have different failure handling than queue-based sources; bisect on error is critical for isolating bad records while continuing to process good ones."
        },
        {
          "question": "A company wants to implement automated DR testing for their multi-tier application without impacting production. They use Infrastructure as Code (CloudFormation) and want to validate that they can recover from region failure within their 4-hour RTO. What is the MOST effective testing strategy?",
          "options": [
            "Manually promote RDS read replicas and fail over to the DR region once per quarter",
            "Use AWS Backup to restore resources in the DR region and test application functionality, then delete the DR resources",
            "Implement continuous DR environment in the DR region, periodically swap production traffic using Route 53 weighted routing (20% DR, 80% primary) for validation",
            "Create a scheduled Systems Manager Automation Document that deploys the full stack in DR region, runs synthetic tests, validates RTO, and tears down resources"
          ],
          "correctAnswer": 3,
          "explanation": "Option D provides true automated DR testing without production impact. Systems Manager Automation can: (1) Deploy the full CloudFormation stack in DR region, (2) Restore latest data from backups or replicas, (3) Execute synthetic transaction tests to validate functionality, (4) Measure and validate that RTO is met, (5) Automatically tear down test resources to minimize cost. This can run on a schedule (monthly/quarterly) ensuring DR readiness. Option A (manual quarterly) doesn't validate RTO effectively and is error-prone. Option B (AWS Backup restore) is partially correct but lacks automation and RTO validation. Option C (continuous DR with traffic splitting) incurs high costs running duplicate infrastructure continuously and risks production impact. The key: DR testing should be automated, scheduled, validate both data and application integrity, measure RTO/RPO, and not impact production. Systems Manager Automation integrated with CloudFormation, AWS Backup, and CloudWatch makes this achievable."
        }
      ]
    },
    {
      "filename": "domain-1-task-1.4-multi-account.json",
      "domain": "Domain 1: Organizational Complexity",
      "task": "Task 1.4: Multi-Account Environment",
      "question_count": 10,
      "questions": [
        {
          "question": "A company has an AWS Organization with 200+ accounts structured in OUs: Production, Development, Sandbox, and Security. They want to deploy AWS Config rules across all accounts to ensure compliance, with the rules automatically deployed to new accounts created in the future. The deployment should happen from a central compliance account. What is the MOST operationally efficient approach?",
          "options": [
            "Create AWS Config rules in each account manually and use AWS Systems Manager to maintain consistency",
            "Use CloudFormation StackSets with service-managed permissions targeting OUs, enable automatic deployments for new accounts",
            "Deploy AWS Config Aggregator in the central account and configure each member account to push data",
            "Use AWS Control Tower guardrails to deploy Config rules across all enrolled accounts"
          ],
          "correctAnswer": 1,
          "explanation": "CloudFormation StackSets with service-managed permissions is the optimal solution. When integrated with AWS Organizations, StackSets: (1) Uses service-managed permissions (no need to create IAM roles manually), (2) Can target entire OUs, deploying to all current accounts in those OUs, (3) Supports automatic deployments - when enabled, new accounts added to targeted OUs automatically receive the stack instances, (4) Centralized management from a single account. The compliance team can create one StackSet containing Config rules and target all OUs. Option A is not scalable for 200+ accounts. Option C (Config Aggregator) aggregates compliance data but doesn't deploy Config rules. Option D (Control Tower) could work if they're using Control Tower, but StackSets provides more flexibility and granular control for this specific use case. StackSets allow parallel deployment control and failure tolerance configuration."
        },
        {
          "question": "An enterprise wants to automate AWS account provisioning for development teams. New accounts should be created with baseline security controls (CloudTrail, GuardDuty, Security Hub), networking (VPC, subnets), and custom applications deployed. The solution should integrate with their existing Terraform workflow. Which approach best meets these requirements?",
          "options": [
            "Use AWS Control Tower Account Factory with Account Factory Customization (AFC) blueprints",
            "Create custom Lambda functions triggered by AWS Service Catalog to provision accounts and resources",
            "Implement AWS Control Tower Account Factory for Terraform (AFT) with account customization pipelines",
            "Use AWS Organizations CreateAccount API with CloudFormation StackSets for resource deployment"
          ],
          "correctAnswer": 2,
          "explanation": "Account Factory for Terraform (AFT) is specifically designed for this use case. AFT: (1) Integrates AWS Control Tower's governance with Terraform workflows, (2) Automates account lifecycle management with Terraform-based customizations, (3) Supports account-specific customizations through Git-based pipelines, (4) Handles baseline Control Tower configurations plus custom resources, (5) Enables teams to use familiar Terraform syntax. AFT creates a pipeline that automatically provisions Control Tower accounts and applies Terraform-defined customizations including VPCs, security controls, and applications. Option A (AFC with CloudFormation) works but doesn't integrate with existing Terraform workflow. Option B (custom Lambda) requires building and maintaining complex orchestration. Option D (Organizations API + StackSets) doesn't provide the integrated account vending and customization pipeline that AFT offers. AFT v1.15.0 (2025) added enhanced configuration options."
        },
        {
          "question": "A company has 500 accounts in AWS Organizations. They need to enforce that all accounts must have CloudTrail logging to a central S3 bucket in the security account, with log file validation enabled. An SCP is in place preventing CloudTrail modification. However, 50 older accounts don't have CloudTrail configured yet. What is the MOST efficient remediation?",
          "options": [
            "Manually create organization trail from the management account, which automatically applies to all accounts",
            "Use CloudFormation StackSets with service-managed permissions to deploy CloudTrail in all accounts, with the SCP ensuring configurations can't be modified",
            "Create an AWS Config rule to detect accounts without CloudTrail and use Systems Manager Automation for remediation",
            "Use AWS Control Tower to enable CloudTrail baseline on all OUs, automatically enrolling accounts"
          ],
          "correctAnswer": 0,
          "explanation": "Creating an organization trail from the management account is the most efficient solution. An organization trail: (1) Automatically applies to all accounts in the organization, including existing and future accounts, (2) Logs all events from all accounts to a centralized S3 bucket, (3) Cannot be disabled or modified by member accounts (automatically enforced, complementing the SCP), (4) Requires only one trail creation operation. This immediately brings all 500 accounts (including the 50 without CloudTrail) into compliance with a single action. Option B (StackSets) would work but requires 500 stack instances when one organization trail suffices. Option C (Config + remediation) is more complex and slower. Option D (Control Tower) requires all accounts to be enrolled in Control Tower, which may not be the case. Important: Organization trails are created in the management account and automatically replicate to all member accounts, making them ideal for centralized compliance requirements."
        },
        {
          "question": "A financial services company has accounts in AWS Organizations with an OU structure: Root  Production OU  AppTeam-A OU. There are SCPs at each level. The Root OU has an SCP allowing all services. Production OU has an SCP denying s3:DeleteBucket. AppTeam-A OU has an SCP allowing only S3 and EC2 services. What are the effective permissions for accounts in AppTeam-A OU?",
          "options": [
            "S3 and EC2 services are allowed because the AppTeam-A SCP explicitly allows them, overriding parent SCPs",
            "S3 (except DeleteBucket) and EC2 services only, as SCPs are inherited and intersected down the OU hierarchy",
            "All services are allowed because the Root SCP allows all services and takes precedence",
            "No services are allowed because the deny in Production OU propagates and blocks all S3 access"
          ],
          "correctAnswer": 1,
          "explanation": "SCPs are inherited and the effective permissions are the intersection (logical AND) of all SCPs from the root to the account. For AppTeam-A accounts: Root SCP allows all services (no restriction). Production OU SCP denies s3:DeleteBucket. AppTeam-A OU SCP allows only S3 and EC2. The intersection is: S3 (minus DeleteBucket) and EC2 only. The AppTeam-A SCP's restriction to S3 and EC2 is the most restrictive, so other services are blocked. Within S3, the Production OU's deny on DeleteBucket further restricts permissions. Important SCP principles: (1) SCPs never grant permissions, they only filter/restrict, (2) Explicit denies cannot be overridden, (3) The most restrictive combination applies, (4) SCPs affect all users and roles in the account, including the root user. Even if an IAM policy grants DynamoDB access, accounts in AppTeam-A cannot use DynamoDB due to the SCP restriction."
        },
        {
          "question": "An organization needs to share a private Application Load Balancer in a central networking account with application accounts. Application teams should be able to register their targets with the ALB but NOT modify the ALB configuration or security groups. Which AWS service enables this capability?",
          "options": [
            "AWS Resource Access Manager (RAM) sharing the ALB with target account",
            "VPC Peering between the networking account and application accounts",
            "AWS PrivateLink to expose the ALB to other accounts",
            "Cross-account IAM roles allowing application accounts to access the ALB"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Resource Access Manager (RAM) supports sharing Application Load Balancers (and Network Load Balancers) across accounts. When you share an ALB: (1) Participant accounts can register their resources (EC2, ECS, Lambda) as targets, (2) The owner account maintains control over ALB configuration, listeners, and security groups, (3) Participants cannot modify ALB settings or delete the ALB. This provides the exact separation of concerns required: central team manages infrastructure (ALB, networking), application teams manage their targets. RAM supports sharing within AWS Organizations or with specific accounts. Option B (VPC Peering) provides network connectivity but doesn't address the permission boundary for ALB management. Option C (PrivateLink) is for exposing services, not for sharing infrastructure resources. Option D (IAM roles) could theoretically work but would require complex policies and doesn't provide the clean separation that RAM offers."
        },
        {
          "question": "A company wants to implement cross-account CloudWatch log aggregation from 100 application accounts to a central logging account for compliance and analysis. The solution should minimize configuration in each application account and automatically include new accounts. What architecture should they implement?",
          "options": [
            "Create CloudWatch log groups in each account with subscription filters sending to Kinesis in the central account",
            "Use CloudFormation StackSets to deploy CloudWatch log groups with cross-account permissions to the central account",
            "Configure CloudWatch Logs cross-account data sharing with a centralized log data account, using CloudFormation StackSets for automation",
            "Use AWS Organizations integration with CloudWatch Logs to automatically stream logs from all accounts to a central destination"
          ],
          "correctAnswer": 2,
          "explanation": "CloudWatch Logs cross-account data sharing allows a centralized logging account to access log data from multiple accounts. The architecture: (1) Create a destination in the central logging account that specifies an access policy allowing source accounts, (2) Use CloudFormation StackSets with service-managed permissions and automatic deployments to create subscription filters in all accounts (current and future), (3) Subscription filters in each account send logs to the central destination (typically Kinesis Data Streams or Kinesis Data Firehose). Option A is partially correct but doesn't mention the automation aspect via StackSets. Option D doesn't exist - Organizations doesn't have native CloudWatch Logs integration like it does for CloudTrail. Option B is incomplete - just creating log groups doesn't aggregate logs. The key is combining: (1) CloudWatch Logs destination with cross-account access policy, (2) StackSets for automated subscription filter deployment, (3) Service-managed permissions and auto-deployment for new accounts."
        },
        {
          "question": "An enterprise has implemented AWS Control Tower with automatic account enrollment enabled on production OUs. When they move an account from Development OU to Production OU, what happens to the account's baseline controls and resources?",
          "options": [
            "Nothing changes; baseline controls remain from the Development OU until manually updated",
            "AWS Control Tower automatically applies the Production OU's enabled baseline resources and controls to the account",
            "The account must be de-enrolled and re-enrolled to receive Production OU controls",
            "CloudFormation StackSets must be manually updated to change the account's baseline"
          ],
          "correctAnswer": 1,
          "explanation": "As of 2025, AWS Control Tower supports automatic account enrollment and baseline updates when accounts move between OUs. When you move an account to a new OU: (1) Control Tower automatically applies the destination OU's enabled baseline resources and controls, (2) Removes or updates controls that differ from the previous OU, (3) This happens automatically without manual intervention. This feature significantly improves operational efficiency for large organizations reorganizing their account structure. Option A describes pre-2025 behavior. Option C was required in older versions but is no longer necessary. Option D misunderstands the relationship - Control Tower manages baselines, not StackSets directly (though it may use StackSets internally). Important: This requires that the AWSControlTowerBaseline is enabled on the destination OU, which enables automatic enrollment and baseline management."
        },
        {
          "question": "A company needs to implement consolidated billing with detailed cost allocation across 200 accounts organized by business units (BUs). Each BU has multiple projects. They want to enforce mandatory cost allocation tags and generate monthly cost reports by BU and project. Which implementation provides the MOST comprehensive solution? (Select TWO)",
          "options": [
            "Enable AWS Organizations consolidated billing and create a cost allocation tag policy requiring 'BusinessUnit' and 'Project' tags",
            "Activate cost allocation tags in the management account and ensure they're activated in all member accounts",
            "Use CloudFormation StackSets to deploy AWS Budgets in each account with alerts for cost overruns",
            "Create AWS Cost and Usage Reports (CUR) with hourly granularity and deliver to S3 with cost allocation tags",
            "Implement SCPs denying resource creation without required cost allocation tags",
            "Use AWS Cost Explorer with saved reports filtered by cost allocation tags"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            3
          ],
          "explanation": "The comprehensive solution requires: (1) Tag policy in AWS Organizations to enforce mandatory tags - tag policies define required tags and allowed values across the organization. When applied, resources must have specified tags (BusinessUnit, Project) to be compliant. (2) Cost and Usage Reports (CUR) with cost allocation tags provides detailed billing data with tags for analysis. CUR can be ingested into Athena, QuickSight, or third-party tools for sophisticated cost reporting by BU and project. Option B (activating tags) is a prerequisite but insufficient alone - doesn't enforce tagging. Option C (Budgets) helps with cost control but doesn't solve cost allocation reporting. Option E (SCP to enforce tags) sounds good but SCPs cannot currently enforce tagging at resource creation (they can deny if conditions aren't met, but tag policies are the proper mechanism for tag governance). Option F (Cost Explorer) is useful for visualization but doesn't provide the enforcement or detailed reporting that tag policies + CUR provide."
        },
        {
          "question": "A security team needs to create a cross-account CI/CD pipeline where CodePipeline in Account A deploys to Account B. The pipeline includes CodeBuild, CodeDeploy, and deployment to S3 and ECS in Account B. What is the MINIMUM set of cross-account configurations required?",
          "options": [
            "Create an IAM role in Account B that CodePipeline can assume, granting permissions to S3, ECS, and CodeDeploy",
            "Configure S3 bucket policy in Account B allowing Account A access, create IAM role in Account B for CodePipeline with S3, ECS, and CodeDeploy permissions, and configure KMS key policy for cross-account encryption if using encrypted artifacts",
            "Use AWS Organizations to enable cross-account service access between accounts",
            "Create identical IAM users in both accounts with same access keys for authentication"
          ],
          "correctAnswer": 1,
          "explanation": "Cross-account CI/CD requires multiple components: (1) S3 bucket policy in Account B allowing Account A to access artifact buckets (CodePipeline stores artifacts in S3), (2) IAM role in Account B with trust policy allowing Account A's CodePipeline to assume it, with permissions for S3, ECS, and CodeDeploy operations, (3) If using encryption (common practice), KMS key policy in Account B allowing Account A to decrypt/encrypt with the key used for artifacts. Additionally, the CodePipeline service role in Account A needs permission to assume the role in Account B. Option A is incomplete - missing S3 bucket policy and KMS configuration. Option C (Organizations integration) doesn't automatically configure the specific permissions needed. Option D (shared IAM users) violates AWS best practices - never share credentials; use role assumption. The KMS component is frequently overlooked but critical - encrypted S3 artifacts require cross-account KMS key permissions, otherwise pipeline fails during artifact handling."
        },
        {
          "question": "An organization wants to use AWS Organizations to manage 500+ accounts but is concerned about the impact of a compromised management account. Which security best practices should they implement to minimize risk? (Select THREE)",
          "options": [
            "Enable MFA delete on the S3 bucket storing CloudTrail logs from the management account",
            "Delegate administrative capabilities to member accounts using delegated administrator feature for Security Hub, GuardDuty, and other services",
            "Limit access to the management account to only a few administrators with strong MFA requirements",
            "Store all application workloads in the management account since it has the highest privileges",
            "Implement SCPs that prevent the management account from accessing member account resources",
            "Never use the management account for workloads; use it only for organization management and billing"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            2,
            5
          ],
          "explanation": "Management account security best practices include: (1) Delegate administrative functions - AWS allows delegating administrator privileges for services like Security Hub, GuardDuty, CloudFormation StackSets, reducing the need to access the management account, (2) Strictly limit access - only essential personnel should have management account access, with strong MFA and potentially hardware MFA devices, (3) Never run workloads in management account - it should be used exclusively for organizational management, billing, and security functions. Workloads increase attack surface. Option A (MFA delete on S3) is a good practice but not specific to management account protection. Option D directly violates best practices. Option E is incorrect - SCPs don't apply to the management account itself; the management account is exempt from SCPs, which is why protecting it is critical. Additional practices: use AWS Control Tower for guardrails, implement break-glass procedures, monitor with CloudTrail, and use separate accounts for different administrative functions."
        }
      ]
    },
    {
      "filename": "domain-1-task-1.5-cost-optimization.json",
      "domain": "Domain 1: Organizational Complexity",
      "task": "Task 1.5: Cost Optimization and Visibility",
      "question_count": 8,
      "questions": [
        {
          "question": "A company has variable compute workloads that run on a mix of EC2 instance families (c5, m5, r5) across multiple regions and also uses AWS Fargate and Lambda. They want to commit to steady-state usage to reduce costs but need maximum flexibility as their architecture evolves. They currently spend $100/hour on compute. Which commitment strategy provides the BEST balance of savings and flexibility for 2025?",
          "options": [
            "Purchase Standard Reserved Instances for the most-used instance type in each region (72% discount)",
            "Purchase Compute Savings Plans with a $70/hour commitment (up to 66% discount with full flexibility)",
            "Purchase EC2 Instance Savings Plans for each instance family separately",
            "Purchase Convertible Reserved Instances to allow instance type changes"
          ],
          "correctAnswer": 1,
          "explanation": "Compute Savings Plans is the optimal choice for 2025. It provides: (1) Up to 66% discount (comparable to Convertible RIs), (2) Flexibility across instance families, sizes, operating systems, and regions, (3) Coverage for EC2, Fargate, and Lambda without separate commitments, (4) Automatic discount application as workloads shift between services. With $100/hour spend, a $70/hour commitment covers steady-state usage while maintaining flexibility for the remaining $30/hour. Option A (Standard RIs) provides slightly higher discount (72% vs 66%) but locks to specific instance types/regions - risky given their evolving architecture and only ~3% additional savings. Option C (EC2 Instance Savings Plans) provides more savings than Compute but less flexibility - requires separate commitments per family. Option D (Convertible RIs) offers flexibility but is legacy; AWS recommends Savings Plans. 2025 guidance: Savings Plans for almost all scenarios due to flexibility and automatic application across services."
        },
        {
          "question": "A financial services company with 200 AWS accounts needs to implement cost allocation and chargeback to business units. Each business unit has multiple projects and environments (dev, staging, prod). They want to enforce tagging compliance and generate monthly cost reports by business unit and project. Which combination of services and configurations is MOST effective? (Select THREE)",
          "options": [
            "Create and activate cost allocation tags for 'BusinessUnit', 'Project', and 'Environment' in the management account",
            "Implement AWS Organizations tag policies requiring mandatory tags on all resources",
            "Enable AWS Cost and Usage Reports (CUR) with resource-level granularity and deliver to S3 for analysis",
            "Use AWS Budgets to enforce spending limits per business unit",
            "Deploy AWS Cost Categories to group costs by business unit and project combinations",
            "Use AWS Cost Explorer saved reports filtered by cost allocation tags"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            2,
            4
          ],
          "explanation": "The comprehensive solution requires: (1) Tag policies in AWS Organizations enforce mandatory tags organization-wide, ensuring compliance by preventing non-compliant resource creation or flagging violations, (2) CUR provides detailed cost data with resource-level tags that can be analyzed using Athena/QuickSight for detailed chargeback reports, (3) Cost Categories allow grouping costs into logical categories based on tag combinations (e.g., 'BU-Engineering-Production'), simplifying reporting and budgeting. Option A (activating tags) is necessary but doesn't enforce compliance. Option D (Budgets) helps control costs but doesn't solve allocation/reporting. Option F (Cost Explorer) is useful but doesn't provide the detailed, automated reporting that CUR + Cost Categories + tag policies provide. Best practice: Tag policies (enforcement) + Cost Categories (grouping) + CUR (detailed reporting) creates a comprehensive cost allocation system. Cost Categories can use rules like: IF tag:BusinessUnit = 'Engineering' AND tag:Project = 'DataPlatform' THEN 'Engineering-DataPlatform'."
        },
        {
          "question": "A company analyzes their AWS Cost and Usage Report and discovers they're spending $50,000/month on data transfer charges. After investigation, they find: 40% is cross-AZ data transfer within the same region, 35% is cross-region replication, and 25% is data transfer to the internet. Which optimization strategy would provide the GREATEST cost reduction?",
          "options": [
            "Implement VPC endpoints for AWS services to eliminate data transfer charges",
            "Redesign the architecture to minimize cross-AZ traffic by placing interdependent services in the same AZ with proper backup strategies",
            "Use CloudFront for content delivery to reduce data transfer to internet charges",
            "Consolidate all workloads in a single region to eliminate cross-region transfer costs"
          ],
          "correctAnswer": 1,
          "explanation": "Minimizing cross-AZ traffic addresses the largest cost component (40% = $20,000/month). AWS charges for data transferred between AZs (typically $0.01/GB each direction). Strategies: (1) Place tightly coupled services in the same AZ, (2) Use VPC endpoints to avoid cross-AZ traffic for AWS service calls, (3) Implement caching to reduce database queries across AZs. However, maintain Multi-AZ for stateful services (RDS, etc.) for resilience. Option C (CloudFront) only addresses 25% of costs and may not be applicable to all workloads. Option D (single region) reduces 35% but eliminates disaster recovery capabilities - unacceptable for most enterprises. Option A (VPC endpoints) helps but typically has smaller impact than architectural redesign. Important: Cross-AZ data transfer within a region is often overlooked but accumulates significantly in high-throughput applications. Balance cost optimization with resilience requirements."
        },
        {
          "question": "An organization enabled AWS Cost Anomaly Detection in July 2025 with the latest model enhancements. They receive anomaly alerts for a $5,000 spike in EC2 costs, but investigation shows this is due to their planned quarterly load testing that occurs every 3 months. How should they configure Cost Anomaly Detection to reduce false positives for known recurring events?",
          "options": [
            "Disable Cost Anomaly Detection during planned load testing periods",
            "Rely on the ML model's automatic learning - the 2025 enhancements distinguish between one-time and recurrent cost events over time",
            "Create separate monitors with higher alert thresholds for services used in load testing",
            "Configure suppression rules in the AWS User Notifications integration to filter alerts during testing windows"
          ],
          "correctAnswer": 1,
          "explanation": "The July 2025 AWS Cost Anomaly Detection model enhancements specifically address this scenario. The improved ML model: (1) Better understands organization's typical spend patterns, (2) Distinguishes between one-time and recurrent cost events, (3) Maintains accuracy in detecting cost changes that require attention. After several occurrences, the model should learn the quarterly pattern and reduce false positives. However, for immediate optimization, Option D (suppression rules via AWS User Notifications, introduced May 2025) provides a complementary approach. Option A (disabling) removes protection during testing. Option C (higher thresholds) reduces sensitivity for all spikes, not just planned ones. Best practice: Let the ML model learn patterns while using AWS User Notifications for advanced filtering. The integration with AWS User Notifications (available since May 2025) enables sophisticated alert management with verified contact management and reusable alert configurations."
        },
        {
          "question": "A media company has 500 TB of data in S3 Standard storage. Analysis shows: 60% of data is accessed frequently in the first 30 days, then rarely accessed. 30% has unpredictable access patterns. 10% is archival data not accessed for 90+ days. Which S3 storage optimization strategy minimizes costs while maintaining access requirements?",
          "options": [
            "Move all data to S3 Glacier Flexible Retrieval after 30 days to minimize storage costs",
            "Use S3 Lifecycle policies: transition to S3 Standard-IA after 30 days, then to S3 Glacier Flexible Retrieval after 90 days",
            "Enable S3 Intelligent-Tiering for the entire dataset to automatically optimize storage classes based on access patterns",
            "Use S3 Lifecycle policy for the 60% (Standard  Standard-IA after 30 days), S3 Intelligent-Tiering for the 30% with unpredictable patterns, and move 10% archival to Glacier Deep Archive"
          ],
          "correctAnswer": 3,
          "explanation": "The optimal strategy uses different approaches for different access patterns: (1) For the 60% with predictable patterns, use Lifecycle policies (Standard  Standard-IA after 30 days) - this is cost-effective for known patterns, (2) For the 30% with unpredictable access, S3 Intelligent-Tiering automatically moves data between tiers based on actual usage without retrieval fees, preventing costly Glacier retrievals for occasionally accessed data, (3) For 10% archival, Glacier Deep Archive offers lowest storage cost ($0.00099/GB vs $0.004/GB for Glacier Flexible). Option A over-archives data, causing expensive retrieval fees for the 30% with unpredictable access. Option B uses one-size-fits-all approach, suboptimal for varied patterns. Option C (all Intelligent-Tiering) incurs monitoring fees ($0.0025/1000 objects) unnecessarily for predictable patterns. Key insight: S3 Intelligent-Tiering is ideal for unpredictable access patterns (no retrieval fees between frequent/infrequent tiers), while Lifecycle policies are more cost-effective for predictable patterns."
        },
        {
          "question": "An enterprise wants to optimize EC2 costs and receives AWS Compute Optimizer recommendations suggesting: downsize 30% of instances (oversized), change instance families for 40% (better price-performance), and no changes for 30%. However, the application teams are hesitant due to concerns about performance impact. What is the BEST approach to safely implement these recommendations?",
          "options": [
            "Implement all Compute Optimizer recommendations immediately during a maintenance window to maximize savings",
            "Start with instances in development/staging environments, validate performance, then gradually roll out to production with monitoring",
            "Only implement the downsizing recommendations (30%) and ignore the instance family changes due to higher risk",
            "Reject all recommendations because application teams understand their workloads better than automated tools"
          ],
          "correctAnswer": 1,
          "explanation": "The safe, phased approach is: (1) Test in lower environments first (dev/staging) to validate Compute Optimizer recommendations without production risk, (2) Monitor performance metrics (CPU, memory, disk, network) to confirm the changes don't degrade performance, (3) Gradually roll out to production, starting with less critical workloads, (4) Implement CloudWatch alarms and dashboards to detect issues early. Compute Optimizer uses ML analysis of actual utilization metrics (CloudWatch data) and can identify optimization opportunities, but validation is prudent. Option A (immediate implementation) risks production outages if recommendations don't account for periodic load spikes or application-specific behaviors. Option C ignores potentially significant savings from family changes. Option D wastes optimization opportunities - Compute Optimizer analyzes actual metrics over 14+ days, often identifying waste humans miss. Best practice: Compute Optimizer provides data-driven recommendations, but implement with testing, monitoring, and gradual rollout for production safety."
        },
        {
          "question": "A company using AWS Organizations wants to implement Reserved Instance (RI) and Savings Plan sharing across accounts to maximize utilization. They have production accounts (high priority) and development accounts (low priority). How should they configure sharing to ensure production workloads receive commitment benefits first?",
          "options": [
            "Purchase all RIs and Savings Plans in the management account; AWS automatically prioritizes based on account creation order",
            "Disable RI/Savings Plan sharing and purchase separately in each account to ensure allocation",
            "Enable RI/Savings Plan sharing (default for Organizations); AWS automatically shares across the organization with the purchasing account having first priority",
            "RI/Savings Plan sharing cannot be controlled by priority; they distribute equally across all accounts"
          ],
          "correctAnswer": 2,
          "explanation": "RI and Savings Plan sharing in AWS Organizations works as follows: (1) When sharing is enabled (default), discounts are first applied to the purchasing account, (2) Remaining unused discounts automatically share to other accounts in the organization, (3) Sharing can be disabled if complete isolation is needed, but this reduces utilization efficiency. For the scenario: purchasing RIs/Savings Plans in production accounts ensures production gets first priority for those commitments. Remaining unused discounts (e.g., during low production load) automatically benefit development accounts, maximizing utilization. Option A is incorrect - there's no automatic prioritization by account creation order. Option B (disabling sharing) prevents efficient utilization of unused commitments. Option D is wrong - the purchasing account always has first priority. Best practice: Purchase commitments in high-priority accounts, enable sharing organization-wide to maximize utilization. Advanced: you can disable sharing for specific accounts if strict isolation is required, but this typically reduces overall efficiency and increases costs."
        },
        {
          "question": "A global company analyzes their AWS bill and discovers that 20% of costs are from inter-region data transfer. They use S3 Cross-Region Replication (CRR) for compliance and CloudFront for content delivery with multiple regional origins. Which optimization provides GREATEST data transfer cost reduction while maintaining functionality?",
          "options": [
            "Replace S3 CRR with S3 Same-Region Replication to eliminate cross-region transfer costs",
            "Consolidate CloudFront to use a single origin region and leverage CloudFront's edge locations for global distribution",
            "Enable S3 Transfer Acceleration for cross-region transfers to reduce costs",
            "Use AWS Direct Connect with Direct Connect Gateway to route inter-region traffic through private connectivity"
          ],
          "correctAnswer": 1,
          "explanation": "Consolidating CloudFront to use a single origin region is the optimal cost optimization. Here's why: (1) CloudFront caches content at edge locations worldwide, so most requests are served from edge cache, not the origin, (2) When edge fetch from origin is needed, CloudFront-to-S3 data transfer in the same region is free within AWS, (3) CloudFront origin fetches use AWS's private network efficiently. Multiple regional origins increase cross-region traffic unnecessarily. Option A (SRR) violates the compliance requirement for cross-region replication. Option C (S3 Transfer Acceleration) actually increases costs - it's designed for faster uploads, not cost savings, and adds fees. Option D (Direct Connect) doesn't reduce inter-region data transfer costs; it's for on-premises connectivity. Important: Data transfer FROM CloudFront to users is charged at CloudFront rates (often lower than EC2/S3 rates), and CloudFront-to-S3 in same region is free for origin fetches. Best practice: Use CloudFront with single origin region for cost-effective global distribution."
        }
      ]
    },
    {
      "filename": "domain-2-all-remaining.json",
      "domain": "Domain 2: Design for New Solutions",
      "task": "Tasks 2.2, 2.3, 2.4",
      "taskKey": "task_2.2_business_continuity",
      "question_count": 14,
      "questions": [
        {
          "id": "D2-T2.2-Q1",
          "question": "A company needs to migrate a 50TB Oracle database to Amazon Aurora PostgreSQL with minimal downtime (< 1 hour). The database experiences 10,000 transactions per hour. Which migration approach minimizes downtime while ensuring data consistency?",
          "options": [
            "Use AWS DMS with full load followed by CDC (Change Data Capture) for ongoing replication, then cutover during low-traffic window",
            "Export Oracle database to S3, then import into Aurora PostgreSQL using native tools",
            "Use AWS SCT to convert schema, then DMS Serverless for automated capacity scaling during migration",
            "Create Oracle read replica, convert it using AWS SCT, then promote to Aurora PostgreSQL"
          ],
          "correctAnswer": 2,
          "explanation": "For heterogeneous migrations (Oracle to PostgreSQL), AWS recommends a two-step process: (1) AWS SCT (Schema Conversion Tool) to convert schema and code to match Aurora PostgreSQL, (2) AWS DMS for data migration. DMS Serverless (released in 2023, with 2025 enhancements for premigration assessments and automatic storage scaling) automatically provisions, monitors, and scales migration resources to optimal capacity, removing manual instance sizing. For minimal downtime with ongoing transactions: Use DMS full load + CDC to capture changes during migration, then perform cutover. Option C specifying DMS Serverless is optimal as it handles capacity automatically for the 50TB dataset and 10K TPS workload, with April 2025 automatic storage scaling eliminating the previous 100GB limit. Option A is correct approach but doesn't leverage Serverless automation. Option B would require significant downtime. Option D doesn't work - you can't directly convert Oracle replica to PostgreSQL."
        },
        {
          "id": "D2-T2.2-Q2",
          "question": "On October 30, 2025, AWS Backup announced a new capability for database snapshots. A company wants to copy their RDS snapshots from us-east-1 to both eu-west-1 and ap-southeast-1 for disaster recovery. What is the MOST operationally efficient method available as of late 2025?",
          "options": [
            "Create two sequential copy actions: us-east-1  eu-west-1, then us-east-1  ap-southeast-1",
            "Use AWS Backup to copy database snapshots to multiple AWS Regions in a single copy action",
            "Use Lambda function triggered by RDS snapshot completion to copy to multiple regions",
            "Create manual snapshots and use AWS CLI to copy to each region sequentially"
          ],
          "correctAnswer": 1,
          "explanation": "In October 2025, AWS Backup added support for copying database snapshots (RDS, Aurora, Neptune, DocumentDB) across AWS Regions and accounts using a single copy action, eliminating the need for sequential copying steps. This significantly simplifies cross-region DR strategies. You can now specify multiple destination regions in a single AWS Backup copy action. Option A describes the old approach (pre-October 2025) requiring sequential operations. Option C adds unnecessary complexity with custom Lambda code. Option D is manual and operationally inefficient. The new capability provides: (1) Single operation for multiple region copies, (2) Automatic re-encryption with destination vault's KMS key, (3) Incremental copies for supported services, (4) Integrated with backup plans for automation."
        },
        {
          "id": "D2-T2.2-Q3",
          "question": "A healthcare company must retain EBS snapshots and RDS backups for 7 years for HIPAA compliance. They want automated lifecycle management and the ability to restore to any point within that period. Which AWS Backup configuration meets these requirements MOST cost-effectively?",
          "options": [
            "Create backup plan with retention of 7 years and lifecycle transition to cold storage after 90 days",
            "Use AWS Backup Vault Lock with compliance mode to enforce 7-year retention with WORM protection",
            "Configure both: backup plan with 7-year retention + lifecycle to cold storage after 90 days + Backup Vault Lock in compliance mode for WORM",
            "Store snapshots in S3 Glacier Deep Archive with lifecycle policy"
          ],
          "correctAnswer": 2,
          "explanation": "For HIPAA compliance with 7-year retention, the comprehensive solution combines: (1) Backup plan with 7-year retention period defining when backups are taken, (2) Lifecycle policy transitioning to cold storage after 90 days for cost optimization (cold storage is up to 90% cheaper), (3) Backup Vault Lock in compliance mode for WORM (Write-Once-Read-Many) protection preventing deletion until retention expires, meeting regulatory requirements. Option A lacks WORM protection required for compliance. Option B provides WORM but doesn't optimize costs with cold storage. Option D doesn't use AWS Backup's centralized management. AWS Backup cold storage supports EBS, EFS, and VMware backups. Important: Vault Lock compliance mode cannot be disabled once enabled - it provides irrevocable protection, which is required for regulatory compliance but should be tested in governance mode first."
        },
        {
          "id": "D2-T2.2-Q4",
          "question": "A company uses S3 Cross-Region Replication (CRR) from us-west-2 to eu-central-1. They enable S3 Replication Time Control (RTC) for compliance SLA. What guarantee does RTC provide?",
          "options": [
            "99.9% of objects replicate within 15 minutes with SLA-backed guarantee, replication metrics, and event notifications",
            "100% of objects replicate within 15 minutes guaranteed",
            "99.99% design target with replication metrics but no formal SLA",
            "Synchronous replication with zero RPO"
          ],
          "correctAnswer": 0,
          "explanation": "S3 Replication Time Control (RTC) is backed by an SLA guaranteeing 99.9% (not 99.99%) of objects replicated within 15 minutes during any billing month. While the design target is 99.99%, the formal SLA commitment is 99.9%. RTC also provides: (1) Replication metrics visible in CloudWatch, (2) Event notifications when objects don't meet the 15-minute SLA, (3) Visibility through S3 console showing missed SLA thresholds. The 15-minute SLA is critical for compliance requirements with specific RPO needs. Option B is incorrect - 100% is impossible to guarantee in distributed systems. Option C describes the design goal but not the SLA commitment. Option D is incorrect - S3 replication is asynchronous, not synchronous; true synchronous replication would severely impact performance. RTC costs more than standard CRR but provides SLA guarantees. Note: The SLA doesn't apply when replication data transfer rate exceeds the default 1 Gbps quota."
        },
        {
          "id": "D2-T2.2-Q5",
          "question": "A financial services application requires RPO of 5 minutes and RTO of 15 minutes for their MySQL database. The database is 2TB with moderate write activity. Which solution meets these requirements MOST cost-effectively?",
          "options": [
            "Aurora MySQL with Aurora Global Database providing ~1 second RPO and <1 minute RTO",
            "RDS MySQL Multi-AZ with automated backups every 5 minutes",
            "RDS MySQL with read replica in another region, promoted manually during failures",
            "Aurora MySQL with cross-region read replica and automated backups every 5 minutes"
          ],
          "correctAnswer": 3,
          "explanation": "Aurora MySQL with cross-region read replica provides: (1) Continuous replication with typical lag of seconds (well within 5-minute RPO), (2) Fast promotion of read replica to standalone cluster (within 15-minute RTO), (3) More cost-effective than Aurora Global Database for this RPO/RTO requirement. Aurora Global Database (Option A) provides superior metrics (~1s RPO, <1min RTO) but costs more due to the globally distributed architecture - overengineered for 5min/15min requirements. Option B is incorrect - RDS automated backups are continuous via transaction logs, not every 5 minutes, and provide point-in-time recovery, but restoring from backup takes longer than 15 minutes for 2TB. Option C (RDS with cross-region replica) works but Aurora provides faster failover. Key decision: Balance requirements vs cost - don't overprovision DR capabilities beyond requirements."
        },
        {
          "id": "D2-T2.2-Q6",
          "question": "A company has encrypted RDS instances in Account A (us-east-1) and needs to create cross-region, cross-account backups to Account B (eu-west-1) for DR. What configuration is required for encrypted backups? (Select THREE)",
          "options": [
            "Share the source KMS key from Account A with Account B",
            "Create a backup vault in Account B (eu-west-1) with its own KMS key",
            "Configure AWS Backup in Account A with a backup plan that copies to Account B's vault",
            "Disable encryption on RDS before creating backups",
            "Use AWS Backup resource-based policy on the destination vault to allow Account A to copy backups",
            "Enable RDS snapshot sharing and manually copy snapshots"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            2,
            4
          ],
          "explanation": "Cross-region, cross-account encrypted backup requires: (1) Backup vault in destination account/region (Account B, eu-west-1) with its own KMS key for re-encryption, (2) Backup plan in source account (Account A) configured to copy to the destination vault, (3) Resource-based policy on destination vault allowing source account to copy backups. AWS Backup automatically re-encrypts backups using the destination vault's KMS key, so you don't share the source KMS key (Option A is incorrect). Option D is wrong - you never disable encryption for compliance/security reasons; AWS Backup handles encrypted backups natively. Option F (manual snapshot sharing) works but is not using AWS Backup's automated cross-account copy feature. Important: The destination vault's resource-based policy must grant permissions to the source account, and the source account's IAM role must have permissions to write to the destination vault."
        },
        {
          "id": "D2-T2.2-Q7",
          "question": "An e-commerce company experiences a regional failure in their primary region (us-east-1). They have pilot light DR in us-west-2 with minimal infrastructure running. They need to scale up capacity to handle production traffic. In which order should they execute their DR runbook to minimize RTO?",
          "options": [
            "1) Update DNS 2) Scale up compute 3) Promote database replica 4) Test application",
            "1) Promote database replica 2) Scale up compute 3) Test application 4) Update DNS to route traffic",
            "1) Test application 2) Promote database replica 3) Update DNS 4) Scale up compute",
            "1) Scale up compute 2) Promote database replica 3) Update DNS 4) Test application"
          ],
          "correctAnswer": 1,
          "explanation": "The correct DR execution order minimizes RTO while ensuring system integrity: (1) Promote database replica FIRST - this is typically the longest operation (promoting RDS/Aurora replica) and must complete before application can function, (2) Scale up compute (Auto Scaling group desired capacity, ECS task count) - while database is promoting or immediately after, (3) Test application functionality to verify everything works before customer impact, (4) Update DNS to route traffic only after confirming the DR environment is functional. Option A (DNS first) sends traffic to an environment that's not ready, causing customer impact. Option C (test before database/compute) is impossible - can't test without functional infrastructure. Option D (DNS before testing) risks routing customers to broken environment. Best practice: Automate DR runbook with AWS Systems Manager Automation Documents that execute these steps in order, with validation gates between each step."
        },
        {
          "id": "D2-T2.2-Q8",
          "question": "A company uses DynamoDB with Point-in-Time Recovery (PITR) enabled. They accidentally delete critical data at 2:00 PM. The deletion is discovered at 2:30 PM. What is the BEST recovery approach?",
          "options": [
            "Restore from PITR to 1:59 PM into a new table, then copy the deleted items back to the production table",
            "Contact AWS Support to recover the deleted data",
            "Restore from the most recent on-demand backup",
            "Enable DynamoDB Streams and replay events from 1:59 PM to 2:00 PM"
          ],
          "correctAnswer": 0,
          "explanation": "DynamoDB Point-in-Time Recovery (PITR) allows restoring to any point within the last 35 days with second-level granularity. The recovery approach: (1) Restore table to 1:59 PM (one minute before deletion) into a new table - this creates a new DynamoDB table with data as it existed at that timestamp, (2) Query the new table for deleted items, (3) Copy/write those items back to the production table using BatchWriteItem or DynamoDB import/export. PITR always restores to a NEW table, never in-place. Option B (AWS Support) cannot recover data; PITR is customer-managed. Option C (on-demand backup) works only if you took a backup between 1:59 PM and 2:00 PM, unlikely for a specific minute. Option D misunderstands DynamoDB Streams - Streams capture changes for 24 hours and are for triggering Lambda/processing, not for replay-based recovery. Important: PITR has a 5-minute lag (backup is current to within 5 minutes of present time)."
        },
        {
          "id": "D2-T2.2-Q9",
          "question": "A company migrates an on-premises Oracle database (10TB) to AWS using AWS DMS. The DMS replication instance keeps running out of storage during full load. What should they do to resolve this?",
          "options": [
            "Increase the DMS replication instance size to a larger class",
            "Enable multi-threading on the DMS task for faster load",
            "Increase the storage allocated to the DMS replication instance",
            "Use DMS Serverless which automatically scales storage"
          ],
          "correctAnswer": 2,
          "explanation": "DMS replication instances come with default storage (50GB or 100GB depending on instance class) used for log files and cached changes. During large migrations or busy source systems, this storage fills up. The solution is to increase allocated storage on the replication instance, not the instance size/class. Storage and compute are independent configurations. Option A (larger instance class) provides more CPU/memory but not necessarily more storage. Option B (multi-threading) speeds up migration but doesn't address storage. Option D (DMS Serverless) automatically scales compute and capacity but the question implies they're using instance-based DMS already deployed. Best practice for large migrations: (1) Estimate storage needs based on transaction volume during migration, (2) Monitor CloudWatch metrics for storage usage, (3) Allocate extra storage headroom (30-50% more than estimate), (4) For very large migrations, consider DMS Serverless which automatically handles capacity scaling."
        },
        {
          "id": "D2-T2.2-Q10",
          "question": "A media company stores video files in S3 (100TB) in us-east-1 and needs them replicated to eu-west-1 within 15 minutes for compliance. Existing videos (uploaded before replication was enabled) must also be replicated. What configuration is required? (Select TWO)",
          "options": [
            "Enable S3 Cross-Region Replication (CRR) with S3 Replication Time Control (RTC)",
            "Enable S3 Versioning on both source and destination buckets",
            "Use S3 Batch Replication to replicate existing objects",
            "Enable S3 Transfer Acceleration for faster replication",
            "Configure S3 Lifecycle policy to copy objects to the destination bucket",
            "Use AWS DataSync to initially sync existing files, then enable CRR"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            1,
            2
          ],
          "explanation": "The complete solution requires: (1) S3 CRR with RTC for 15-minute SLA (RTC provides 99.99% of objects replicated within 15 minutes), (2) S3 Versioning on both buckets (prerequisite for CRR), (3) S3 Batch Replication to replicate existing objects since CRR only replicates objects uploaded after enabling replication. Option D (Transfer Acceleration) is for faster uploads to S3, not for replication between buckets. Option E (Lifecycle policy) can transition storage classes but doesn't replicate/copy objects across regions. Option F (DataSync) could work for initial sync but is unnecessary complexity - S3 Batch Replication is the AWS-native solution for replicating existing objects. Important: RTC adds cost but provides SLA guarantees and replication metrics. S3 Batch Replication creates a one-time job to replicate existing objects; after completion, ongoing CRR handles new objects."
        },
        {
          "id": "D2-T2.2-Q11",
          "question": "A company has an Aurora MySQL cluster in us-east-1 with 1 writer and 3 readers. They need to implement DR in us-west-2 with RTO of 2 minutes and RPO of 10 seconds. Which Aurora configuration provides the BEST balance of cost and requirements?",
          "options": [
            "Aurora Global Database with managed planned failover for zero RPO during maintenance",
            "Aurora cross-region read replica with manual promotion during failures",
            "Aurora Multi-AZ with automated failover (only protects against AZ failures, not regional)",
            "Aurora backtrack feature for point-in-time recovery"
          ],
          "correctAnswer": 0,
          "explanation": "Aurora Global Database is the correct choice for cross-region DR with stringent RTO/RPO requirements: (1) RPO of ~1 second (typical replication lag) meets the 10-second requirement, (2) RTO of approximately 1-5 minutes for cross-region failover meets the 2-minute requirement, (3) Managed planned failover provides RPO of 0 for maintenance windows. Option B (cross-region read replica) provides similar RPO but longer RTO as manual promotion takes more time than Global Database's coordinated failover. Option C (Multi-AZ) only protects against AZ failures within a region, not regional disasters. Option D (backtrack) is for rewinding the database to a prior state within the same cluster, not cross-region DR. Aurora Global Database supports up to 5 secondary regions with up to 16 read replicas per region. Cost consideration: Global Database costs more than single-region cross-region replicas but provides better RTO and managed failover capabilities."
        },
        {
          "id": "D2-T2.2-Q12",
          "question": "A SaaS platform uses DynamoDB Global Tables with Multi-Region Eventual Consistency (MREC) across us-east-1, eu-west-1, and ap-southeast-1. A customer reports that they updated a record in the EU but still see old data when querying from the AP region. What is the MOST likely cause?",
          "options": [
            "DynamoDB Global Tables are broken and need AWS Support intervention",
            "The application is using eventually consistent reads, and replication lag hasn't completed yet (typically sub-second but can be higher under load)",
            "Multi-Region Eventual Consistency is misconfigured; should use Multi-Region Strong Consistency (MRSC)",
            "DynamoDB Global Tables only replicate on a scheduled batch basis, not in real-time"
          ],
          "correctAnswer": 1,
          "explanation": "With DynamoDB Global Tables MREC (Multi-Region Eventual Consistency), writes are asynchronously replicated across regions, typically within one second or less. However, under high load or network conditions, replication can take longer. Additionally, if the application uses eventually consistent reads (default for GetItem/Query), it might read from a replica that hasn't received the latest update yet. To see the latest data, use strongly consistent reads locally (reads within the same region where the write occurred). Option A is overly dramatic - this is expected behavior with eventual consistency. Option C (MRSC) is available as of June 2025 but has constraints (exactly 3 regions, no transactions, higher latency) and isn't necessary for most use cases. Option D is incorrect - Global Tables replicate continuously, not in batches. Best practice: Understand consistency model implications. If your use case requires reading latest writes immediately, either: (1) Use strongly consistent reads in the same region, (2) Design application to handle eventual consistency, or (3) Evaluate MRSC if zero RPO is required."
        },
        {
          "id": "D2-T2.2-Q13",
          "question": "A company performs quarterly DR testing by promoting RDS read replicas in their DR region to standalone instances. After testing, they want to revert to the original configuration. What is the MOST operationally efficient approach?",
          "options": [
            "Promote read replica for testing, then delete it and create a new read replica from the primary",
            "Use AWS Backup to snapshot the read replica before promotion, promote for testing, then restore from snapshot",
            "Never promote read replicas during testing; only promote during actual disasters",
            "Use Route 53 Application Recovery Controller to simulate failover without actually promoting the replica"
          ],
          "correctAnswer": 0,
          "explanation": "RDS read replica promotion is a one-way operation - once promoted, the replica becomes a standalone instance and cannot be converted back to a replica. The standard approach for DR testing: (1) Promote read replica to test failover, (2) Conduct DR test, (3) Delete the promoted instance, (4) Create a new read replica from the primary for future DR. This validates the complete DR process including replica promotion. Option B adds complexity with snapshots and doesn't test the actual promotion process. Option C (never promote during testing) leaves you uncertain whether DR will work during a real disaster - the purpose of testing is to validate the complete procedure. Option D (Route 53 ARC) can orchestrate failover but doesn't validate the actual database promotion mechanism. Best practice: Automate the entire process - promote, test, delete, recreate - using AWS Systems Manager Automation Documents or Step Functions. This ensures DR procedures are validated and repeatable. Cost optimization: Schedule DR testing during low-traffic periods and use smaller instance classes for DR replicas that are scaled up during actual failover."
        },
        {
          "id": "D2-T2.2-Q14",
          "question": "A company uses AWS Backup to protect EC2 instances, EBS volumes, and RDS databases across 50 accounts. They want centralized visibility of backup compliance and automated notifications when backups fail. Which AWS Backup feature provides this?",
          "options": [
            "AWS Backup Audit Manager with compliance framework and SNS notifications",
            "CloudWatch Events triggered by AWS Backup job completions",
            "AWS Config rules for backup validation",
            "CloudTrail logs analysis with Athena queries"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Backup Audit Manager provides centralized backup compliance monitoring: (1) Built-in compliance frameworks (e.g., requiring daily backups, cross-region copies, retention policies), (2) Automated evaluation of backup activity against frameworks, (3) Compliance reports showing which resources meet/violate policies, (4) Integration with SNS for notifications on compliance violations, (5) Cross-account and cross-region visibility when using Organizations integration. Option B (CloudWatch Events) can trigger on backup events but requires custom logic to evaluate compliance. Option C (Config rules) can validate backups but isn't purpose-built for backup compliance. Option D (CloudTrail + Athena) is overly complex for what Audit Manager provides natively. AWS Backup Audit Manager frameworks can enforce requirements like: 'All EC2 instances must have daily backups with 7-day retention and cross-region copy to at least one other region.' This is critical for demonstrating compliance to auditors with automated evidence."
        }
      ]
    },
    {
      "filename": "domain-2-task-2.1-deployment-strategy.json",
      "domain": "Domain 2: Design for New Solutions",
      "task": "Task 2.1: Deployment Strategy",
      "question_count": 12,
      "questions": [
        {
          "question": "In July 2025, AWS introduced built-in blue/green deployment capability for ECS, eliminating the need for CodeDeploy. A company wants to implement this new feature for their ECS Fargate service. During deployment, they need to run integration tests against the green environment before routing production traffic. Which capability should they leverage?",
          "options": [
            "Configure CodeDeploy lifecycle hooks even though using built-in ECS blue/green",
            "Use ECS's automatic Lambda function invocation at specified lifecycle stages to run tests against the green revision",
            "Implement manual testing by keeping both blue and green running simultaneously",
            "Configure Application Load Balancer health checks to validate the green environment"
          ],
          "correctAnswer": 1,
          "explanation": "The new built-in ECS blue/green deployment feature (July 2025) automatically invokes Lambda functions at specified lifecycle stages, allowing comprehensive testing against the green revision before traffic cutover. This is included with Amazon ECS at no additional charge. Teams can configure Lambda functions to run integration tests, smoke tests, or any validation logic. Option A is incorrect - the whole point of the new feature is eliminating the need for CodeDeploy. Option C (manual testing) defeats the automation purpose. Option D (ALB health checks) validates basic health but doesn't run comprehensive integration tests. Key benefit: This new capability makes complex CodeDeploy workarounds unnecessary while providing near-instantaneous rollback capability if issues arise during validation."
        },
        {
          "question": "A team is implementing canary deployments for their Lambda function processing critical financial transactions. They want to route 10% of traffic to the new version for 10 minutes, then automatically shift the remaining 90% if no errors occur. Which deployment configuration achieves this?",
          "options": [
            "Use Lambda versions with weighted aliases: assign 90% weight to $LATEST and 10% to the new version",
            "Configure CodeDeploy with Lambda deployment using Canary10Percent10Minutes configuration",
            "Implement API Gateway canary deployment with 10% traffic to the new stage",
            "Use EventBridge rules to route 10% of events to the new Lambda version"
          ],
          "correctAnswer": 1,
          "explanation": "AWS CodeDeploy for Lambda supports predefined canary configurations like 'Canary10Percent10Minutes' which shifts 10% of traffic to the new version, waits 10 minutes, then automatically shifts the remaining 90% if CloudWatch alarms don't trigger. CodeDeploy can automatically rollback if failures are detected. Option A (weighted aliases) allows percentage-based routing but doesn't provide the automatic time-based progression or automated rollback that CodeDeploy offers. Option C (API Gateway canary) works for API-triggered Lambdas but doesn't help with event-driven sources like SQS, Kinesis, etc. Option D doesn't exist as a native capability. CodeDeploy also supports Linear (e.g., Linear10PercentEvery10Minutes) and All-at-once deployment patterns. Best practice: Use CloudWatch Alarms with CodeDeploy to automatically trigger rollback on error rate increases."
        },
        {
          "question": "A company needs to choose an Infrastructure as Code tool for their new AWS-only project. The development team is proficient in Python and wants to use familiar programming constructs (loops, conditionals, functions). They need to deploy across 10 AWS accounts with slight variations per account. Which tool is MOST appropriate?",
          "options": [
            "AWS CloudFormation with YAML templates and nested stacks for reusability",
            "AWS CDK with Python, leveraging programming language features and CDK constructs",
            "Terraform with HCL, using modules for reusability across accounts",
            "AWS CloudFormation with JSON templates and template parameters"
          ],
          "correctAnswer": 1,
          "explanation": "AWS CDK with Python is the optimal choice given the requirements: (1) Team is proficient in Python - no new language to learn, (2) Can use Python's loops, conditionals, functions, classes for complex logic and variations across accounts, (3) AWS-only project - CDK's tight AWS integration is beneficial, (4) CDK constructs provide higher-level abstractions reducing boilerplate. CDK synthesizes to CloudFormation, so you get CloudFormation's state management benefits. Option A (CloudFormation YAML) requires learning YAML/CloudFormation syntax and is more verbose for variations. Option C (Terraform) requires learning HCL and is better suited for multi-cloud; for AWS-only, CDK's ergonomics are superior. Option D (JSON) is even more verbose than YAML. As of 2025, CDK has growing momentum and AWS investment, making it increasingly the default choice for AWS-only infrastructure. CDK supports L1 (CloudFormation), L2 (curated), and L3 (patterns) constructs for different abstraction levels."
        },
        {
          "question": "A development team uses AWS CloudFormation StackSets to deploy a VPC architecture across 50 accounts in AWS Organizations. They update the template to add new subnets. They want to ensure that if the update fails in more than 3 accounts, the entire deployment stops. Which StackSets configuration achieves this?",
          "options": [
            "Set Maximum Concurrent Accounts to 3 and Continue Deploying to Remaining Accounts on Failure",
            "Set Failure Tolerance Count to 3 and Stop Operation on Subsequent Failures",
            "Configure CloudFormation rollback triggers with CloudWatch alarms",
            "Use StackSets drift detection to prevent deployments with more than 3 drifted accounts"
          ],
          "correctAnswer": 1,
          "explanation": "StackSets provides deployment control options including Failure Tolerance. Setting Failure Tolerance Count to 3 means: if updates fail in 3 or fewer accounts, the operation continues; if failures exceed 3, the operation stops and doesn't deploy to remaining accounts. This prevents cascading failures across the organization. You also configure Maximum Concurrent Accounts (parallelism) separately. For example: Max Concurrent = 10, Failure Tolerance = 3 means StackSets deploys to 10 accounts at a time but stops the entire operation if total failures exceed 3. Option A controls parallelism but doesn't stop on failures. Option C (rollback triggers) works for individual stacks but not for controlling StackSets operations across multiple accounts. Option D (drift detection) is for identifying configuration drift, not for controlling deployment failures. Best practice: Set conservative failure tolerance for production changes to prevent wide-scale issues."
        },
        {
          "question": "A SaaS company deploys customer-specific resources (databases, compute, storage) using Infrastructure as Code. They need feature flags to enable/disable features per customer without redeploying infrastructure. Which approach provides the MOST operationally efficient solution?",
          "options": [
            "Use CloudFormation parameters and update stacks with different parameter values per customer",
            "Implement AWS AppConfig for feature flag management, decoupling feature enablement from infrastructure deployment",
            "Store feature flags in DynamoDB and query on each request",
            "Use separate CloudFormation templates for each feature combination"
          ],
          "correctAnswer": 1,
          "explanation": "AWS AppConfig is purpose-built for feature flag and configuration management. It provides: (1) Dynamic configuration changes without redeployment, (2) Safe deployment strategies for configuration changes (similar to code deployments), (3) Validation of configuration data before deployment, (4) Rollback capability if issues occur, (5) Integration with Lambda, ECS, EC2, and other compute services. Feature flags can be enabled/disabled per customer instantly without infrastructure changes. Option A (CloudFormation parameters) requires stack updates which are slow and risky for simple feature toggles. Option C (DynamoDB) can work but requires custom implementation of safe rollout, validation, and rollback - reinventing what AppConfig provides. Option D (separate templates) creates maintenance nightmare. AppConfig deployment strategies include all-at-once, linear, and exponential rollout patterns, similar to CodeDeploy but for configuration."
        },
        {
          "question": "A financial services company must deploy applications with zero downtime and the ability to instantly rollback. They use Application Load Balancer with two target groups. During deployment, they want to validate the new version with synthetic transactions before routing real user traffic. Which deployment strategy and validation approach should they implement?",
          "options": [
            "Blue/green deployment with Route 53 weighted routing for gradual traffic shift",
            "Blue/green deployment with ALB listener rules and weighted target groups, using a pre-traffic Lambda hook for validation",
            "Canary deployment with CloudWatch Synthetics running tests against the canary version",
            "Rolling deployment with Connection Draining enabled"
          ],
          "correctAnswer": 1,
          "explanation": "Blue/green with ALB weighted target groups provides instant rollback capability and zero downtime. The architecture: (1) Blue target group handles production traffic (100% weight), (2) Deploy new version to green target group (0% weight), (3) Use Lambda hook to run synthetic tests against green, (4) If tests pass, shift traffic by adjusting weights (can be gradual: 10%, 50%, 100%), (5) If issues occur, instantly revert weights to 100% blue. ALB allows weights from 0-100 on target groups. Option A (Route 53) has slower rollback due to DNS propagation/caching. Option C (canary) works but doesn't provide the instant rollback that ALB weighted target groups offer. Option D (rolling) has downtime during rollback (must redeploy previous version). With ALB weighted target groups, rollback is near-instantaneous (just adjust weights), meeting the 'instantly rollback' requirement."
        },
        {
          "question": "A company uses CloudFormation to deploy a complex application with dependencies between resources (VPC  Subnets  EC2  Load Balancer  DNS). Template updates sometimes fail midway, leaving the stack in UPDATE_ROLLBACK_FAILED state. Which CloudFormation features help prevent and recover from this scenario? (Select TWO)",
          "options": [
            "Use Stack Policy to prevent updates to critical resources",
            "Enable Termination Protection on the stack",
            "Implement CloudFormation DeletionPolicy: Retain on critical resources",
            "Use Change Sets to preview changes before execution",
            "Configure CloudFormation to Continue Update Rollback to skip problematic resources",
            "Enable Automatic Rollback on CloudWatch Alarm triggers"
          ],
          "type": "multiple",
          "correctAnswer": [
            3,
            4
          ],
          "explanation": "The best combination is: (1) Change Sets allow previewing exactly what CloudFormation will change before executing, reducing surprises that cause failures, and (2) Continue Update Rollback allows recovering from UPDATE_ROLLBACK_FAILED state by skipping resources that can't be rolled back. Option A (Stack Policy) prevents accidental updates but doesn't help with failed rollbacks. Option B (Termination Protection) prevents stack deletion, not relevant here. Option C (DeletionPolicy Retain) prevents resource deletion but doesn't address rollback failures. Option F (Alarm triggers) can automatically rollback but doesn't prevent or recover from UPDATE_ROLLBACK_FAILED state. When UPDATE_ROLLBACK_FAILED occurs, you use Continue Update Rollback in the console or AWS CLI, optionally specifying resources to skip. This is critical for recovering from complex failure scenarios without losing the entire stack."
        },
        {
          "question": "An enterprise manages infrastructure for both AWS and on-premises VMware environments. They need a single Infrastructure as Code tool that can provision resources in both environments, with existing team expertise in declarative configuration. Which tool should they standardize on?",
          "options": [
            "AWS CDK, using AWS CDK for CloudFormation (AWS resources) and CDK8s for Kubernetes-based VMware",
            "Terraform, leveraging AWS provider for cloud resources and VMware provider for on-premises",
            "AWS CloudFormation with custom resources backed by Lambda to provision VMware resources",
            "Ansible for both environments as it supports both cloud and on-premises provisioning"
          ],
          "correctAnswer": 1,
          "explanation": "Terraform is the correct choice for hybrid cloud/on-premises infrastructure. Key advantages: (1) Official providers for both AWS and VMware vSphere, (2) Declarative HCL syntax consistent across providers, (3) Single workflow and state management across environments, (4) Mature ecosystem and community support for hybrid scenarios. The team can use Terraform modules to abstract differences between AWS and VMware. Option A (CDK) is primarily AWS-focused; while CDK8s exists for Kubernetes, it doesn't directly support VMware vSphere. Option C (CloudFormation with custom resources) is overly complex and defeats the purpose of IaC - custom Lambda code to manage VMware is maintenance-heavy. Option D (Ansible) is a configuration management tool, not primarily an IaC provisioning tool, though it can provision resources. Terraform's strength in multi-cloud and hybrid scenarios makes it the industry standard for this use case, as confirmed by 2025 trends showing Terraform as the default choice for multi-provider scenarios."
        },
        {
          "question": "A company deploys microservices using AWS CodePipeline with stages: Source (GitHub)  Build (CodeBuild)  Deploy to Dev (ECS)  Manual Approval  Deploy to Prod (ECS). They want to add security scanning after Build and automatically fail the pipeline if critical vulnerabilities are found. Where should security scanning be added?",
          "options": [
            "Add a Test stage after Build with CodeBuild project running security scanners (Snyk, Trivy), configured to fail the stage on critical findings",
            "Implement security scanning in the Build stage CodeBuild project as a post-build phase",
            "Use Lambda function triggered by CodePipeline between Build and Deploy stages",
            "Enable Amazon Inspector scanning in ECR which will automatically block vulnerable images from deployment"
          ],
          "correctAnswer": 0,
          "explanation": "Adding a dedicated Test stage with security scanning provides: (1) Clear separation of concerns (build vs test), (2) Explicit visibility in pipeline - stakeholders see that security scanning occurred, (3) Ability to run multiple test types in parallel, (4) Clear failure indication if vulnerabilities found. The CodeBuild project in the Test stage runs security scanners and uses exit codes to signal pass/fail, stopping the pipeline before Dev deployment if critical vulnerabilities exist. Option B (post-build phase) works but lacks visibility - failures appear as 'Build failed' rather than 'Security scan failed'. Option C (Lambda) adds unnecessary complexity - CodeBuild can run any security tool. Option D (Inspector in ECR) provides scanning but doesn't automatically block deployments - it reports findings. Best practice: Use dedicated Test stage with parallel actions for different scan types (SAST with CodeGuru, container scanning with Trivy/Snyk, dependency checking with OWASP Dependency-Check)."
        },
        {
          "question": "A development team uses Terraform to manage AWS infrastructure across 20 environments (4 regions  5 stages). They experience frequent state locking conflicts when multiple team members deploy simultaneously. Which configuration provides the MOST robust state management?",
          "options": [
            "Use local state files with Git for version control",
            "Configure S3 backend with DynamoDB for state locking, with separate state files per environment",
            "Use Terraform Cloud for state management and collaboration",
            "Store state in S3 with versioning enabled, without DynamoDB locking"
          ],
          "correctAnswer": 1,
          "explanation": "S3 backend with DynamoDB state locking is the AWS-native, robust solution for team collaboration. Configuration: (1) S3 bucket with versioning for state files (disaster recovery), (2) DynamoDB table for state locking (prevents concurrent modifications), (3) Separate state files per environment using workspace or different S3 keys. This prevents conflicts while maintaining separation. S3 provides durability, versioning for rollback, and encryption. DynamoDB locking ensures only one person/process can modify state at a time. Option A (local + Git) is extremely problematic - state contains sensitive data and Git isn't designed for state management; merge conflicts are disastrous. Option C (Terraform Cloud) is excellent but adds external dependency and cost. Option D (S3 without DynamoDB) risks state corruption from concurrent updates. Best practice: Enable S3 versioning, bucket encryption, and restricted IAM access. Use separate state files per environment (either via workspaces or different S3 keys) to prevent cross-environment impact."
        },
        {
          "question": "A company uses AWS CDK to deploy infrastructure. They want to ensure that developers can see what CloudFormation resources will be created before deploying. Additionally, they need to validate that CDK applications comply with organizational policies (e.g., all S3 buckets encrypted, no public access). Which CDK features address these requirements? (Select TWO)",
          "options": [
            "Use 'cdk diff' command to preview changes before deployment",
            "Implement CDK Aspects to validate and enforce policies across constructs",
            "Enable CloudFormation Change Sets in CDK configuration",
            "Use 'cdk synth' to generate CloudFormation templates for review",
            "Configure AWS Config rules to validate deployed resources",
            "Implement custom Lambda-backed CloudFormation resources for validation"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            3
          ],
          "explanation": "The correct combination is: (1) CDK Aspects allow implementing cross-cutting concerns and policy validation. Aspects visit all constructs in the CDK app and can validate, modify, or enforce rules. For example, an aspect can verify all S3 buckets have encryption enabled and fail the synth process if not. (2) 'cdk synth' generates the CloudFormation template that will be deployed, allowing review of exact resources before deployment. While 'cdk diff' (Option A) shows changes, it doesn't help with initial deployments or policy validation. Option C is incorrect - Change Sets are CloudFormation feature automatically used by CDK, not separately configured. Option E (Config rules) validates after deployment, not before. Option F is overengineered. CDK Aspects example: class BucketEncryptionAspect implements IAspect { visit(node: IConstruct) { if (node instanceof s3.Bucket && !node.encryptionKey) { Annotations.of(node).addError('Bucket must be encrypted'); } }}. This fails synthesis if policies aren't met, preventing deployment."
        },
        {
          "question": "A platform team manages a base networking infrastructure (VPC, subnets, security groups) deployed via CloudFormation. Application teams need to deploy resources into this VPC without having permissions to modify the networking stack. Which CloudFormation feature enables this safe resource sharing?",
          "options": [
            "Use CloudFormation Cross-Stack References with Outputs and ImportValue",
            "Grant application teams read-only access to the networking stack",
            "Use AWS Resource Access Manager (RAM) to share VPC resources",
            "Store VPC IDs in Systems Manager Parameter Store for application stacks to reference"
          ],
          "correctAnswer": 0,
          "explanation": "CloudFormation Cross-Stack References using Outputs and ImportValue is the native solution. The networking stack exports values (VPC ID, subnet IDs, security group IDs) using Outputs with Export names. Application stacks import these values using Fn::ImportValue. Benefits: (1) Type-safe references (CloudFormation validates that exported values exist), (2) Dependency tracking (CloudFormation prevents deleting exported values while they're in use), (3) No additional permissions needed on networking stack. Example: Networking stack: Outputs: VPCId: Value: !Ref VPC Export: Name: Platform-VPC-ID. Application stack: Resources: EC2Instance: Properties: SubnetId: !ImportValue Platform-Subnet-ID. Option B (read access) doesn't solve the deployment integration problem. Option C (RAM) is for sharing actual resources, not for CloudFormation integration. Option D (Parameter Store) works but lacks CloudFormation's native dependency tracking and validation. Cross-stack references provide clean separation: platform team owns infrastructure, app teams deploy into it without modification permissions."
        }
      ]
    },
    {
      "filename": "domain-2-task-2.3-security-controls.json",
      "domain": "Domain 2: Design for New Solutions",
      "task": "Task 2.3: Security Controls",
      "question_count": 16,
      "questions": [
        {
          "id": "D2-T2.3-Q1",
          "question": "A web application behind an ALB experiences bot attacks that bypass traditional rate limiting. The security team needs protection against sophisticated bots using machine learning detection while allowing legitimate search engine crawlers. Which AWS WAF configuration provides the BEST protection?",
          "options": [
            "AWS WAF Bot Control Common level (detects self-identifying bots only)",
            "AWS WAF Bot Control Targeted Protection level with ML-based detection and search engine allow rules",
            "AWS WAF rate-based rule blocking requests exceeding 2000 per 5 minutes",
            "AWS Shield Advanced with DDoS protection"
          ],
          "correctAnswer": 1,
          "explanation": "AWS WAF Bot Control Targeted Protection level provides comprehensive bot protection using: (1) Machine learning analysis (rules starting with TGT_ML_) that detect anomalous behavior indicative of distributed bot activity, (2) Browser interrogation and fingerprinting for sophisticated bots that don't self-identify, (3) Built-in rules to allow legitimate bots like search engines (GoogleBot, BingBot). The ML model analyzes traffic statistics including timestamps, browser characteristics, and behavioral patterns. Option A (Common level) only detects self-identifying bots using static analysis, missing sophisticated attacks. Option C (rate-based rules) is easily bypassed by distributed bots. Option D (Shield Advanced) protects against DDoS but doesn't provide granular bot detection. Bot Control also includes Token Reuse Detection (2025 enhancement) identifying token reuse across different ASNs and geographic locations with adjustable sensitivity."
        },
        {
          "id": "D2-T2.3-Q2",
          "question": "A financial application requires encryption of data at rest and in transit. Database credentials must be rotated every 30 days automatically. The application runs on ECS Fargate and connects to RDS PostgreSQL. Which combination provides the MOST secure and automated solution? (Select TWO)",
          "options": [
            "Store credentials in AWS Secrets Manager with automatic rotation enabled using Lambda rotation function",
            "Use IAM database authentication for RDS eliminating the need for passwords",
            "Store credentials in Systems Manager Parameter Store with manual rotation",
            "Enable RDS encryption at rest with AWS-managed KMS keys",
            "Use application-managed credential rotation with credentials in environment variables",
            "Enable SSL/TLS for RDS connections with certificate verification"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            1
          ],
          "explanation": "The optimal combination is: (1) Secrets Manager with automatic rotation - provides automated credential rotation every 30 days using AWS-provided or custom Lambda functions. ECS tasks retrieve credentials at runtime, getting the latest rotated values. (2) IAM database authentication eliminates passwords entirely - ECS task role generates authentication tokens valid for 15 minutes, providing better security than password-based auth. While both can be used, IAM auth is more secure (no secrets to rotate). Options D and F are important but address encryption, not the credential rotation requirement. Option C (Parameter Store) doesn't provide automatic rotation like Secrets Manager. Option E (environment variables) is insecure - credentials are visible in task definitions. Best practice: Use IAM database authentication where possible; for applications requiring traditional passwords, use Secrets Manager with automatic rotation."
        },
        {
          "id": "D2-T2.3-Q3",
          "question": "A company uses AWS KMS customer-managed keys (CMKs) to encrypt S3 buckets, EBS volumes, and RDS databases across 50 AWS accounts. They need centralized key management and the ability to immediately disable access to all encrypted data in case of a security incident. What is the MOST operationally efficient approach?",
          "options": [
            "Create separate CMKs in each account and manually disable them during incidents",
            "Create a CMK in a central security account, share it across accounts using key policies, and disable the CMK to revoke all access",
            "Use AWS-managed keys which can be centrally controlled through Organizations",
            "Create CMKs in each account but use AWS Config to automate disabling via Lambda"
          ],
          "correctAnswer": 1,
          "explanation": "A centralized CMK in a security account with cross-account sharing provides: (1) Single point of management - one key policy controls access from all accounts, (2) Immediate revocation - disabling the CMK instantly revokes decrypt access across all accounts and resources, (3) Centralized audit trail - all key usage logged to one CloudTrail. The key policy grants usage permissions to IAM roles in other accounts. During a security incident, disabling the CMK immediately prevents decryption of any data encrypted with it. Option A lacks centralization and requires 50 manual operations. Option C is incorrect - AWS-managed keys cannot be disabled or centrally controlled. Option D adds unnecessary complexity. Important: Key policies can limit which services and principals can use the key. For multi-account architectures, a centralized KMS key in a security account following least privilege is a best practice. Note: Disabled keys prevent decryption but don't delete data; re-enabling restores access."
        },
        {
          "id": "D2-T2.3-Q4",
          "question": "An API Gateway REST API exposes sensitive financial data. The security team requires: (1) Authentication via corporate Active Directory, (2) Authorization based on user groups, (3) Request throttling per user. Which API Gateway configuration meets all requirements?",
          "options": [
            "Use API Gateway Lambda authorizer validating AD credentials and returning user context with throttling limits",
            "Integrate with Amazon Cognito User Pools federated with AD via SAML, use Cognito groups for authorization, implement usage plans with API keys",
            "Use IAM authorization with IAM roles mapped to AD groups via federation",
            "Implement AWS WAF with rate-based rules for throttling and custom Lambda for AD authentication"
          ],
          "correctAnswer": 1,
          "explanation": "Cognito User Pools with AD federation provides the complete solution: (1) Federation with AD via SAML 2.0 - users authenticate against corporate AD, (2) Cognito groups mapped to AD groups - API Gateway can authorize based on groups in the JWT token, (3) Usage plans with API keys - API Gateway natively supports per-user throttling using usage plans. The flow: User authenticates via AD  Cognito issues JWT with groups  API Gateway validates JWT and checks groups  Usage plan enforces throttling. Option A (Lambda authorizer) can work but requires custom implementation of all logic. Option C (IAM) doesn't integrate well with per-user throttling and requires AWS credentials. Option D (WAF) doesn't provide authentication or per-user throttling granularity. API Gateway usage plans can set throttle and quota limits per API key. Best practice: Use Cognito User Pools for user management and API Gateway authorizers for validation."
        },
        {
          "id": "D2-T2.3-Q5",
          "question": "A container application uses AWS Secrets Manager to retrieve database passwords. Security audit reveals that containers running for days still have the initial password even after Secrets Manager rotation. How should the application be updated to use rotated credentials?",
          "options": [
            "Restart containers every 30 days to force credential refresh",
            "Implement application code to periodically call Secrets Manager GetSecretValue API to retrieve current credentials",
            "Use Secrets Manager rotation Lambda function to update environment variables in running containers",
            "Enable Secrets Manager automatic credential injection into containers"
          ],
          "correctAnswer": 1,
          "explanation": "Applications must actively retrieve secrets from Secrets Manager to get rotated values. Best practices: (1) Retrieve secrets at runtime on each database connection (or cache for short duration like 5 minutes), (2) Implement retry logic with exponential backoff if connection fails due to rotation in progress, (3) Use Secrets Manager caching libraries (AWS provides caching clients for multiple languages) to reduce API calls while ensuring freshness. Option A (restart containers) causes downtime and doesn't scale. Option C doesn't exist - rotation Lambda rotates secrets in Secrets Manager and the database, not container environment variables. Option D doesn't exist as an automatic feature. Common mistake: Retrieving secrets once at startup and caching indefinitely. Correct approach: Periodic retrieval or event-driven (using CloudWatch Events when rotation completes). For containers, sidecar pattern can handle secret retrieval and provide to application via localhost."
        },
        {
          "id": "D2-T2.3-Q6",
          "question": "A company runs a public web application that must pass PCI DSS compliance. They use Application Load Balancer with EC2 instances. Which security controls are REQUIRED for PCI DSS? (Select THREE)",
          "options": [
            "Use AWS WAF to protect against OWASP Top 10 vulnerabilities",
            "Implement encryption in transit using TLS 1.2 or higher on the ALB",
            "Enable AWS Shield Standard (automatically included) for DDoS protection",
            "Enable VPC Flow Logs to capture network traffic for audit",
            "Use Security Groups to restrict inbound traffic to only necessary ports (443 for HTTPS)",
            "Enable AWS Config to monitor security group configuration compliance"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            3,
            4
          ],
          "explanation": "PCI DSS requirements for AWS applications include: (1) Encryption in transit - TLS 1.2+ is required for transmitting cardholder data (PCI DSS requirement 4.1), (2) Network traffic logging - VPC Flow Logs provide audit trails of network access (PCI DSS requirement 10), (3) Network segmentation and access control - Security Groups implementing least privilege access (PCI DSS requirement 1). Option A (WAF) is highly recommended but not strictly required by PCI DSS, though it helps with several requirements. Option C (Shield Standard) is included automatically but isn't a PCI DSS requirement. Option F (Config) is recommended for compliance monitoring but not a core requirement. Other PCI DSS requirements include: encryption at rest, access controls, regular security testing, intrusion detection (GuardDuty), and centralized logging (CloudTrail). Use AWS PCI DSS compliance documentation and AWS Config conformance packs for PCI DSS to ensure all controls are implemented."
        },
        {
          "id": "D2-T2.3-Q7",
          "question": "A healthcare application on ECS Fargate must comply with HIPAA. Container images are stored in ECR. Which security measures ensure HIPAA compliance for container images? (Select TWO)",
          "options": [
            "Enable ECR image scanning to detect vulnerabilities in container images",
            "Use ECR lifecycle policies to retain images for 6 years for compliance",
            "Enable encryption at rest for ECR repositories using AWS KMS customer-managed keys",
            "Implement ECR cross-region replication for disaster recovery",
            "Use ECR pull-through cache for frequently accessed images",
            "Enable ECR tag immutability to prevent image tag overwrites"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            2
          ],
          "explanation": "HIPAA compliance for container images requires: (1) Image scanning - ECR image scanning (basic or enhanced with Inspector) detects vulnerabilities in images. HIPAA requires systems to be free from known vulnerabilities. Scanning on push and continuous scanning help maintain compliance. (2) Encryption at rest - HIPAA requires encryption of ePHI (electronic Protected Health Information). ECR supports encryption with KMS CMKs, providing audit trails of key usage. Option B (6-year retention) is excessive for images; HIPAA requires audit logs and data retention but not necessarily container images for 6 years. Option D (replication) supports business continuity but isn't a HIPAA requirement. Option E (pull-through cache) is for performance. Option F (tag immutability) is good practice but not a HIPAA requirement. Additional HIPAA requirements: access controls (IAM policies), audit logging (CloudTrail), network isolation (VPC), encryption in transit. AWS provides HIPAA-eligible services including ECR, ECS, and Fargate."
        },
        {
          "id": "D2-T2.3-Q8",
          "question": "A serverless application uses API Gateway with Lambda backend processing sensitive customer data. The security team requires request/response logging for audit while ensuring sensitive data in logs is not exposed. What is the MOST secure logging configuration?",
          "options": [
            "Enable API Gateway CloudWatch Logs with full request/response body logging",
            "Enable API Gateway CloudWatch Logs with INFO level logging (metadata only, no request/response bodies)",
            "Disable API Gateway logging and implement custom logging in Lambda functions only",
            "Enable API Gateway access logs to S3 with server-side encryption"
          ],
          "correctAnswer": 1,
          "explanation": "API Gateway CloudWatch Logs with INFO level provides: (1) Request metadata (timestamp, source IP, method, path, status codes) for audit, (2) No request/response bodies preventing sensitive data exposure in logs, (3) Integration with CloudWatch Insights for analysis. This balances audit requirements with data protection. Option A (full logging) risks exposing sensitive data (PII, passwords, tokens) in CloudWatch Logs. Even with encryption, principle of least privilege suggests not logging sensitive data. Option C (no API Gateway logs) loses valuable audit information about API access patterns, error rates, and request metadata. Option D (access logs to S3) provides similar information to INFO level but CloudWatch Logs offers better querying and alerting. Best practice: INFO level logging + custom application logging for business logic (without sensitive data) + AWS WAF logging for security events. Use CloudWatch Logs data protection to redact sensitive patterns if full logging is required."
        },
        {
          "id": "D2-T2.3-Q9",
          "question": "A company must implement defense in depth for their three-tier web application (ALB  EC2  RDS). Which security architecture provides multiple layers of protection? (Select THREE)",
          "options": [
            "AWS WAF on ALB with managed rules for SQL injection and XSS protection",
            "Network ACLs allowing only return traffic and explicit allow rules",
            "Security Groups: ALB allows 443 from internet, EC2 allows traffic only from ALB security group, RDS allows traffic only from EC2 security group",
            "GuardDuty for threat detection analyzing VPC Flow Logs and CloudTrail",
            "AWS Firewall Manager to centrally manage security groups",
            "VPC endpoint for S3 to prevent internet access"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            2,
            3
          ],
          "explanation": "Defense in depth requires multiple security layers: (1) WAF at application layer - protects against OWASP Top 10 (SQL injection, XSS, etc.) before requests reach the application, (2) Security Groups as micro-segmentation - each tier only accepts traffic from the tier above (ALB  Internet, EC2  ALB only, RDS  EC2 only), preventing lateral movement, (3) GuardDuty for threat detection - analyzes logs to detect compromises, unusual API calls, and malicious activity. Option B (NACLs) can add value but is less critical than the selected options; Security Groups are stateful and more manageable. Option E (Firewall Manager) is for centralized management, not an additional security layer. Option F (VPC endpoint) is good practice but doesn't directly protect the three-tier app. Additional layers: CloudTrail for API audit, AWS Config for configuration compliance, Systems Manager for patch management, Inspector for vulnerability scanning."
        },
        {
          "id": "D2-T2.3-Q10",
          "question": "An application uses client-side encryption before uploading objects to S3. The security team wants to ensure that any object uploaded to the bucket without client-side encryption is automatically rejected. How can this be enforced?",
          "options": [
            "Enable S3 default encryption with SSE-S3",
            "Create S3 bucket policy denying PutObject unless x-amz-server-side-encryption header is present",
            "Create S3 bucket policy denying PutObject unless x-amz-meta-client-encrypted metadata is present",
            "Use S3 Object Lock to prevent unencrypted uploads"
          ],
          "correctAnswer": 2,
          "explanation": "Client-side encryption means data is encrypted before reaching AWS. To enforce this, the application must indicate encryption using custom metadata. S3 bucket policy denying uploads without the custom metadata (e.g., x-amz-meta-client-encrypted=true) enforces the requirement: Policy condition: 'StringNotEquals': {'s3:x-amz-meta-client-encrypted': 'true'}. Option A (default encryption) is server-side encryption, doesn't validate client-side encryption. Option B checks for server-side encryption headers, not client-side. Option D (Object Lock) prevents deletion/modification, not related to encryption enforcement. Important: Client-side encryption provides strongest security (AWS never sees unencrypted data) but requires proper key management. Options: AWS Encryption SDK, S3 encryption client libraries. The application encrypts with a key (stored in KMS, locally, etc.) before upload. S3 bucket policy enforces the workflow but doesn't perform encryption."
        },
        {
          "id": "D2-T2.3-Q11",
          "question": "A company runs microservices on EKS and needs to implement mutual TLS (mTLS) authentication between services for zero-trust security. Which AWS service provides the MOST automated solution for certificate management and mTLS?",
          "options": [
            "Use AWS Certificate Manager (ACM) to issue certificates and manually distribute to pods",
            "Implement AWS App Mesh with TLS encryption and AWS Certificate Manager Private CA for automatic certificate rotation",
            "Use cert-manager on Kubernetes with Let's Encrypt for certificate issuance",
            "Manually generate certificates with OpenSSL and mount as Kubernetes secrets"
          ],
          "correctAnswer": 1,
          "explanation": "AWS App Mesh with ACM Private CA provides automated mTLS for microservices: (1) App Mesh Envoy proxies handle TLS termination/origination transparently, (2) ACM Private CA issues certificates for each service, (3) Automatic certificate rotation before expiry (no application downtime), (4) Centralized policy management for which services can communicate. App Mesh implements service mesh pattern with automatic mTLS enforcement. Option A (ACM) doesn't support automatic distribution to EKS pods; ACM is for load balancers. Option C (cert-manager + Let's Encrypt) works but requires more operational overhead and Let's Encrypt is for public certificates, not ideal for internal mTLS. Option D (manual OpenSSL) is operationally intensive with manual rotation. App Mesh also provides: observability (metrics, traces), traffic management (canary deployments), and circuit breaking. Alternative: Istio service mesh also supports mTLS but requires self-management vs App Mesh's AWS-managed control plane."
        },
        {
          "id": "D2-T2.3-Q12",
          "question": "A financial application requires that all S3 buckets have versioning enabled, access logging enabled, and default encryption. They want to prevent creation of non-compliant buckets across 100 AWS accounts. Which preventive control is MOST effective?",
          "options": [
            "AWS Config rules that detect non-compliant buckets and trigger automatic remediation",
            "Service Control Policy (SCP) denying s3:CreateBucket unless versioning, logging, and encryption are enabled",
            "CloudFormation StackSets deploying compliant bucket templates to all accounts",
            "AWS Security Hub with CIS AWS Foundations Benchmark checks"
          ],
          "correctAnswer": 1,
          "explanation": "SCP provides true preventive control stopping non-compliant bucket creation at the organization level. The SCP denies s3:CreateBucket API calls unless the request includes versioning, logging, and encryption configurations. This prevents human error and ensures compliance from creation. With September 2025 SCP enhancements supporting full IAM policy language, you can write: Deny s3:CreateBucket unless conditions check for s3:x-amz-server-side-encryption, versioning, and logging configurations. Option A (Config rules) is detective, not preventive - buckets are created first, then detected as non-compliant. Option C (StackSets) ensures compliant buckets but doesn't prevent manual creation of non-compliant ones. Option D (Security Hub) aggregates findings but doesn't prevent actions. Important: Combine preventive (SCPs) with detective (Config) controls. SCPs prevent violations; Config detects configuration drift if SCPs are bypassed (e.g., changes after creation)."
        },
        {
          "id": "D2-T2.3-Q13",
          "question": "A company uses Amazon Cognito User Pools for their mobile app authentication. They need to implement step-up authentication requiring MFA for sensitive operations (money transfer) but not for viewing account balance. How should this be implemented?",
          "options": [
            "Enable MFA required for all users in Cognito User Pool settings",
            "Use Cognito User Pool Lambda triggers to challenge users for MFA based on the operation",
            "Implement custom application logic checking for MFA in ID tokens, requesting MFA challenge when needed",
            "Create separate Cognito User Pools for high-security and low-security operations"
          ],
          "correctAnswer": 2,
          "explanation": "Step-up authentication requires application-level logic: (1) Cognito issues ID tokens containing authentication_time and amr (authentication methods reference) claims, (2) For sensitive operations, application checks if MFA was used recently (amr contains 'mfa'), (3) If not, application calls Cognito's GetSession or InitiateAuth with required MFA, challenging the user, (4) After successful MFA, new tokens include MFA claim. This allows operation-specific security. Option A (always require MFA) forces MFA for all operations, poor UX. Option B (Lambda triggers) can customize auth flows but doesn't specifically support operation-based MFA challenges; triggers are for auth flow customization, not post-auth operation validation. Option D (separate pools) is overly complex. Implementation: Check ID token amr and auth_time claims. If MFA wasn't used or auth_time is too old, call InitiateAuth with AUTH_FLOW: 'CUSTOM_AUTH' or use Cognito API to request MFA. This pattern is common in financial apps: basic operations with password only, sensitive operations require recent MFA."
        },
        {
          "id": "D2-T2.3-Q14",
          "question": "A company migrates to AWS and must implement data sovereignty requirements ensuring EU customer data never leaves EU regions. They use S3 for storage and Lambda for processing. What controls ensure compliance? (Select THREE)",
          "options": [
            "Use S3 Block Public Access to prevent data exfiltration",
            "Implement S3 bucket policies restricting replication and actions to EU regions only",
            "Use SCP denying operations in non-EU regions for accounts handling EU data",
            "Enable S3 Object Lock for data protection",
            "Configure Lambda functions with VPC endpoints in EU regions",
            "Use AWS Organizations to create an OU for EU accounts with geographical restrictions"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            2,
            5
          ],
          "explanation": "Data sovereignty requires multi-layer geographical restrictions: (1) S3 bucket policies restricting cross-region replication and denying PutObject/GetObject from non-EU regions using aws:RequestedRegion condition, (2) SCPs at organization level denying API calls to non-EU regions for accounts handling EU data - prevents accidental or intentional data movement, (3) Organizational structure with dedicated OU for EU accounts enabling policy-based enforcement and clear data boundaries. Option A (Block Public Access) prevents public internet access but doesn't enforce regional restrictions. Option D (Object Lock) prevents deletion/modification, not regional restrictions. Option E (VPC endpoints) are for private connectivity, not data sovereignty. Additional controls: IAM policies restricting S3 GetObject to EU IPs, CloudTrail monitoring for non-EU API calls, Config rules validating all resources in EU regions. Example SCP condition: 'StringNotEquals': {'aws:RequestedRegion': ['eu-west-1', 'eu-central-1']}."
        },
        {
          "id": "D2-T2.3-Q15",
          "question": "An application uses AWS Secrets Manager to store API keys for third-party services. The security team discovers that IAM users are retrieving secrets using GetSecretValue and exfiltrating them. Which additional security control prevents unauthorized secret retrieval?",
          "options": [
            "Enable CloudTrail logging for Secrets Manager API calls",
            "Implement resource-based policy on secrets restricting access to specific IAM roles used by applications, not users",
            "Enable Secrets Manager automatic rotation to invalidate exfiltrated secrets",
            "Use AWS CloudWatch Alarms to detect unusual GetSecretValue API calls"
          ],
          "correctAnswer": 1,
          "explanation": "Resource-based policies on secrets provide least privilege access control: Only allow GetSecretValue from specific IAM roles (e.g., ECS task roles, Lambda execution roles), explicitly deny IAM users. Policy example: {'Effect': 'Deny', 'Principal': {'AWS': '*'}, 'Action': 'secretsmanager:GetSecretValue', 'Condition': {'StringNotLike': {'aws:PrincipalArn': 'arn:aws:iam::*:role/AllowedAppRoles*'}}}. This prevents direct user access while allowing application roles. Option A (CloudTrail) is detective, not preventive - logs the access but doesn't stop it. Option C (rotation) helps limit damage but doesn't prevent retrieval. Option D (CloudWatch Alarms) is also detective. Best practice: Applications should use IAM roles, not users. Secrets Manager permissions should be granted only to application roles. Use session tags and ABAC for fine-grained control. Monitor CloudTrail for GetSecretValue calls from unexpected principals. Combine preventive (resource policies) with detective (CloudTrail, alarms) controls."
        },
        {
          "id": "D2-T2.3-Q16",
          "question": "A company runs workloads requiring FIPS 140-2 validated cryptographic modules. Which AWS services and configurations provide FIPS 140-2 compliance? (Select TWO)",
          "options": [
            "Use AWS KMS in FIPS endpoints (kms-fips.region.amazonaws.com) for encryption operations",
            "Configure S3 to use FIPS 140-2 validated encryption modules",
            "Use CloudHSM which provides FIPS 140-2 Level 3 validated hardware security modules",
            "Enable FIPS mode in EC2 instances using AWS-provided AMIs",
            "Use ACM certificates which are automatically FIPS compliant",
            "Configure RDS encryption which uses FIPS modules automatically"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            2
          ],
          "explanation": "FIPS 140-2 compliance in AWS requires: (1) AWS KMS FIPS endpoints - KMS uses FIPS 140-2 validated cryptographic modules, but applications must connect to FIPS endpoints (kms-fips.region.amazonaws.com) to ensure FIPS mode, (2) CloudHSM - provides FIPS 140-2 Level 3 validated HSMs for customer-exclusive cryptographic operations, suitable for workloads requiring dedicated HSMs. Option B is incorrect - S3 encryption uses KMS or S3-managed keys; you ensure FIPS by using KMS FIPS endpoints. Option D is partially true - you can configure FIPS mode on Linux instances, but this is OS-level, not AWS-provided. Option E is incorrect - ACM uses cryptographic modules but you don't configure FIPS mode for ACM. Option F is incorrect - RDS encryption uses KMS; for FIPS compliance, the application connecting to RDS should use FIPS endpoints and encrypted connections. Many AWS services support FIPS endpoints including S3, DynamoDB, and others. List available at: https://aws.amazon.com/compliance/fips/."
        }
      ]
    },
    {
      "filename": "domain-2-task-2.4-reliability.json",
      "domain": "Domain 2: Design for New Solutions",
      "task": "Task 2.4: Reliability Requirements",
      "question_count": 16,
      "questions": [
        {
          "id": "D2-T2.4-Q1",
          "question": "A web application experiences variable traffic patterns with daily spikes from 100 to 5000 requests per minute. Historical data shows traffic spikes occur predictably at 9 AM and 6 PM daily. Which Auto Scaling configuration minimizes costs while maintaining performance?",
          "options": [
            "Target tracking scaling policy maintaining 50% CPU utilization",
            "Predictive scaling policy analyzing 14 days of historical metrics to forecast and scale ahead of predicted load",
            "Scheduled scaling actions at 8:50 AM and 5:50 PM to pre-scale capacity",
            "Step scaling policy with multiple thresholds for different load levels"
          ],
          "correctAnswer": 1,
          "explanation": "Predictive scaling (available as of 2025 in most regions) uses machine learning to analyze up to 14 days of CloudWatch metrics and forecast capacity needs for the next 48 hours. For workloads with cyclical patterns (daily spikes at 9 AM/6 PM), predictive scaling: (1) Scales ahead of forecasted load (proactive vs reactive), (2) Provides smoother scaling than reactive policies, (3) Reduces the lag between demand increase and capacity availability. Predictive scaling can operate in 'Forecast Only' mode (test forecasts) or 'Forecast and Scale' mode (actually scale). Option C (scheduled scaling) works but requires manual maintenance and doesn't adapt to changing patterns. Option A (target tracking) is reactive - waits for CPU to reach 50% before scaling, causing performance degradation during rapid spikes. Option D (step scaling) is also reactive. Best practice: Combine predictive scaling for cyclical patterns with target tracking or step scaling for unexpected spikes beyond forecasted levels."
        },
        {
          "id": "D2-T2.4-Q2",
          "question": "An e-commerce application uses Application Load Balancer with Auto Scaling. During deployments, new instances pass ALB health checks before completing application initialization, causing errors for requests routed to them. What configuration prevents traffic to incompletely initialized instances?",
          "options": [
            "Increase ALB health check interval and unhealthy threshold",
            "Enable connection draining with 300-second timeout",
            "Configure Auto Scaling lifecycle hooks to delay instance in-service state until application initialization completes",
            "Use ALB health check path pointing to a lightweight endpoint"
          ],
          "correctAnswer": 2,
          "explanation": "Auto Scaling lifecycle hooks pause instance launch at a defined point (Pending:Wait state), allowing custom actions before the instance enters service. Implementation: (1) Create lifecycle hook for instance launch, (2) Application signals completion using complete-lifecycle-action API or CloudWatch Event after initialization, (3) Only then does instance enter InService state and receive ALB traffic. This ensures full application readiness before traffic. Option A (longer health checks) delays detection of unhealthy instances but doesn't prevent initial traffic to unready instances. Option B (connection draining) handles in-flight requests during instance termination, not launch. Option D (lightweight health check) makes the problem worse - instance passes health check faster while still initializing. Alternative: Use ALB health check path requiring complete initialization (e.g., /health that verifies all dependencies), but lifecycle hooks provide better control. Lifecycle hooks support up to 2-hour wait time. Use SNS/SQS notifications to trigger initialization workflows."
        },
        {
          "id": "D2-T2.4-Q3",
          "question": "A video processing application uses SQS queue feeding Lambda functions. Processing a video takes 3 minutes on average, but the Lambda timeout is 15 minutes. Occasionally, messages are processed multiple times causing duplicate video outputs. What is the ROOT cause and solution?",
          "options": [
            "SQS visibility timeout (30 seconds default) is too short; increase to 15 minutes (6x max Lambda timeout)",
            "Lambda is not deleting messages after processing; add explicit DeleteMessage call",
            "SQS message retention is too long; reduce retention period",
            "Use SQS FIFO queue instead of standard queue to prevent duplicates"
          ],
          "correctAnswer": 0,
          "explanation": "SQS visibility timeout controls how long a message is invisible after a consumer receives it. If processing isn't complete before visibility timeout expires, the message becomes visible again and another consumer receives it, causing duplicate processing. For Lambda: (1) Lambda automatically manages message deletion on successful completion, (2) Visibility timeout must be >= 6x Lambda timeout to account for retries (Lambda retries twice on failure), (3) For 15-minute Lambda timeout, visibility timeout should be 15  6 = 90 minutes (maximum allowed is 12 hours). Option B is incorrect - Lambda event source mapping automatically deletes messages on successful processing. Option C (retention) controls how long messages stay in queue if unprocessed, not related to duplicates. Option D (FIFO) provides exactly-once processing within a 5-minute deduplication window but adds complexity and has lower throughput (3000 msg/sec vs standard's unlimited). Solution: Set visibility timeout = Lambda timeout  6 in SQS queue configuration or event source mapping."
        },
        {
          "id": "D2-T2.4-Q4",
          "question": "A microservices application uses Network Load Balancer distributing traffic to services running on EC2 instances across three AZs. They want to ensure that if an entire AZ fails, traffic is only routed to healthy AZs. Which NLB configuration provides this?",
          "options": [
            "Enable cross-zone load balancing on the NLB",
            "Disable cross-zone load balancing and configure health checks with fast failover",
            "Use Route 53 health checks monitoring each AZ's NLB node",
            "Configure NLB with target group health checks and deregistration delay of 0 seconds"
          ],
          "correctAnswer": 1,
          "explanation": "With NLB cross-zone load balancing DISABLED: Each NLB node (one per AZ) routes traffic only to targets in its own AZ. If an AZ fails: (1) The NLB node in that AZ becomes unavailable, (2) DNS/routing directs traffic to NLB nodes in healthy AZs, (3) Those nodes route to targets in their respective (healthy) AZs only. This prevents failed AZ impact. With cross-zone enabled (Option A), NLB nodes route to targets in all AZs - if an AZ fails, healthy AZ nodes try to route to failed AZ targets (causing delays/failures until health checks mark them unhealthy). Option C (Route 53) adds complexity and slower failover than NLB's built-in AZ isolation. Option D (deregistration delay) controls connection draining duration, not AZ failure handling. Trade-off: Disabling cross-zone can cause uneven traffic distribution if AZs have unequal target counts. Best practice for AZ independence: Disable cross-zone for equal targets per AZ; enable for uneven distribution accepting slower AZ failure detection."
        },
        {
          "id": "D2-T2.4-Q5",
          "question": "A financial application requires that all requests are processed exactly once with strong ordering guarantees. Messages are published from multiple sources. Which combination of AWS services meets these requirements MOST cost-effectively?",
          "options": [
            "SQS FIFO queue with content-based deduplication and message group ID",
            "SQS standard queue with application-level deduplication logic",
            "Kinesis Data Streams with consumer tracking of sequence numbers",
            "EventBridge with DLQ for failed deliveries"
          ],
          "correctAnswer": 0,
          "explanation": "SQS FIFO queue provides: (1) Exactly-once processing - automatic deduplication within 5-minute window using message deduplication ID, (2) Strict ordering - messages in same message group ID are delivered in order, (3) Content-based deduplication - uses SHA-256 hash of message body as deduplication ID (no need to set manually), (4) Support for multiple message groups - different groups can be processed in parallel while maintaining order within each group. This is purpose-built for the requirement. Option B (standard queue) provides at-least-once delivery (duplicates possible) requiring complex application logic for deduplication. Option C (Kinesis) provides ordering within a shard and at-least-once delivery, but requires managing checkpoints and is more complex/expensive for simple queue use cases. Option D (EventBridge) provides at-least-once delivery. FIFO queue limitations: 3000 messages/second (with batching, 300 ops/second otherwise), higher cost than standard queue. Use message group ID strategically for parallel processing while maintaining order."
        },
        {
          "id": "D2-T2.4-Q6",
          "question": "A latency-sensitive application uses Application Load Balancer with connection draining. During deployments, users experience intermittent errors. Analysis shows the application gracefully handles in-flight requests but the default 300-second deregistration delay is too long. What configuration optimizes deployments?",
          "options": [
            "Reduce deregistration delay to match application's maximum request duration (e.g., 60 seconds)",
            "Increase deregistration delay to 900 seconds for safer draining",
            "Set deregistration delay to 0 seconds for fastest deployment",
            "Use connection draining with sticky sessions to maintain user connections"
          ],
          "correctAnswer": 0,
          "explanation": "Deregistration delay (connection draining) controls how long the ALB waits before fully deregistering a target. During this time: (1) No new connections are sent to the target, (2) Existing connections are allowed to complete, (3) After the delay expires, connections are forcibly closed. Optimal configuration: Set delay slightly longer than longest expected request duration. If application requests complete within 60 seconds, set delay to 60-90 seconds. This minimizes deployment time while preventing request interruption. Option B (900 seconds) unnecessarily prolongs deployments. Option C (0 seconds) immediately closes connections causing errors. Option D conflates two concepts - sticky sessions maintain affinity but don't affect draining duration. Best practice: Monitor CloudWatch metrics for request duration, set deregistration delay to 95th percentile request duration + buffer. For websocket applications, delay should account for longest session duration. Range: 0-3600 seconds."
        },
        {
          "id": "D2-T2.4-Q7",
          "question": "A data processing application uses Lambda functions triggered by Kinesis Data Streams. During high-volume periods, Lambda throttles occur causing processing delays. The application can tolerate 5-minute processing delays. Which configuration improves reliability? (Select TWO)",
          "options": [
            "Increase Lambda concurrent execution limit (reserved concurrency)",
            "Enable Kinesis Enhanced Fan-Out for dedicated throughput per Lambda consumer",
            "Configure Lambda event source mapping with batch size of 10,000",
            "Enable Lambda function parallelization factor on the event source mapping",
            "Increase Kinesis shard count to handle higher throughput",
            "Configure Lambda retry attempts to 0 to prevent duplicate processing"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            3
          ],
          "explanation": "For Lambda + Kinesis reliability: (1) Enhanced Fan-Out provides 2 MB/sec dedicated read throughput per consumer (vs shared 2 MB/sec per shard with standard iterators), preventing consumer competition and read throttling, (2) Parallelization factor (1-10) allows multiple Lambda invocations processing from the same shard simultaneously, increasing processing throughput. With factor=10, one shard can invoke up to 10 Lambdas concurrently. These address throughput limitations. Option A (reserved concurrency) helps if Lambda concurrency is the bottleneck but not if Kinesis read is the issue. Option C (large batch) increases efficiency but max batch size is 10,000; doesn't solve throttling. Option E (more shards) helps but is more expensive than Enhanced Fan-Out. Option F (0 retries) causes data loss on failures. Enhanced Fan-Out costs more but critical for multiple consumers. Parallelization factor requires Lambda functions to be idempotent (out-of-order processing possible within shard)."
        },
        {
          "id": "D2-T2.4-Q8",
          "question": "A global application uses CloudFront with multiple origin endpoints (US, EU, APAC) based on viewer geography. During regional failures, requests should failover to the nearest healthy origin. Which CloudFront configuration achieves this?",
          "options": [
            "Use CloudFront origin groups with primary and secondary origins, configuring failover status codes (5xx, 4xx)",
            "Use Route 53 latency-based routing behind CloudFront origins",
            "Configure CloudFront with Lambda@Edge selecting origin based on viewer location and health",
            "Use CloudFront origin access control with health check monitoring"
          ],
          "correctAnswer": 0,
          "explanation": "CloudFront origin groups provide native origin failover: (1) Create origin group with primary and secondary origins, (2) Configure failover criteria (HTTP status codes: 500, 502, 503, 504, 403, 404 - customizable), (3) If primary origin returns failover status code, CloudFront automatically tries secondary origin, (4) Can nest multiple origin groups for multi-level failover. For geographic distribution: Create behavior patterns routing based on headers/paths to different origin groups, each with primary/secondary. Option B (Route 53) behind origins works but adds DNS propagation delay; CloudFront origin groups provide faster failover at edge. Option C (Lambda@Edge) can implement custom logic but is more complex and costly than native origin groups. Option D misunderstands - origin access control (OAC) is for securing S3 origins, not failover. Origin groups check pattern: Primary fails (5xx)  Try secondary  If secondary fails  Return error to viewer. Use CloudWatch metrics to monitor origin health and failover events."
        },
        {
          "id": "D2-T2.4-Q9",
          "question": "A mobile application backend uses DynamoDB with provisioned capacity. Traffic is unpredictable with occasional spikes to 10x normal throughput lasting 5-10 minutes. They want to avoid throttling during spikes without over-provisioning. What is the MOST cost-effective solution?",
          "options": [
            "Switch to DynamoDB on-demand capacity mode",
            "Keep provisioned mode and enable DynamoDB auto scaling with target utilization of 70%",
            "Keep provisioned mode and enable DynamoDB burst capacity (automatic)",
            "Increase provisioned capacity to 10x normal throughput"
          ],
          "correctAnswer": 1,
          "explanation": "DynamoDB auto scaling with provisioned capacity provides: (1) Automatically adjusts provisioned capacity based on actual utilization, (2) Scales up when utilization exceeds target (70% default), (3) Scales down during low usage, saving costs vs fixed high capacity, (4) Can handle spikes up to 2x current capacity using burst capacity while auto scaling adjusts, (5) More cost-effective than on-demand for predictable baseline with occasional spikes. Option A (on-demand) works but is more expensive if you have a predictable baseline throughput; on-demand costs ~5x more per request. Use on-demand for truly sporadic, unpredictable traffic. Option C (burst capacity) is automatic but limited to 300 seconds of unused capacity; insufficient for sustained spikes. Option D (10x provisioning) wastes money during normal periods. Best practice: Provisioned + auto scaling for workloads with identifiable baseline; on-demand for highly variable/unpredictable workloads with no baseline. Auto scaling can take a few minutes to adjust, hence target utilization of 70% provides buffer."
        },
        {
          "id": "D2-T2.4-Q10",
          "question": "A serverless application uses API Gateway invoking Lambda functions. During sudden traffic spikes, Lambda concurrent execution limit is reached, causing 429 throttling errors. The application can handle 500 concurrent requests maximum. What configuration prevents service degradation?",
          "options": [
            "Enable API Gateway throttling at 500 requests per second",
            "Configure Lambda reserved concurrency of 500 for the function",
            "Enable API Gateway caching to reduce Lambda invocations",
            "Use Lambda provisioned concurrency of 500"
          ],
          "correctAnswer": 1,
          "explanation": "Lambda reserved concurrency guarantees that exactly that amount of concurrency is available for the function and prevents it from using more (protecting other functions/accounts from noisy neighbor). Setting reserved concurrency of 500: (1) Ensures the function can handle 500 concurrent requests, (2) Prevents exceeding capacity (which would cause failures), (3) Protects account-level concurrency for other functions. Important: Regional concurrent execution limit is 1000 by default (can be increased). Reserved concurrency allocates from this pool. Option A (API Gateway throttling) limits requests per second, not concurrency. RPS and concurrency are different: RPS  duration = concurrency. Option C (caching) reduces load but doesn't guarantee capacity. Option D (provisioned concurrency) pre-warms functions for low latency but doesn't limit concurrency - can still exceed capacity and throttle. Reserved concurrency = ceiling; provisioned concurrency = warm floor. Best practice: Set reserved concurrency to protect critical functions; use provisioned concurrency for latency-sensitive functions."
        },
        {
          "id": "D2-T2.4-Q11",
          "question": "A real-time analytics application processes streaming data from Kinesis Data Streams using Lambda. They observe that during processing failures, the same records are retried multiple times, then eventually moved to the Dead Letter Queue (DLQ). They want failed records to be retried with exponential backoff before moving to DLQ. How should this be configured?",
          "options": [
            "Configure Lambda event source mapping with maximum retry attempts and maximum record age",
            "Implement exponential backoff logic in the Lambda function code",
            "Configure Lambda destination for on-failure events pointing to SQS for retry logic",
            "Use Kinesis Data Streams retention period to allow re-processing"
          ],
          "correctAnswer": 0,
          "explanation": "Lambda event source mapping for streams (Kinesis, DynamoDB Streams) provides configurable retry behavior: (1) Maximum retry attempts - how many times to retry failed batches (-1 = retry until record expires or processed), (2) Maximum record age - discard records older than this (seconds), (3) On-failure destination - where to send records after retries exhausted, (4) Bisect on function error - split failed batches to isolate bad records. Lambda implements exponential backoff automatically between retries. Configuration ensures: Records are retried with backoff, old records don't retry forever (record age), failed records go to DLQ after max attempts. Option B (code-level backoff) doesn't apply - Lambda retries are automatic. Option C (destinations) can send failure info but doesn't configure retry logic. Option D (retention) keeps records in stream but doesn't control retry behavior. Best practice: Set max record age to prevent retrying very old data; set max retry attempts based on failure tolerance; use bisect on error to isolate poisonous messages."
        },
        {
          "id": "D2-T2.4-Q12",
          "question": "A multi-tier application uses Auto Scaling groups for web and application tiers. During scale-in events, instances are terminated immediately even though connections are still active. What configuration ensures graceful shutdown? (Select TWO)",
          "options": [
            "Configure Auto Scaling lifecycle hooks to delay termination, allowing application to finish processing",
            "Enable connection draining on the load balancer with appropriate timeout",
            "Set Auto Scaling termination policy to OldestInstance",
            "Implement application-level shutdown logic responding to SIGTERM signals",
            "Use Auto Scaling scheduled actions to prevent scale-in during business hours",
            "Configure health check grace period to delay termination"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            3
          ],
          "explanation": "Graceful shutdown requires: (1) Auto Scaling lifecycle hook (terminating:wait) - pauses termination, allowing custom logic. Application completes requests, saves state, then signals completion via complete-lifecycle-action API. Default timeout is 1 hour (max: 48 hours for scale-in). (2) Application handles SIGTERM - when EC2 receives shutdown, it sends SIGTERM to processes. Application should catch this signal, stop accepting new work, complete in-flight work, and exit gracefully. Combined approach: Lifecycle hook provides time, application handles signal properly. Option B (connection draining) is ALB feature for deregistration, applies when instance is removed from target group but doesn't delay Auto Scaling termination. Option C (termination policy) selects which instance to terminate, doesn't affect graceful shutdown. Option E (scheduled actions) is a workaround, not a solution. Option F (health check grace period) prevents premature health check failures during launch, not relevant to termination. Best practice: Lifecycle hook + SIGTERM handling + ELB connection draining for complete graceful shutdown."
        },
        {
          "id": "D2-T2.4-Q13",
          "question": "An application uses EventBridge to route events from multiple sources to various targets (Lambda, SQS, SNS). During outages, some events are lost. They need guaranteed event delivery with the ability to replay events for up to 30 days after failures are resolved. What should they implement?",
          "options": [
            "Enable EventBridge Archive for the event bus, configure retention of 30 days, and use Replay to reprocess events after outage resolution",
            "Configure EventBridge to use SQS as a target with 14-day message retention",
            "Enable CloudTrail to log all events for replay",
            "Use EventBridge global endpoints for automatic failover"
          ],
          "correctAnswer": 0,
          "explanation": "EventBridge Archive and Replay provides event sourcing capabilities: (1) Archive captures all events matching filter patterns (or all events), (2) Stores events for specified retention (up to indefinite), (3) Replay allows reprocessing archived events to configured targets, (4) Useful for disaster recovery, testing, and auditing. After resolving outages: Create a replay specifying time range, EventBridge reprocesses archived events to targets. Option B (SQS) only provides 14-day retention (extended queue) and doesn't help with events already delivered and failed. Option C (CloudTrail) logs API calls, not application events in EventBridge. Option D (global endpoints) provides regional failover but doesn't solve event replay after resolution. Archive configuration: Specify event pattern filter (archive specific events or all), retention period, and encryption. Replay: Select archive, time window, and destination event bus. Events are replayed in order with original timestamps preserved but delivery occurs at replay time. Use case: After fixing bug in Lambda consumer, replay last 24 hours of events."
        },
        {
          "id": "D2-T2.4-Q14",
          "question": "A latency-sensitive application requires database query latency under 10ms with high throughput. The application reads heavily (90% reads, 10% writes) with strong consistency requirements. Which database solution meets these requirements?",
          "options": [
            "DynamoDB with DynamoDB Accelerator (DAX) using strongly consistent reads",
            "ElastiCache for Redis with cluster mode enabled and read replicas",
            "Aurora MySQL with read replicas using read endpoints",
            "RDS PostgreSQL with Multi-AZ and read replicas"
          ],
          "correctAnswer": 0,
          "explanation": "DynamoDB + DAX provides: (1) DAX caches DynamoDB reads with microsecond latency (well under 10ms), (2) Supports both eventually consistent and strongly consistent reads from DAX cache, (3) Write-through cache automatically updated on writes, (4) Scales to millions of requests per second. For strongly consistent reads, DAX uses consistent read against DynamoDB, caching the result. Option B (Redis) achieves low latency but doesn't natively provide strong consistency with cross-region scenarios (though single-region reads from primary are consistent). Option C (Aurora read replicas) provides read scaling but replica lag means replicas have eventual consistency; only primary provides strong consistency. Option D (RDS PostgreSQL) similar issue plus higher latency than DAX. Trade-off: DAX strongly consistent reads have slightly higher latency than eventually consistent reads (still sub-10ms) because they bypass certain cache layers. DAX cluster: Primary node handles writes, multiple read replicas for read scaling. Use TTL settings to control cache freshness. For pure cache use case without DynamoDB table, use ElastiCache; DAX is optimized for DynamoDB acceleration."
        },
        {
          "id": "D2-T2.4-Q15",
          "question": "A video streaming application uses CloudFront with S3 origin. During popular live events, the S3 origin returns 503 errors due to request rate exceeding S3 limits. What architecture change prevents origin overload?",
          "options": [
            "Enable S3 Transfer Acceleration to handle higher request rates",
            "Configure CloudFront with origin shield to reduce requests to S3 origin",
            "Use multiple S3 buckets with CloudFront origin groups for load distribution",
            "Enable S3 request rate performance optimization with randomized key prefixes"
          ],
          "correctAnswer": 1,
          "explanation": "CloudFront Origin Shield acts as an additional caching layer between edge locations and origin: (1) All requests from all edge locations in a region go through Origin Shield, (2) Origin Shield consolidates requests, significantly reducing load on origin, (3) Improves cache hit ratio - if one edge requests content, all edges benefit from Shield's cache, (4) Reduces origin's request rate by 40-80% typically. Perfect for S3 origins with rate limit concerns. Option A (Transfer Acceleration) is for faster uploads to S3, not for request rate handling. Option C (multiple buckets) adds operational complexity; S3 auto-scales but Shield is simpler solution. Option D (randomized prefixes) helps distribute requests across S3 partitions but Origin Shield provides better protection. Origin Shield cost: Per 10,000 requests + per GB data transfer. Enable in CloudFront origin settings by selecting Origin Shield region (usually same as origin region). Best practice: Use Origin Shield for origins with limited request capacity or high cost per request (compute origins, third-party APIs)."
        },
        {
          "id": "D2-T2.4-Q16",
          "question": "A financial application must guarantee that SNS notifications are delivered to all subscribers even during service interruptions. Some subscribers are SQS queues, others are Lambda functions, and some are HTTPS endpoints. Which configuration ensures reliable delivery? (Select TWO)",
          "options": [
            "Configure SNS subscription filter policies to ensure relevant messages reach each subscriber",
            "Enable SNS DLQ for each subscription to capture failed deliveries for retry",
            "Use SNS message archiving to store all messages for 30 days",
            "Configure SNS delivery retry policies with exponential backoff for HTTPS endpoints",
            "Replace SNS with EventBridge for guaranteed delivery",
            "Enable SNS FIFO topics for ordered delivery"
          ],
          "type": "multiple",
          "correctAnswer": [
            1,
            3
          ],
          "explanation": "SNS reliable delivery requires: (1) Dead Letter Queue (DLQ) per subscription - when deliveries fail after retries, messages go to DLQ (SQS queue) where they can be processed after resolving issues. DLQ prevents message loss. (2) Delivery retry policies - SNS automatically retries failed deliveries with exponential backoff. For HTTPS endpoints, configure retry policy parameters (number of retries, min/max delay, backoff function). These ensure delivery attempts continue, with DLQ catching ultimate failures. Option A (filter policies) routes messages but doesn't improve reliability. Option C doesn't exist - SNS doesn't have message archiving (EventBridge has Archive). Option E (EventBridge) provides archival/replay but SNS is sufficient for this use case with proper DLQ config. Option F (FIFO topics) provides ordering but doesn't improve delivery reliability over standard topics. SNS delivery guarantees by endpoint type: SQS/Lambda (high durability, automatic retries), HTTPS (best-effort, configure retries), SMS/Email (best-effort). Always configure DLQ for critical subscriptions. Monitor DLQ depth in CloudWatch."
        }
      ]
    },
    {
      "filename": "domain-3-task-3.1-operational-excellence.json",
      "domain": "Domain 3: Continuous Improvement for Existing Solutions",
      "task": "Task 3.1: Operational Excellence",
      "question_count": 12,
      "questions": [
        {
          "id": "D3-T3.1-Q1",
          "question": "A company needs to query application logs across 100 AWS accounts to troubleshoot a distributed transaction failure. Logs are stored in CloudWatch Logs in each account. Which approach provides the FASTEST query capability across all accounts?",
          "options": [
            "Use CloudWatch Logs Insights with cross-account cross-region functionality",
            "Export all logs to S3 and query with Athena",
            "Use CloudWatch Logs subscription filters sending to centralized Kinesis Data Streams",
            "Manually query each account's CloudWatch Logs individually"
          ],
          "correctAnswer": 0,
          "explanation": "CloudWatch Logs Insights supports cross-account, cross-region queries directly. You can select multiple log groups across accounts and regions in a single query, making it the fastest approach for ad-hoc troubleshooting. Setup requires: (1) Create a monitoring account, (2) Set up resource links in each source account, (3) Query from the monitoring account selecting all relevant log groups. Logs Insights uses a SQL-like query language with automatic field discovery. Option B (S3 + Athena) has export delay and requires data to be in S3 first - slower for real-time troubleshooting. Option C (Kinesis) is for streaming processing, not interactive queries. Option D is impractical for 100 accounts. CloudWatch Logs Insights pricing is per GB scanned, making it cost-effective for targeted queries. Use saved queries and dashboards for recurring analysis. The cross-account feature simplifies centralized monitoring without complex ETL pipelines."
        },
        {
          "id": "D3-T3.1-Q2",
          "question": "An application uses X-Ray for distributed tracing. The development team reports that trace data for failed requests is incomplete, missing segments from downstream Lambda functions. What is the MOST likely cause and solution?",
          "options": [
            "X-Ray sampling rate is too low; increase to 100% for all requests",
            "Lambda functions don't have X-Ray tracing enabled; enable active tracing on Lambda functions",
            "X-Ray SDK is not initialized in Lambda code; add X-Ray SDK initialization",
            "IAM role for Lambda lacks xray:PutTraceSegments permission"
          ],
          "correctAnswer": 1,
          "explanation": "Lambda requires explicit enablement of X-Ray active tracing either: (1) In Lambda console/CLI with TracingConfig mode: Active, or (2) Via infrastructure as code (CloudFormation, SAM, CDK). Without active tracing enabled, Lambda doesn't send trace segments to X-Ray even if the X-Ray SDK is in the code. When active tracing is enabled, Lambda automatically: patches HTTP requests, sends trace data, provides environment variables (AWS_XRAY_DAEMON_ADDRESS). Option A (sampling) wouldn't cause missing segments; sampling decisions are made at request entry - if a request is traced, all segments should appear. Option C is incorrect because Lambda's active tracing mode automatically instruments common libraries without SDK initialization (though SDK provides more features). Option D would cause permission errors logged in CloudWatch, not silently missing segments. Best practice: Enable active tracing on all Lambda functions in distributed traces, use X-Ray SDK for custom subsegments and annotations. Check X-Ray service map to visualize request flow and identify missing components."
        },
        {
          "id": "D3-T3.1-Q3",
          "question": "A company wants to automate patching of 500 EC2 instances across multiple accounts, with different maintenance windows for production (Sundays 2AM) and development (daily 2AM). Failed patches should trigger alerts. Which solution provides the MOST operationally efficient approach?",
          "options": [
            "Use Systems Manager Patch Manager with patch baselines, maintenance windows, and SNS notifications for compliance",
            "Create Lambda functions with CloudWatch Events (EventBridge) to trigger yum/apt update commands",
            "Manually apply patches during maintenance windows using SSH",
            "Use third-party patch management tools integrated with AWS"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Systems Manager Patch Manager provides comprehensive, native patch management: (1) Patch baselines define which patches to install (security, critical, all), (2) Maintenance windows specify when patching occurs with different schedules per environment, (3) Run commands execute patching across instance groups (tags), (4) Patch compliance reporting shows which instances are compliant, (5) SNS integration for alerts on patch failures, (6) AWS-managed patch baselines updated by AWS as new patches release, (7) Support for cross-account patching with Organizations integration. For this scenario: Create maintenance window for production (Sunday 2AM, tagged Env=prod) and development (daily 2AM, tagged Env=dev). Associate patch baseline (e.g., AWS-DefaultPatchBaseline). Patch Manager handles orchestration, reboot, and compliance reporting. Option B (Lambda + EventBridge) requires custom code for patching logic, error handling, and reporting. Option C is not scalable or automatable. Option D adds cost and complexity. Patch Manager also supports custom patch baselines, testing patches before production (install-override list), and integration with Change Manager for change approval workflows."
        },
        {
          "id": "D3-T3.1-Q4",
          "question": "An operations team receives hundreds of CloudWatch Alarms daily, many from transient issues that self-resolve. They want alarms only when multiple related metrics indicate a real problem (e.g., high CPU AND high error rate AND high latency). Which CloudWatch feature addresses this?",
          "options": [
            "Create composite alarms combining multiple alarms with AND/OR logic",
            "Increase alarm evaluation periods to reduce false positives",
            "Use CloudWatch anomaly detection on each metric",
            "Configure SNS filter policies to suppress duplicate notifications"
          ],
          "correctAnswer": 0,
          "explanation": "CloudWatch composite alarms allow combining multiple alarms using boolean logic (AND, OR, NOT). For this scenario: Create individual alarms for CPU (>80%), error rate (>5%), and latency (>2s). Then create a composite alarm: ALARM when (CPUAlarm AND ErrorRateAlarm AND LatencyAlarm). The composite alarm triggers only when all three conditions are true simultaneously, reducing false positives from isolated metric spikes. Composite alarms support: nested composition (composite alarms referencing other composite alarms), up to 100 alarm rules, suppression of underlying alarm notifications (preventing alert fatigue). Option B (longer evaluation periods) may miss short-duration but severe issues. Option C (anomaly detection) helps identify unusual patterns but doesn't correlate multiple metrics. Option D (SNS filtering) suppresses notifications but doesn't change alarm logic. Use composite alarms for: complex failure scenarios requiring multi-metric correlation, reducing alert fatigue, implementing service-level indicators (SLIs) requiring multiple metrics. Configure different thresholds for warning (2 of 3) vs critical (all 3) composite alarms."
        },
        {
          "id": "D3-T3.1-Q5",
          "question": "A DevOps team manages infrastructure changes via CloudFormation but notices stacks showing DRIFT even though no manual changes were made. Investigation shows that some resources (security groups, IAM roles) were modified outside CloudFormation by automation scripts. How should they prevent and detect this?",
          "options": [
            "Run CloudFormation drift detection daily and automatically update stacks to fix drift",
            "Implement AWS Config rules to prevent changes to CloudFormation-managed resources and alert on violations",
            "Use CloudFormation Stack Policy to prevent updates to critical resources",
            "Delete and recreate stacks monthly to eliminate drift"
          ],
          "correctAnswer": 1,
          "explanation": "AWS Config rules provide preventive and detective controls for drift: (1) Create custom Config rules checking if resources are managed by CloudFormation (using cloudformation:stack-id tag or GetStackResources API), (2) Config rule evaluates on configuration changes, (3) Non-compliant resources (modified outside CloudFormation) trigger alerts via SNS, (4) Optional: Use Config remediation actions with Systems Manager Automation to revert unauthorized changes. This prevents drift by alerting immediately when out-of-band changes occur, allowing quick correction. Option A (auto-update stacks) is risky - drift might be intentional for valid reasons; automatic updates could revert legitimate emergency changes. Option C (Stack Policy) prevents CloudFormation updates but doesn't prevent external changes. Option D (recreation) causes unnecessary downtime. Best practice: (1) Tag CloudFormation-managed resources consistently, (2) Use Config to monitor for external changes, (3) Educate teams to make changes through CloudFormation only, (4) Run drift detection before stack updates to avoid conflicts. For preventing changes entirely: use SCPs denying API calls without CloudFormation role, or IAM policies restricting manual changes."
        },
        {
          "id": "D3-T3.1-Q6",
          "question": "A company uses AWS Service Catalog to provision pre-approved infrastructure for development teams. They want to ensure launched products (VPCs, databases) remain compliant with organizational standards over their lifecycle, detecting drift from the original portfolio configuration. What should they implement?",
          "options": [
            "Use AWS Config rules to monitor Service Catalog launched resources for compliance",
            "Enable Service Catalog TagOptions to track provisioned products",
            "Use CloudFormation drift detection on underlying stacks",
            "Implement AWS CloudTrail logging for Service Catalog actions"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Config provides ongoing compliance monitoring for Service Catalog-launched resources: (1) Service Catalog products are deployed via CloudFormation stacks, (2) Config rules evaluate these stacks' resources against compliance requirements (encryption, tagging, network configs), (3) Config detects configuration changes over time, reporting non-compliance, (4) Service Catalog integrates with Config for governance at scale. Example rules: ensuring S3 buckets have encryption, RDS has backups enabled, EC2 in approved VPCs. Option B (TagOptions) helps with organization and cost allocation but doesn't monitor compliance. Option C (drift detection) shows changes from original template but requires manual triggering and doesn't evaluate against compliance policies. Option D (CloudTrail) logs actions but doesn't evaluate compliance. Service Catalog best practices: (1) Define constraints in portfolios (launch constraints, tag update constraints), (2) Use Config for continuous monitoring, (3) Integrate with AWS Budgets for cost control, (4) Use CloudFormation StackSets to update products across accounts. Service Catalog constraints can enforce things like specific IAM roles, required tags, and resource limits at launch time."
        },
        {
          "id": "D3-T3.1-Q7",
          "question": "An application generates high-cardinality custom metrics (unique customer IDs as dimensions) in CloudWatch, resulting in thousands of metric streams and high costs. Which approach optimizes cost while maintaining observability?",
          "options": [
            "Use CloudWatch embedded metric format (EMF) in application logs, extracting metrics only when needed for queries",
            "Continue publishing all metrics but increase aggregation period from 1 minute to 5 minutes",
            "Store custom metrics in DynamoDB instead of CloudWatch",
            "Publish metrics to S3 and use Athena for analysis"
          ],
          "correctAnswer": 0,
          "explanation": "CloudWatch Embedded Metric Format (EMF) provides cost-effective high-cardinality metrics: (1) Application logs structured JSON to CloudWatch Logs with metric metadata, (2) Metrics are extracted automatically from logs, appearing in CloudWatch Metrics, (3) You only pay for log storage ($0.50/GB ingested) instead of custom metrics ($0.30 per metric), (4) High-cardinality dimensions (customer IDs) can be included in logs without creating thousands of metric streams, (5) CloudWatch Logs Insights can query logs with full dimensionality. EMF is ideal for: high-cardinality metrics, metrics from serverless functions (Lambda automatically uses EMF), scenarios where not all dimensions are queried regularly. Option B (longer aggregation) saves some storage but loses granularity and doesn't address cardinality. Option C (DynamoDB) requires custom code for metric collection, querying, and visualization - reinventing CloudWatch. Option D (S3/Athena) has query latency unsuitable for real-time dashboards. EMF format example: {\"_aws\": {\"CloudWatchMetrics\": [{\"Namespace\": \"App\", \"Metrics\": [{\"Name\": \"Latency\"}], \"Dimensions\": [[\"CustomerId\"]]}]}, \"CustomerId\": \"12345\", \"Latency\": 145}. This appears as both a log entry and a CloudWatch metric."
        },
        {
          "id": "D3-T3.1-Q8",
          "question": "A company runs a multi-tier application with ALB, ECS, and RDS. They want automated remediation when: ALB returns 5XX errors, ECS tasks restart frequently, or RDS CPU exceeds 90%. Actions should include: restart tasks, scale out, or page on-call engineer. Which architecture provides this automation?",
          "options": [
            "CloudWatch Alarms triggering Lambda functions with remediation logic, using SNS for paging",
            "EventBridge rules detecting CloudWatch alarm state changes, triggering Systems Manager Automation documents for remediation and SNS for paging",
            "CloudWatch Alarms directly triggering Auto Scaling policies and SNS topics",
            "AWS Config remediation actions for non-compliant resources"
          ],
          "correctAnswer": 1,
          "explanation": "EventBridge + Systems Manager Automation provides comprehensive remediation automation: (1) CloudWatch Alarms detect issues (5XX, task restarts, high CPU), (2) Alarms change state (OK  ALARM), (3) EventBridge rule matches alarm state change events, (4) EventBridge triggers Systems Manager Automation documents with remediation logic, (5) Automation documents can: restart ECS tasks (ECS:UpdateService), modify RDS (RDS:ModifyDBInstance), invoke Lambda, execute AWS APIs, (6) SNS notifies on-call for severe issues. This architecture separates concerns: CloudWatch for detection, EventBridge for routing, Automation for remediation, SNS for human notification. Option A works but Lambda requires custom code for each remediation; Automation documents are reusable and AWS-managed. Option C (direct alarm actions) limited to Auto Scaling and SNS - can't restart tasks or modify RDS. Option D (Config remediation) for configuration compliance, not performance issues. Systems Manager Automation benefits: visual workflow editor, AWS-managed documents for common tasks (e.g., AWS-StopEC2Instance), approval steps for human validation, runbooks as code (version control). Use Automation for: self-healing (restart failed components), auto-remediation (security group fixes), operational runbooks (deployment procedures)."
        },
        {
          "id": "D3-T3.1-Q9",
          "question": "A global application uses CloudWatch dashboards for monitoring. Operations teams in different regions want customized views (US team sees US resources, EU team sees EU resources) without maintaining separate dashboards. How can this be achieved?",
          "options": [
            "Create dashboard variables allowing users to select region dynamically",
            "Duplicate dashboards per region with different resource filters",
            "Use CloudWatch cross-region functionality but manually switch regions",
            "Create a custom dashboard application querying CloudWatch APIs"
          ],
          "correctAnswer": 0,
          "explanation": "CloudWatch dashboard variables (also called dynamic dashboards) allow runtime customization: (1) Define variables for dimensions like Region, InstanceType, Environment, (2) Dashboard widgets reference variables: {region}, {instance}, (3) Users select variable values from dropdowns, (4) Dashboard updates to show selected resources. This enables a single dashboard serving multiple teams/regions. Variables support: property values (regions, AZs), dimension values from metrics, label values, custom values. For this scenario: Create variable 'region' with values [us-east-1, eu-west-1, ap-southeast-1], reference in widgets: \"AWS/EC2\" metrics for region=variable.region. Users switch regions via dropdown. Option B (duplicate dashboards) creates maintenance burden - changes must be applied to all copies. Option C still requires manual switching. Option D is unnecessary complexity. Dashboard variables are also useful for: environment selection (dev/staging/prod), application filtering (AppA/AppB), auto-scaling group selection. Combine with CloudWatch dashboard sharing and IAM permissions to provide role-based dashboard access. Variables can populate from CloudWatch Metric streams dynamically (e.g., all Auto Scaling groups in the account)."
        },
        {
          "id": "D3-T3.1-Q10",
          "question": "A company needs to inventory all EC2 instances, RDS databases, and S3 buckets across 50 AWS accounts, including configuration details (encryption, public access, tags). They need this data queryable for compliance reports. Which AWS service provides this with minimal operational overhead?",
          "options": [
            "AWS Config with aggregator for multi-account configuration tracking",
            "Custom Lambda functions querying AWS APIs and storing results in DynamoDB",
            "AWS Systems Manager Inventory for resource data collection",
            "CloudTrail logs analysis with Athena"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Config with multi-account multi-region aggregator provides comprehensive resource inventory: (1) Enable Config in all accounts to record resource configurations, (2) Create aggregator in central account collecting data from all source accounts, (3) Config tracks: resource type, creation time, relationships, configuration changes over time, (4) Config Advanced Queries use SQL to query across all accounts/regions, (5) Compliance dashboard shows aggregate compliance across organization. Example query: SELECT resourceId, resourceType, configuration.encrypted WHERE resourceType = 'AWS::RDS::DBInstance' AND configuration.encrypted = false. This identifies unencrypted RDS instances across all accounts. Option B (custom Lambda) requires significant development for API pagination, handling limits, incremental updates, and maintaining schema. Option C (Systems Manager Inventory) focuses on EC2 instance software inventory (applications, OS details), not broad AWS resource inventory. Option D (CloudTrail) logs API calls but doesn't maintain current state inventory. Config aggregator supports: up to 10,000 source accounts, retention of configuration history (up to 7 years), snapshot delivery to S3 for compliance archives. Use Config for: compliance reporting, resource tracking, change management, security audits, cross-account asset inventory."
        },
        {
          "id": "D3-T3.1-Q11",
          "question": "An application team wants to receive notifications when CloudFormation stack operations fail, succeed, or require manual intervention. They use stacks across multiple accounts and want centralized notification handling. Which solution provides this?",
          "options": [
            "Configure SNS topics per stack with subscriptions for operations team",
            "Use EventBridge rules matching CloudFormation events, routing to central SNS topic across accounts",
            "Enable CloudFormation stack notifications in each stack configuration",
            "Use CloudTrail to log CloudFormation API calls and trigger Lambda on failures"
          ],
          "correctAnswer": 1,
          "explanation": "EventBridge (CloudWatch Events) provides centralized, event-driven notifications for CloudFormation: (1) CloudFormation emits events for stack operations (create complete/failed, update complete/failed, drift detected), (2) EventBridge rules match these events using event patterns, (3) Rules can route to cross-account/cross-region targets including SNS, (4) Single SNS topic receives all CloudFormation notifications from all stacks/accounts, (5) SNS filters allow subscribers to filter by account, stack name, or operation type. Event pattern example: {\"source\": [\"aws.cloudformation\"], \"detail-type\": [\"CloudFormation Stack Status Change\"], \"detail\": {\"stack-status\": [\"CREATE_FAILED\", \"UPDATE_FAILED\"]}}. This matches only failure events. Option A (SNS per stack) requires configuring each stack individually - not scalable. Option C (stack notifications) requires manual config per stack and doesn't aggregate across accounts. Option D (CloudTrail + Lambda) adds unnecessary complexity when EventBridge provides native event matching. EventBridge benefits: event filtering (only failures, only specific stacks), transformation (customize notification format), multiple targets (SNS, Lambda, Step Functions simultaneously), cross-account event bus for central event collection. Use for: centralized operations monitoring, compliance tracking (drift detection events), integration with incident management systems."
        },
        {
          "id": "D3-T3.1-Q12",
          "question": "A company enforces tagging standards (Project, Environment, Owner) on all resources. They want automated detection and remediation: new untagged resources should be tagged automatically if possible, or notifications sent to resource owners for manual tagging. Which combination achieves this? (Select TWO)",
          "options": [
            "AWS Config rule detecting untagged resources with automatic remediation via Systems Manager Automation",
            "Service Control Policy (SCP) denying resource creation without required tags",
            "EventBridge rule detecting resource creation events, triggering Lambda to tag resources",
            "CloudFormation drift detection to identify tagging drift",
            "AWS Organizations tag policies enforcing required tags",
            "CloudWatch alarm on untagged resource count"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            2
          ],
          "explanation": "Comprehensive tagging automation requires detection and remediation: (1) AWS Config rule for tag compliance - evaluates resources against tagging requirements, marks non-compliant resources. Config remediation actions trigger Systems Manager Automation to apply tags automatically (if permissions allow). (2) EventBridge rule on resource creation - detects AWS API calls creating resources (via CloudTrail), triggers Lambda to immediately tag new resources before they violate compliance. This provides real-time tagging vs Config's periodic evaluation. Option B (SCP) is preventive but blocks resource creation entirely if tags missing - too strict for scenarios where tags should be applied post-creation. Option D (drift detection) is for CloudFormation managed resources only. Option E (tag policies) validates tag keys/values but doesn't automatically remediate. Option F (CloudWatch alarm) doesn't provide remediation. Best practice: Layered approach: (1) Tag policies enforce valid values, (2) EventBridge + Lambda for immediate tagging, (3) Config for compliance detection, (4) Systems Manager Automation for remediation, (5) SCPs as last resort to prevent untagged resources in critical environments. Tag automation Lambda should: attempt to infer tags from creator identity (Owner), environment from VPC (Environment), query CMDB for Project tag, notify if tags can't be determined."
        }
      ]
    },
    {
      "filename": "domain-3-task-3.2-security-improvements.json",
      "domain": "Domain 3: Continuous Improvement for Existing Solutions",
      "task": "Task 3.2: Security Improvements",
      "question_count": 10,
      "questions": [
        {
          "id": "D3-T3.2-Q1",
          "question": "GuardDuty detects an EC2 instance communicating with a known command-and-control server. The security team wants automated response: isolate the instance, capture forensic data, and notify the security team. Which architecture provides automated incident response?",
          "options": [
            "GuardDuty finding triggers EventBridge rule  Lambda function: modifies security group to block all traffic, creates EBS snapshots, sends SNS notification",
            "Configure GuardDuty to automatically quarantine compromised instances",
            "Use Security Hub to aggregate GuardDuty findings and manually respond",
            "Create CloudWatch alarm on GuardDuty finding count and page security team"
          ],
          "correctAnswer": 0,
          "explanation": "Automated incident response requires EventBridge + Lambda orchestration: (1) GuardDuty publishes findings to EventBridge as events, (2) EventBridge rule matches specific finding types (e.g., Trojan:EC2/DNSDataExfiltration), (3) Lambda function executes response: modify instance security group to deny all ingress/egress (isolate), create EBS volume snapshots (preserve evidence), tag instance as 'quarantined', invoke Systems Manager to capture memory dump if SSM agent running, send SNS notification to security team with finding details. Option B is incorrect - GuardDuty doesn't have automatic response capabilities; it only detects and reports. Option C (Security Hub aggregation) provides centralized view but doesn't automate response. Option D (CloudWatch alarm) only notifies, doesn't remediate. Lambda function should: use least privilege IAM role, log all actions to CloudTrail for audit, create forensic S3 bucket for evidence storage (snapshots, logs), optionally invoke Step Functions for complex multi-step response workflows. Consider using AWS Security Hub's built-in automated response and remediation actions (ASFF) as an alternative to custom Lambda. For regulated environments: ensure forensic data collection complies with chain of custody requirements, use CloudFormation to deploy response automation consistently across accounts."
        },
        {
          "id": "D3-T3.2-Q2",
          "question": "A company uses AWS Macie to discover sensitive data in S3. Macie found PII in 15 out of 10,000 buckets. The security team wants automated remediation: move sensitive objects to encrypted, restricted-access buckets, and alert data owners. What should they implement?",
          "options": [
            "Macie sensitive data discovery job  EventBridge rule on findings  Lambda: copy objects to secure bucket, delete from source, notify via SNS",
            "Enable Macie auto-remediation feature to move sensitive data automatically",
            "Use S3 Batch Operations to copy objects based on Macie finding reports",
            "Create AWS Config rule to detect sensitive data and remediate via Systems Manager"
          ],
          "correctAnswer": 0,
          "explanation": "Macie automated remediation workflow: (1) Macie sensitive data discovery job analyzes S3 objects, (2) Macie publishes findings to EventBridge when PII/sensitive data detected, (3) EventBridge rule matches Macie finding events (SensitiveData:S3Object/Personal or Custom), (4) Lambda function triggered with finding metadata (bucket, object key, PII types), (5) Lambda: verifies finding severity, copies object to restricted S3 bucket (versioning enabled, MFA delete, bucket key encryption), updates object ACL to private, optionally deletes from source or tags for review, queries identity/access management to determine data owner, sends SNS notification to owner. Option B doesn't exist - Macie detects but doesn't remediate automatically. Option C (S3 Batch Operations) requires manual job creation from Macie reports - not automated. Option D (Config) is for configuration compliance, not data content analysis. Implementation considerations: Lambda function should handle large objects (use multipart copy), implement exponential backoff for S3 API limits, maintain audit log in DynamoDB (what moved, when, by whom), use S3 inventory to track object locations. For compliance: Macie classification results should drive DLP policies, integrate with data governance tools, consider AWS Clean Rooms for data sharing without exposure. Macie finding types: Financial (credit card), Personal (SSN, passport), Credentials (AWS secrets), Custom (regex patterns). Configure suppression rules to ignore false positives (test data, encrypted data)."
        },
        {
          "id": "D3-T3.2-Q3",
          "question": "Security audit reveals that ACM certificates are expiring without renewal, causing service outages. Certificates are used with ALB, CloudFront, and API Gateway. How can certificate lifecycle management be improved to prevent expirations?",
          "options": [
            "Implement EventBridge rule detecting ACM DaysToExpiry metric, triggering Lambda to renew certificates 30 days before expiration",
            "Use ACM automatic certificate renewal for publicly trusted certificates and set up CloudWatch alarms on DaysToExpiry for private certificates",
            "Migrate to AWS Certificate Manager Private CA for automated renewal",
            "Create calendar reminders for manual certificate renewal"
          ],
          "correctAnswer": 1,
          "explanation": "ACM certificate lifecycle management requires understanding automatic renewal: (1) Publicly trusted ACM certificates (domain-validated via DNS or email) renew AUTOMATICALLY if validation records remain in place. ACM attempts renewal 60 days before expiration. (2) Private certificates (issued by ACM Private CA) require manual renewal or automation. (3) Imported certificates (from external CAs) do NOT auto-renew - must be manually reimported before expiration. The solution: Ensure DNS validation records persist (CNAME for ACM validation in Route 53), monitor CloudWatch metric AWS/CertificateManager DaysToExpiry for all certificates, create CloudWatch alarm triggering SNS when DaysToExpiry < 30 for private/imported certificates, automate private certificate renewal using Lambda + ACM API RequestCertificate. Option A is incorrect - you cannot manually renew ACM-issued public certificates; ACM handles this automatically. Attempting to request new certificate for same domain creates duplicate, doesn't renew existing. Option C (ACM Private CA) is for different use case (internal certificates), not solution for publicly trusted certificates. Option D (manual reminders) is error-prone. Best practices: Use DNS validation (not email) for automatic renewal, monitor all ACM certificates in centralized account, use AWS Config rule (acm-certificate-expiration-check) to detect approaching expiration, document certificate owners and rotation procedures, for imported certificates: automate renewal with source CA and reimport via Lambda. Certificate transparency logs: ACM certificates appear in public CT logs for audit. ACM supports up to 10 SANs per certificate for efficiency."
        },
        {
          "id": "D3-T3.2-Q4",
          "question": "A company wants to continuously verify that no IAM policies grant broad permissions (Principal: *, Action: *, Resource: *) and automatically flag them for review. Which service provides this capability with minimal operational overhead?",
          "options": [
            "IAM Access Analyzer with policy validation scans",
            "AWS Config with managed rule iam-policy-no-statements-with-admin-access",
            "Custom Lambda function analyzing IAM policies daily",
            "Security Hub compliance standard checks"
          ],
          "correctAnswer": 1,
          "explanation": "AWS Config rule 'iam-policy-no-statements-with-admin-access' continuously monitors IAM policies: (1) Evaluates IAM policies attached to users, groups, and roles, (2) Flags non-compliant policies granting admin access (Action: *, Resource: *), (3) Triggers on configuration changes (new policy, policy update), (4) Integrates with Config remediation for automated response, (5) Compliance timeline shows when violations occurred. For this use case: Enable Config in all accounts, activate managed rule, set up SNS notification on non-compliance, optionally configure remediation to detach overly permissive policies (with approval step). Option A (Access Analyzer) focuses on external resource sharing and policy validation during development, not continuous monitoring of existing policies. Option C (Lambda) requires custom code for policy parsing, IAM API pagination, and handling policy variations - reinventing Config. Option D (Security Hub) aggregates findings from Config and other services but doesn't directly evaluate policies. Config rules for IAM best practices: iam-password-policy (password requirements), iam-user-unused-credentials-check (inactive users), iam-root-access-key-check (root keys exist), access-keys-rotated (key age), mfa-enabled-for-iam-console-access (MFA on users). Use Config conformance packs to deploy multiple related rules together (e.g., 'Operational Best Practices for IAM'). Config remediation via Systems Manager Automation: AWS-DisableS3BucketPublicReadWrite, AWS-DeleteUnusedIAMRole. For preventive control: use SCPs to deny creation of policies with Action: * and Resource: *, blocking overly permissive policies at creation time (defense in depth: SCP prevents, Config detects)."
        },
        {
          "id": "D3-T3.2-Q5",
          "question": "Security team uses AWS Detective to investigate a GuardDuty finding about unusual API calls from an IAM user. They want to understand: which resources the user accessed, source IPs over time, and whether this represents privilege escalation. Which Detective capability provides this analysis?",
          "options": [
            "Detective finding groups automatically correlating related security events",
            "Detective visualizations showing IAM user activity timeline, resource access patterns, and IP address history with machine learning anomaly detection",
            "CloudTrail Insights analysis integrated into Detective",
            "VPC Flow Logs correlation in Detective"
          ],
          "correctAnswer": 1,
          "explanation": "AWS Detective provides ML-powered security investigation with visualization: (1) Ingests CloudTrail, VPC Flow Logs, GuardDuty findings automatically, (2) Creates behavior graph showing relationships between users, roles, IP addresses, AWS resources, (3) Time-based visualizations: 'Scope time' window to view activity during investigation period, Baselines showing typical vs anomalous behavior, IP address geo-location and first-seen dates, Resource access patterns (which S3 buckets, EC2 instances, databases user accessed), API call volumes and types over time. (4) Detective uses ML to highlight unusual activities: new IP addresses, new geolocations, spike in API calls, new services accessed. For this scenario: Select IAM user in Detective console, view Activity timeline showing all API calls, examine ResourceAccessed panel for accessed resources, check IPAddress panel for source IPs with geo-location, look for PrivilegeEscalation panel highlighting suspicious permission changes. Option A (finding groups) exists but is for aggregating related findings, not detailed investigation. Option C (CloudTrail Insights) detects unusual API activity but Detective provides deeper investigation with graph visualization. Option D (Flow Logs) show network traffic, not IAM API activity. Detective investigation workflow: Start with GuardDuty finding in Detective (integrated), pivot to involved entities (user, role, resource), examine behavior during suspected time + baseline comparison, expand to related entities (which other users from same IP, what else did the user access), export findings to case management system. Detective supports up to 1 year of aggregated data for investigations. Pricing: per GB of ingested data (CloudTrail events, VPC Flow Logs, GuardDuty findings)."
        },
        {
          "id": "D3-T3.2-Q6",
          "question": "A company needs to enforce network segmentation: production workloads (subnet-prod) cannot communicate with development workloads (subnet-dev) even though both are in the same VPC. Security groups and NACLs are already configured, but audit shows some cross-environment traffic. What additional security control should be implemented?",
          "options": [
            "AWS Network Firewall with stateful rule groups blocking traffic between production and development CIDR ranges",
            "VPC Security Groups with explicit deny rules (Security Groups only support allow rules, so this won't work)",
            "AWS WAF protecting application endpoints from cross-environment access",
            "VPC Flow Logs to monitor traffic and manually block violating instances"
          ],
          "correctAnswer": 0,
          "explanation": "AWS Network Firewall provides stateful, inline traffic inspection at VPC level: (1) Deploy Network Firewall endpoints in dedicated subnet per AZ, (2) Update route tables to route traffic through firewall endpoints, (3) Create stateful rule group with Suricata-compatible rules: 'drop ip $PROD_CIDR any -> $DEV_CIDR any' (block prod to dev), 'drop ip $DEV_CIDR any -> $PROD_CIDR any' (block dev to prod), (4) Firewall inspects packets, enforces rules, logs violations to S3/CloudWatch. Network Firewall provides defense-in-depth beyond Security Groups/NACLs: IDS/IPS capabilities (detect exploits, malware), domain filtering (block DNS queries to malicious domains), centralized rule management for complex policies, protocol enforcement (block non-standard traffic). Option B is incorrect - Security Groups are stateful whitelist only (implicit deny); you cannot create explicit deny rules (though NACLs support explicit deny, they might be insufficient if misconfured). Option C (WAF) operates at application layer (HTTP/HTTPS), not network layer - won't block TCP/UDP between subnets. Option D (Flow Logs) is detective, not preventive. Implement Network Firewall: (1) Create firewall policy with rule groups, (2) Deploy firewall in VPC, (3) Update route tables: 0.0.0.0/0  firewall endpoint for inter-subnet routing, (4) Monitor firewall logs for denied connections, (5) Use AWS Firewall Manager to deploy Network Firewall across VPCs centrally. Use cases: segment workloads (prod/dev), comply with regulations requiring stateful inspection, block outbound traffic to unapproved destinations, inspect encrypted traffic with TLS inspection (requires certificate). Rule group types: Stateful (track connection state, bi-directional), Stateless (simple allow/deny, processed before stateful), Domain list (block/allow based on domain names)."
        },
        {
          "id": "D3-T3.2-Q7",
          "question": "CloudTrail logs show an IAM user making API calls that should not be possible given their attached policies (e.g., launching EC2 instances when user has no EC2 permissions). Investigation is needed to identify the permission source. Which IAM feature helps identify HOW the user gained these permissions?",
          "options": [
            "IAM Access Analyzer policy validation",
            "IAM policy simulator with CloudTrail event details",
            "CloudTrail Insights to detect unusual IAM activity",
            "IAM credential report showing user permissions"
          ],
          "correctAnswer": 1,
          "explanation": "IAM Policy Simulator helps debug permission issues: (1) Select IAM user/role, (2) Input specific API action (ec2:RunInstances), (3) Optionally input CloudTrail event request parameters (specific AMI, VPC), (4) Simulator evaluates: identity-based policies (attached to user), resource-based policies (e.g., AMI launch permissions), permission boundaries, SCPs, session policies (if assumed role), (5) Results show: whether action is allowed/denied, which policy statements contributed to decision, evaluation logic explaining why. For this scenario: Enter user and ec2:RunInstances action, Simulator may reveal: user assumed a role with EC2 permissions (check CloudTrail for AssumeRole), user is member of group with broad permissions, resource-based policy on AMI grants access, temporary credentials from federation had broader permissions. Policy Simulator shows complete evaluation logic following AWS's policy evaluation flowchart. Option A (Access Analyzer) validates policies for external access and policy correctness, not runtime permission evaluation. Option C (Insights) detects unusual activity but doesn't explain permission source. Option D (credential report) shows users and credential status, not permission evaluation. Debugging IAM permissions: (1) Use Policy Simulator to test access, (2) Check CloudTrail for AssumeRole calls (user may have assumed role), (3) Review all group memberships (users inherit group policies), (4) Examine resource policies (S3 bucket policies, KMS key policies may grant cross-account access), (5) Verify SCPs not restricting (though SCPs deny, not grant), (6) Check session policies if using STS assume-role with session tags. Policy evaluation order: Explicit Deny in any policy  SCP allow check  Resource-based policy allow  Identity-based policy allow  Session policy allow (if applicable)  Permission boundary allow. If action allowed anywhere and not explicitly denied, access grants."
        },
        {
          "id": "D3-T3.2-Q8",
          "question": "A company wants to implement just-in-time privileged access: developers can request temporary admin access to production accounts for incident response, access expires after 4 hours, and all actions during elevated access are logged. Which solution provides this capability?",
          "options": [
            "AWS SSO with permission sets that automatically expire after 4 hours",
            "Systems Manager Session Manager with time-limited assume-role policies and CloudTrail logging",
            "Custom workflow: developer requests via ServiceNow  Lambda creates temporary IAM user with 4-hour STS session  CloudTrail logs  Lambda deletes user",
            "IAM roles with maximum session duration of 4 hours, requested via self-service portal, activities logged to CloudTrail"
          ],
          "correctAnswer": 3,
          "explanation": "IAM roles with session duration limits provide just-in-time access: (1) Create 'BreakGlassAdmin' role with maximum session duration = 4 hours (default 1 hour, max 12 hours), (2) Trust policy allows developers to assume role (or federated users), (3) Self-service portal (API Gateway + Lambda): developer authenticates, requests access with justification, Lambda assumes role via STS, returns temporary credentials valid 4 hours, logs request to audit table, (4) CloudTrail automatically logs all API actions made with temporary credentials including role session name identifying requester, (5) Credentials expire after 4 hours, revoking access automatically. Option A (AWS SSO permission sets) have session duration but SSO doesn't support 'request access' workflow natively - permission sets are pre-assigned. Option B (Session Manager) is for interactive shell access to EC2, not for AWS API access. Option C (temporary IAM user) is overly complex - IAM users are permanent until deleted (not truly temporary), uses STS for session but still creates IAM user unnecessarily. Enhancements: Use AWS CloudFormation or CDK to deploy break-glass infrastructure, Integrate approval workflow (Step Functions): developer requests  manager approves via SNS  Lambda grants access, Use CloudWatch Events to alert security team when break-glass role assumed, Store justifications in DynamoDB for compliance audit, Implement MFA requirement for assume-role (aws:MultiFactorAuthPresent condition in role trust policy), Monitor with GuardDuty for unusual break-glass usage patterns. Session duration in STS AssumeRole API: DurationSeconds parameter (900 to 43200 seconds / 15 min to 12 hours). Role's maximum session duration setting overrides API request if API requests longer session. Audit trail: CloudTrail logs show roleSessionName identifying who assumed role, all subsequent API calls include role ARN and session name."
        },
        {
          "id": "D3-T3.2-Q9",
          "question": "Security audit reveals that CloudTrail logs are being deleted from S3 buckets in some accounts, potentially hiding malicious activity. What controls should be implemented to prevent CloudTrail log tampering? (Select TWO)",
          "options": [
            "Enable S3 Object Lock in compliance mode on CloudTrail S3 bucket with retention period",
            "Use S3 bucket policies denying all delete operations even from account root",
            "Enable MFA Delete on the CloudTrail S3 bucket",
            "Configure CloudTrail log file validation to detect modifications",
            "Use AWS Organizations to enforce CloudTrail in all accounts",
            "Store CloudTrail logs in S3 Glacier Deep Archive immediately"
          ],
          "type": "multiple",
          "correctAnswer": [
            0,
            2
          ],
          "explanation": "Preventing CloudTrail log deletion requires WORM protection: (1) S3 Object Lock in compliance mode with retention period (e.g., 7 years for SOX, 90 days minimum recommended) ensures objects cannot be deleted or modified until retention expires, even by root account. Compliance mode lock cannot be removed. (2) MFA Delete requires MFA token for object deletion or versioning changes, adding human verification step preventing automated or accidental deletion. Combined: Object Lock prevents deletion during retention, MFA Delete adds authentication layer for operations after retention. Option B (bucket policy deny delete) can be overridden by root or by changing bucket policy itself - not as secure as Object Lock. Option D (log file validation) detects tampering but doesn't prevent it (detective vs preventive control). Option E (Organizations enforcing CloudTrail) ensures trails exist but doesn't protect logs from deletion. Option F (Glacier) delays access but doesn't prevent deletion. Implementation: Create S3 bucket with versioning enabled (prerequisite for Object Lock), Enable Object Lock, Set default retention (compliance mode, 90 days), Enable MFA Delete, Bucket policy denying unencrypted uploads and non-SSL access, CloudTrail configured to use this bucket. Additional controls: Cross-account CloudTrail logging (logs from member accounts to security account bucket), CloudWatch Logs for real-time monitoring despite S3 log delays, SNS notifications on S3 bucket policy changes or Object Lock configuration changes, Regular access review for CloudTrail S3 bucket (minimize permissions). CloudTrail best practices: Organization trail (one trail for all accounts), Log file validation enabled, Encrypted with KMS CMK (audit key usage), Multi-region trail (logs from all regions to one bucket), Integrated with CloudWatch Logs for alerting. S3 Object Lock modes: Compliance (cannot be deleted even by root, for regulatory compliance), Governance (can be deleted with special permissions, for operational flexibility). Use Compliance mode for compliance requirements, Governance mode for flexible retention policies."
        },
        {
          "id": "D3-T3.2-Q10",
          "question": "A company implements infrastructure as code using CloudFormation. They want to enforce that all stacks use encrypted storage (encrypted EBS, S3 buckets with encryption, encrypted RDS) BEFORE deployment. Which approach provides pre-deployment validation?",
          "options": [
            "Use CloudFormation Hooks to validate stack templates before CREATE/UPDATE operations",
            "Enable AWS Config rules to detect non-encrypted resources after deployment",
            "Implement CI/CD pipeline step running cfn-lint to check templates",
            "Use SCPs to deny creation of unencrypted resources"
          ],
          "correctAnswer": 0,
          "explanation": "CloudFormation Hooks provide pre-deployment validation: (1) Hooks are registered with CloudFormation, (2) Hooks execute before CREATE, UPDATE, or DELETE operations on stacks, (3) Hook evaluates stack template and resources, (4) If hook returns FAILED, CloudFormation operation aborts (stack not created/updated), (5) If hook returns SUCCESS, operation proceeds. For this scenario: Create Hook checking CloudFormation template for: EBS volumes with Encrypted=true, S3 buckets with BucketEncryption configuration, RDS instances with StorageEncrypted=true. Hook Lambda function parses template, validates encryption properties, returns pass/fail. This prevents deployment of non-compliant stacks. Option B (Config rules) is detective (after deployment), not preventive. Option C (cfn-lint) is static analysis tool useful for syntax and basic validation but doesn't have context of organizational policies - would need custom rules. Option D (SCPs) prevents API calls but is organization-wide, not template-specific, and difficult to manage for complex policies. CloudFormation Hooks use cases: Policy enforcement (tagging, encryption, approved resource types), Cost control (deny expensive instance types), Security validation (no public S3 buckets, required security groups), Compliance (HIPAA, PCI-DSS resource requirements). Hooks can: Scan entire template, Evaluate specific resource types, Make API calls for external validation, Timeout after 30 seconds (plan hook execution accordingly). Deploy hooks using: CloudFormation Registry, Hooks CLI plugin, Share hooks across organization using AWS Organizations. Alternative: Use CloudFormation Guard rules (policy-as-code) to validate templates, integrated into CI/CD pipeline. Guard uses declarative rules: 'AWS::S3::Bucket { Properties.BucketEncryption exists }'. Hook vs Guard: Hooks run in CloudFormation service (required for deployment), Guard runs in CI/CD (shift-left validation). Use both for defense-in-depth: Guard in CI/CD for fast feedback, Hooks as final enforcement gate."
        }
      ]
    }
  ]
}